# 1.2 íŒŒí‹°ì…˜ ë° ìƒ¤ë“œ ê´€ë¦¬

## Kafka íŒŒí‹°ì…˜ vs Kinesis ìƒ¤ë“œ

### ê°œë… ë¹„êµ
| ì¸¡ë©´ | Kafka íŒŒí‹°ì…˜ | Kinesis ìƒ¤ë“œ |
|------|--------------|--------------|
| **ì²˜ë¦¬ëŸ‰** | íŒŒí‹°ì…˜ë‹¹ ë¬´ì œí•œ | ìƒ¤ë“œë‹¹ 1MB/ì´ˆ ë˜ëŠ” 1000 ë ˆì½”ë“œ/ì´ˆ |
| **ìŠ¤ì¼€ì¼ë§** | ìˆ˜ë™ íŒŒí‹°ì…˜ ì¶”ê°€ | ìë™/ìˆ˜ë™ ìƒ¤ë“œ ë¶„í• /ë³‘í•© |
| **ë¹„ìš©** | íŒŒí‹°ì…˜ ìˆ˜ ë¬´ê´€ | ìƒ¤ë“œë‹¹ ì‹œê°„ ë‹¨ìœ„ ê³¼ê¸ˆ |
| **ë³µì œ** | ë³µì œ íŒ©í„° ì„¤ì • | ìë™ 3ê°œ AZ ë³µì œ |
| **ìˆœì„œ ë³´ì¥** | íŒŒí‹°ì…˜ ë‚´ ìˆœì„œ ë³´ì¥ | ìƒ¤ë“œ ë‚´ ìˆœì„œ ë³´ì¥ |

## ìƒ¤ë“œ ê´€ë¦¬ ì „ëµ

### ì†ŒìŠ¤ íŒŒì¼ êµ¬ì¡°
* `2.1.2.src/shard_calculator.py` - ìƒ¤ë“œ ìˆ˜ ê³„ì‚° ë„êµ¬
* `2.1.2.src/shard_manager.py` - ë™ì  ìƒ¤ë“œ ìŠ¤ì¼€ì¼ë§ ê´€ë¦¬
* `2.1.2.src/partition_key_tester.py` - íŒŒí‹°ì…˜ í‚¤ ë¶„ì‚° í…ŒìŠ¤íŠ¸
* `2.1.2.src/kinesis-streams-terraform.tf` - Terraform ì¸í”„ë¼ ì½”ë“œ

### 1. ìƒ¤ë“œ ê³„ì‚° ë°©ë²•
ì‹¤í–‰: `python 2.1.2.src/shard_calculator.py`

ì£¼ìš” ê¸°ëŠ¥:
* ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ ìƒ¤ë“œ ìˆ˜ ê³„ì‚°
* ë ˆì½”ë“œ ìˆ˜ ê¸°ì¤€ ê³„ì‚°
* íŒŒí‹°ì…˜ í‚¤ ê¸°ì¤€ ê³„ì‚°
* ë¹„ìš© ì¶”ì • ë° ë²„í¼ ê³ ë ¤

ê³„ì‚° ì˜ˆì‹œ ì½”ë“œ:
```python
import math

def calculate_required_shards(requirements):
    """í•„ìš”í•œ ìƒ¤ë“œ ìˆ˜ ê³„ì‚°"""
    
    # ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ ê³„ì‚°
    throughput_shards = math.ceil(requirements['mb_per_second'] / 1.0)  # 1MB/ì´ˆ ì œí•œ
    
    # ë ˆì½”ë“œ ìˆ˜ ê¸°ì¤€ ê³„ì‚°
    records_shards = math.ceil(requirements['records_per_second'] / 1000)  # 1000 ë ˆì½”ë“œ/ì´ˆ ì œí•œ
    
    # íŒŒí‹°ì…˜ í‚¤ ê¸°ì¤€ ê³„ì‚° (ìˆœì„œ ë³´ì¥)
    partition_shards = requirements.get('partition_keys', 1)
    
    # ìµœëŒ€ê°’ ì„ íƒ
    required_shards = max(throughput_shards, records_shards, partition_shards)
    
    # í–¥í›„ í™•ì¥ì„±ì„ ìœ„í•œ ë²„í¼ (20% ì¶”ê°€)
    buffered_shards = math.ceil(required_shards * 1.2)
    
    return {
        'minimum_shards': required_shards,
        'recommended_shards': buffered_shards,
        'throughput_based': throughput_shards,
        'records_based': records_shards,
        'partition_based': partition_shards,
        'estimated_cost_per_hour': buffered_shards * 0.015
    }

# ì‚¬ìš© ì˜ˆì‹œ
requirements = {
    'mb_per_second': 3.5,
    'records_per_second': 2500,
    'partition_keys': 4  # 4ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ íŒŒí‹°ì…˜ í‚¤
}

shard_plan = calculate_required_shards(requirements)
print(f"ê¶Œì¥ ìƒ¤ë“œ ìˆ˜: {shard_plan['recommended_shards']}")
print(f"ì‹œê°„ë‹¹ ì˜ˆìƒ ë¹„ìš©: ${shard_plan['estimated_cost_per_hour']:.2f}")
```

### 2. ë™ì  ìƒ¤ë“œ ìŠ¤ì¼€ì¼ë§
ì‹¤í–‰: `python 2.1.2.src/shard_manager.py <stream_name> [action]`

ì‚¬ìš© ì˜ˆì‹œ:
```bash
# ìë™ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰
python 2.1.2.src/shard_manager.py my-stream auto_scale

# ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
python 2.1.2.src/shard_manager.py my-stream status

# ìˆ˜ë™ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
python 2.1.2.src/shard_manager.py my-stream scale_out
```

ì£¼ìš” ê¸°ëŠ¥:
```python
import boto3
import time
from datetime import datetime, timedelta

class KinesisShardManager:
    def __init__(self, stream_name):
        self.kinesis = boto3.client('kinesis')
        self.cloudwatch = boto3.client('cloudwatch')
        self.stream_name = stream_name
    
    def get_stream_metrics(self, minutes=5):
        """ìŠ¤íŠ¸ë¦¼ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=minutes)
        
        # ì²˜ë¦¬ëŸ‰ ë©”íŠ¸ë¦­ ì¡°íšŒ
        response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/Kinesis',
            MetricName='IncomingBytes',
            Dimensions=[
                {'Name': 'StreamName', 'Value': self.stream_name}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=60,  # 1ë¶„ ë‹¨ìœ„
            Statistics=['Average', 'Maximum']
        )
        
        if not response['Datapoints']:
            return {'avg_bytes_per_sec': 0, 'max_bytes_per_sec': 0}
        
        # ë°”ì´íŠ¸/ì´ˆë¡œ ë³€í™˜
        avg_bytes = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])
        max_bytes = max(dp['Maximum'] for dp in response['Datapoints'])
        
        return {
            'avg_bytes_per_sec': avg_bytes,
            'max_bytes_per_sec': max_bytes
        }
    
    def auto_scale(self):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
        print(f"ìë™ ìŠ¤ì¼€ì¼ë§ í™•ì¸: {self.stream_name}")
        
        metrics = self.get_stream_metrics()
        current_shards = self.get_current_shard_count()
        
        print(f"í˜„ì¬ ìƒ¤ë“œ ìˆ˜: {current_shards}")
        print(f"í‰ê·  ì²˜ë¦¬ëŸ‰: {metrics['avg_bytes_per_sec']/1024/1024:.2f} MB/ì´ˆ")
        print(f"ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {metrics['max_bytes_per_sec']/1024/1024:.2f} MB/ì´ˆ")
        
        if self.should_scale_out(metrics):
            self.scale_out()
        elif self.should_scale_in(metrics):
            self.scale_in()
        else:
            print("ìŠ¤ì¼€ì¼ë§ ë¶ˆí•„ìš”")
```

## íŒŒí‹°ì…˜ í‚¤ ì „ëµ

### íŒŒí‹°ì…˜ í‚¤ ë¶„ì‚° í…ŒìŠ¤íŠ¸
ì‹¤í–‰: `python 2.1.2.src/partition_key_tester.py [strategy_name]`

ì‚¬ìš© ì˜ˆì‹œ:
```bash
# ëª¨ë“  ì „ëµ ë¹„êµ
python 2.1.2.src/partition_key_tester.py compare

# íŠ¹ì • ì „ëµ í…ŒìŠ¤íŠ¸
python 2.1.2.src/partition_key_tester.py hash_based_key
```

### 1. íš¨ê³¼ì ì¸ íŒŒí‹°ì…˜ í‚¤ ì„¤ê³„
```python
import hashlib
import uuid
from datetime import datetime

class PartitionKeyStrategy:
    @staticmethod
    def user_based_key(user_id):
        """ì‚¬ìš©ì ê¸°ë°˜ íŒŒí‹°ì…˜ í‚¤ - ì‚¬ìš©ìë³„ ìˆœì„œ ë³´ì¥"""
        return str(user_id)
    
    @staticmethod
    def hash_based_key(identifier):
        """í•´ì‹œ ê¸°ë°˜ íŒŒí‹°ì…˜ í‚¤ - ê· ë“± ë¶„ì‚°"""
        return hashlib.md5(str(identifier).encode()).hexdigest()
    
    @staticmethod
    def time_based_key(timestamp, buckets=10):
        """ì‹œê°„ ê¸°ë°˜ íŒŒí‹°ì…˜ í‚¤ - ì‹œê°„ëŒ€ë³„ ë¶„ì‚°"""
        minute_bucket = (timestamp.minute // (60 // buckets))
        return f"{timestamp.hour:02d}_{minute_bucket:02d}"
    
    @staticmethod
    def composite_key(user_id, category):
        """ë³µí•© íŒŒí‹°ì…˜ í‚¤ - ì‚¬ìš©ì+ì¹´í…Œê³ ë¦¬ë³„ ìˆœì„œ ë³´ì¥"""
        return f"{user_id}_{category}"
    
    @staticmethod
    def random_key():
        """ì™„ì „ ëœë¤ í‚¤ - ìµœëŒ€ ë¶„ì‚°"""
        return str(uuid.uuid4())

# íŒŒí‹°ì…˜ í‚¤ ë¶„ì‚° í…ŒìŠ¤íŠ¸
def test_partition_distribution(key_generator, sample_size=10000):
    """íŒŒí‹°ì…˜ í‚¤ ë¶„ì‚° í…ŒìŠ¤íŠ¸"""
    keys = {}
    
    for i in range(sample_size):
        if key_generator == PartitionKeyStrategy.user_based_key:
            key = key_generator(i % 100)  # 100ëª…ì˜ ì‚¬ìš©ì
        elif key_generator == PartitionKeyStrategy.time_based_key:
            key = key_generator(datetime.now())
        else:
            key = key_generator(i)
        
        # í•´ì‹œê°’ì„ ìƒ¤ë“œ ìˆ˜ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ë¡œ ë¶„ì‚° ê³„ì‚°
        shard_id = int(hashlib.md5(key.encode()).hexdigest(), 16) % 4
        keys[shard_id] = keys.get(shard_id, 0) + 1
    
    print(f"íŒŒí‹°ì…˜ í‚¤ ë¶„ì‚° ê²°ê³¼ ({key_generator.__name__}):")
    for shard_id, count in sorted(keys.items()):
        percentage = (count / sample_size) * 100
        print(f"  ìƒ¤ë“œ {shard_id}: {count:,} ({percentage:.1f}%)")
    
    # ë¶„ì‚° ê· ë“±ì„± ê³„ì‚° (í‘œì¤€í¸ì°¨)
    values = list(keys.values())
    mean = sum(values) / len(values)
    variance = sum((x - mean) ** 2 for x in values) / len(values)
    std_dev = variance ** 0.5
    cv = (std_dev / mean) * 100  # ë³€ë™ê³„ìˆ˜
    
    print(f"  ë¶„ì‚° ê· ë“±ì„±: {cv:.1f}% (ë‚®ì„ìˆ˜ë¡ ê· ë“±)")
    return cv

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
print("=== íŒŒí‹°ì…˜ í‚¤ ì „ëµë³„ ë¶„ì‚° í…ŒìŠ¤íŠ¸ ===")
test_partition_distribution(PartitionKeyStrategy.hash_based_key)
test_partition_distribution(PartitionKeyStrategy.user_based_key)
test_partition_distribution(PartitionKeyStrategy.random_key)
```

### 2. í•« ìƒ¤ë“œ ê°ì§€ ë° í•´ê²°
```python
class HotShardDetector:
    def __init__(self, stream_name):
        self.kinesis = boto3.client('kinesis')
        self.cloudwatch = boto3.client('cloudwatch')
        self.stream_name = stream_name
    
    def detect_hot_shards(self):
        """í•« ìƒ¤ë“œ ê°ì§€"""
        
        # ê° ìƒ¤ë“œë³„ ë©”íŠ¸ë¦­ ì¡°íšŒ
        stream_desc = self.kinesis.describe_stream(StreamName=self.stream_name)
        shards = stream_desc['StreamDescription']['Shards']
        
        shard_metrics = {}
        
        for shard in shards:
            if shard.get('SequenceNumberRange', {}).get('EndingSequenceNumber'):
                continue  # ë¹„í™œì„± ìƒ¤ë“œ ì œì™¸
            
            shard_id = shard['ShardId']
            
            # ìƒ¤ë“œë³„ ì²˜ë¦¬ëŸ‰ ì¡°íšŒ
            response = self.cloudwatch.get_metric_statistics(
                Namespace='AWS/Kinesis',
                MetricName='IncomingBytes',
                Dimensions=[
                    {'Name': 'StreamName', 'Value': self.stream_name},
                    {'Name': 'ShardId', 'Value': shard_id}
                ],
                StartTime=datetime.utcnow() - timedelta(minutes=10),
                EndTime=datetime.utcnow(),
                Period=300,  # 5ë¶„ ë‹¨ìœ„
                Statistics=['Average']
            )
            
            if response['Datapoints']:
                avg_bytes = sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])
                shard_metrics[shard_id] = avg_bytes / 1024 / 1024  # MB/ì´ˆ
        
        if not shard_metrics:
            return []
        
        # í‰ê·  ëŒ€ë¹„ 2ë°° ì´ìƒì¸ ìƒ¤ë“œë¥¼ í•« ìƒ¤ë“œë¡œ íŒë‹¨
        avg_throughput = sum(shard_metrics.values()) / len(shard_metrics)
        hot_threshold = avg_throughput * 2
        
        hot_shards = [
            (shard_id, throughput) 
            for shard_id, throughput in shard_metrics.items() 
            if throughput > hot_threshold
        ]
        
        return hot_shards
    
    def suggest_partition_key_fix(self, hot_shards):
        """íŒŒí‹°ì…˜ í‚¤ ê°œì„  ì œì•ˆ"""
        
        if not hot_shards:
            print("í•« ìƒ¤ë“œê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ")
            return
        
        print("ğŸ”¥ í•« ìƒ¤ë“œ ê°ì§€ë¨!")
        for shard_id, throughput in hot_shards:
            print(f"  ìƒ¤ë“œ {shard_id}: {throughput:.2f} MB/ì´ˆ")
        
        print("\nğŸ’¡ ê°œì„  ì œì•ˆ:")
        print("1. íŒŒí‹°ì…˜ í‚¤ì— ëœë¤ ìš”ì†Œ ì¶”ê°€")
        print("   ex) user_id -> user_id + random_suffix")
        
        print("\n2. ë³µí•© íŒŒí‹°ì…˜ í‚¤ ì‚¬ìš©")
        print("   ex) user_id -> user_id + timestamp_bucket")
        
        print("\n3. í•´ì‹œ ê¸°ë°˜ íŒŒí‹°ì…˜ í‚¤ë¡œ ë³€ê²½")
        print("   ex) md5(user_id + event_type)")

# ì‚¬ìš© ì˜ˆì‹œ
detector = HotShardDetector('my-stream')
hot_shards = detector.detect_hot_shards()
detector.suggest_partition_key_fix(hot_shards)
```

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
import boto3
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

class OptimizedKinesisProducer:
    def __init__(self, stream_name, max_workers=10):
        self.kinesis = boto3.client('kinesis')
        self.stream_name = stream_name
        self.max_workers = max_workers
        self.batch_size = 500  # ë°°ì¹˜ë‹¹ ë ˆì½”ë“œ ìˆ˜
        
    def put_records_batch(self, records):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë ˆì½”ë“œ ì „ì†¡"""
        
        # Kinesis put_recordsëŠ” ìµœëŒ€ 500ê°œ ë ˆì½”ë“œê¹Œì§€ ì§€ì›
        batch_records = []
        for i, record in enumerate(records):
            batch_records.append({
                'Data': json.dumps(record['data']) if isinstance(record['data'], dict) else record['data'],
                'PartitionKey': record['partition_key']
            })
            
            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ë ˆì½”ë“œì¸ ê²½ìš° ì „ì†¡
            if len(batch_records) == self.batch_size or i == len(records) - 1:
                try:
                    response = self.kinesis.put_records(
                        Records=batch_records,
                        StreamName=self.stream_name
                    )
                    
                    # ì‹¤íŒ¨í•œ ë ˆì½”ë“œ í™•ì¸
                    failed_count = response['FailedRecordCount']
                    if failed_count > 0:
                        print(f"ë°°ì¹˜ ì „ì†¡ ì¤‘ {failed_count}ê°œ ë ˆì½”ë“œ ì‹¤íŒ¨")
                        
                        # ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¬ì‹œë„
                        failed_records = []
                        for j, record_result in enumerate(response['Records']):
                            if 'ErrorCode' in record_result:
                                failed_records.append(batch_records[j])
                        
                        if failed_records:
                            self._retry_failed_records(failed_records)
                    
                    print(f"ë°°ì¹˜ ì „ì†¡ ì™„ë£Œ: {len(batch_records) - failed_count}ê°œ ì„±ê³µ")
                    
                except Exception as e:
                    print(f"ë°°ì¹˜ ì „ì†¡ ì˜¤ë¥˜: {e}")
                
                batch_records = []  # ë°°ì¹˜ ì´ˆê¸°í™”
    
    def _retry_failed_records(self, failed_records, max_retries=3):
        """ì‹¤íŒ¨í•œ ë ˆì½”ë“œ ì¬ì‹œë„"""
        
        for attempt in range(max_retries):
            if not failed_records:
                break
                
            print(f"ì¬ì‹œë„ {attempt + 1}/{max_retries}: {len(failed_records)}ê°œ ë ˆì½”ë“œ")
            
            try:
                response = self.kinesis.put_records(
                    Records=failed_records,
                    StreamName=self.stream_name
                )
                
                # ì—¬ì „íˆ ì‹¤íŒ¨í•œ ë ˆì½”ë“œë§Œ ë‹¤ìŒ ì¬ì‹œë„ì— í¬í•¨
                still_failed = []
                for i, record_result in enumerate(response['Records']):
                    if 'ErrorCode' in record_result:
                        still_failed.append(failed_records[i])
                
                failed_records = still_failed
                
                # ì§€ìˆ˜ ë°±ì˜¤í”„
                if failed_records and attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    
            except Exception as e:
                print(f"ì¬ì‹œë„ {attempt + 1} ì˜¤ë¥˜: {e}")
                time.sleep(2 ** attempt)
        
        if failed_records:
            print(f"ìµœì¢… ì‹¤íŒ¨: {len(failed_records)}ê°œ ë ˆì½”ë“œ")
    
    def send_data_parallel(self, all_records):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë°ì´í„° ì „ì†¡"""
        
        # ë ˆì½”ë“œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = [
            all_records[i:i + self.batch_size] 
            for i in range(0, len(all_records), self.batch_size)
        ]
        
        print(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬")
        
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_chunk = {
                executor.submit(self.put_records_batch, chunk): chunk 
                for chunk in chunks
            }
            
            completed = 0
            for future in as_completed(future_to_chunk):
                try:
                    future.result()
                    completed += 1
                    print(f"ì§„í–‰ë¥ : {completed}/{len(chunks)} ({completed/len(chunks)*100:.1f}%)")
                except Exception as e:
                    print(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
def generate_sample_records(count=10000):
    """ìƒ˜í”Œ ë ˆì½”ë“œ ìƒì„±"""
    import random
    
    records = []
    for i in range(count):
        records.append({
            'data': {
                'user_id': random.randint(1, 1000),
                'event_type': random.choice(['click', 'view', 'purchase']),
                'timestamp': int(time.time()),
                'value': random.uniform(10, 500)
            },
            'partition_key': PartitionKeyStrategy.hash_based_key(f"user_{random.randint(1, 1000)}")
        })
    
    return records

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
producer = OptimizedKinesisProducer('test-stream', max_workers=5)
sample_records = generate_sample_records(5000)

start_time = time.time()
producer.send_data_parallel(sample_records)
end_time = time.time()

print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
print(f"ì²˜ë¦¬ìœ¨: {len(sample_records)/(end_time - start_time):.0f} ë ˆì½”ë“œ/ì´ˆ")
```

## ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ

### 1. ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
```python
import boto3
import json

def create_kinesis_dashboard(stream_name):
    """Kinesis ìŠ¤íŠ¸ë¦¼ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    dashboard_body = {
        "widgets": [
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/Kinesis", "IncomingRecords", "StreamName", stream_name],
                        [".", "IncomingBytes", ".", "."],
                        [".", "OutgoingRecords", ".", "."],
                        [".", "OutgoingBytes", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Sum", 
                    "region": "us-east-1",
                    "title": "ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ëŸ‰"
                }
            },
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/Kinesis", "WriteProvisionedThroughputExceeded", "StreamName", stream_name],
                        [".", "ReadProvisionedThroughputExceeded", ".", "."],
                        [".", "UserRecordsPending", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Sum",
                    "region": "us-east-1", 
                    "title": "ì˜¤ë¥˜ ë° ì œí•œ"
                }
            }
        ]
    }
    
    response = cloudwatch.put_dashboard(
        DashboardName=f'Kinesis-{stream_name}',
        DashboardBody=json.dumps(dashboard_body)
    )
    
    print(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ: Kinesis-{stream_name}")
    return response

def setup_kinesis_alarms(stream_name, sns_topic_arn):
    """Kinesis ì•ŒëŒ ì„¤ì •"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    alarms = [
        {
            'name': f'{stream_name}-HighIncomingRecords',
            'description': 'ë†’ì€ ìˆ˜ì‹  ë ˆì½”ë“œ ìˆ˜',
            'metric_name': 'IncomingRecords',
            'threshold': 10000,
            'comparison': 'GreaterThanThreshold'
        },
        {
            'name': f'{stream_name}-WriteThrottling',
            'description': 'ì“°ê¸° ìŠ¤ë¡œí‹€ë§ ë°œìƒ',
            'metric_name': 'WriteProvisionedThroughputExceeded',
            'threshold': 0,
            'comparison': 'GreaterThanThreshold'
        },
        {
            'name': f'{stream_name}-ReadThrottling',
            'description': 'ì½ê¸° ìŠ¤ë¡œí‹€ë§ ë°œìƒ',
            'metric_name': 'ReadProvisionedThroughputExceeded',
            'threshold': 0,
            'comparison': 'GreaterThanThreshold'
        }
    ]
    
    for alarm in alarms:
        cloudwatch.put_metric_alarm(
            AlarmName=alarm['name'],
            AlarmDescription=alarm['description'],
            ComparisonOperator=alarm['comparison'],
            EvaluationPeriods=2,
            MetricName=alarm['metric_name'],
            Namespace='AWS/Kinesis',
            Period=300,
            Statistic='Sum',
            Threshold=alarm['threshold'],
            ActionsEnabled=True,
            AlarmActions=[sns_topic_arn],
            Dimensions=[
                {
                    'Name': 'StreamName',
                    'Value': stream_name
                }
            ]
        )
        
        print(f"ì•ŒëŒ ìƒì„±: {alarm['name']}")

# ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤í–‰
create_kinesis_dashboard('my-stream')
setup_kinesis_alarms('my-stream', 'arn:aws:sns:us-east-1:123456789012:kinesis-alerts')
```

## ê¶Œì¥ì‚¬í•­

### ìƒ¤ë“œ ê´€ë¦¬ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
* **ì ì ˆí•œ ìƒ¤ë“œ ìˆ˜**: ì²˜ë¦¬ëŸ‰ê³¼ ë¹„ìš©ì˜ ê· í˜•ì  ì°¾ê¸°
* **íŒŒí‹°ì…˜ í‚¤ ì„¤ê³„**: ê· ë“± ë¶„ì‚°ê³¼ ìˆœì„œ ë³´ì¥ ìš”êµ¬ì‚¬í•­ ê³ ë ¤
* **ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ì„±ëŠ¥ ë° ë¹„ìš© ì¶”ì 
* **ìë™í™”**: ìŠ¤ì¼€ì¼ë§ ë° ì•ŒëŒ ìë™í™”ë¡œ ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ

### ì£¼ì˜ì‚¬í•­
* **ìƒ¤ë“œ ë¶„í• /ë³‘í•© ì‹œê°„**: ì™„ë£Œê¹Œì§€ ìˆ˜ ë¶„ ì†Œìš”
* **ìˆœì„œ ë³´ì¥**: íŒŒí‹°ì…˜ í‚¤ê°€ ê°™ì€ ë ˆì½”ë“œë§Œ ìˆœì„œ ë³´ì¥
* **ì²˜ë¦¬ëŸ‰ ì œí•œ**: ìƒ¤ë“œë‹¹ 1MB/ì´ˆ ë˜ëŠ” 1000 ë ˆì½”ë“œ/ì´ˆ
* **ë¹„ìš© ìµœì í™”**: ë¶ˆí•„ìš”í•œ ìƒ¤ë“œëŠ” ì¦‰ì‹œ ë³‘í•©
