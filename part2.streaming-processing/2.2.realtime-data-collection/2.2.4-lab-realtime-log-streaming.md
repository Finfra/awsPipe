# 2.2.4. 실습: 실시간 로그 데이터 스트리밍

## 실습 목표
* 웹 서버 로그와 애플리케이션 로그를 실시간으로 Kinesis로 전송
* 다양한 로그 형식 파싱 및 표준 형식으로 변환
* 실시간 로그 모니터링 및 성능 측정

## 사전 준비

### 필요한 AWS 서비스
* **Kinesis Data Streams**: 로그 스트리밍 수집
* **EC2 인스턴스**: 로그 생성 및 스트리밍 에이전트 실행
* **CloudWatch**: 로그 모니터링 및 메트릭

### 로컬 환경 설정
```bash
# Python 의존성 설치
pip install boto3 

# AWS CLI 설정 확인
aws configure list
aws kinesis list-streams
```

## Step 1: Kinesis 스트림 생성

### 스트림 생성
```bash
# 로그 스트리밍용 Kinesis 스트림 생성
aws kinesis create-stream \
    --stream-name log-streaming-demo \
    --shard-count 2

# 스트림 상태 확인
aws kinesis describe-stream \
    --stream-name log-streaming-demo
```

### 스트림 활성화 대기
```bash
# 스트림이 ACTIVE 상태가 될 때까지 대기
while [ "$(aws kinesis describe-stream --stream-name log-streaming-demo --query 'StreamDescription.StreamStatus' --output text)" != "ACTIVE" ]; do
    echo "스트림 활성화 대기 중..."
    sleep 10
done
echo "스트림 준비 완료!"
```

## Step 2: 로그 파일 준비

### 테스트 로그 디렉토리 생성
```bash
mkdir -p /tmp/streaming-logs/{apache,nginx,application}
```

### Apache 로그 형식 예시
```bash
# /tmp/streaming-logs/apache/access.log
192.168.1.100 - - [15/Jan/2024:10:30:45 +0000] "GET /index.html HTTP/1.1" 200 2435 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
10.0.0.50 - - [15/Jan/2024:10:30:46 +0000] "POST /api/users HTTP/1.1" 201 156 "https://example.com/signup" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"
```

### Nginx 로그 형식 예시
```bash
# /tmp/streaming-logs/nginx/access.log  
192.168.1.101 - - [15/Jan/2024:10:31:00 +0000] "GET /api/orders HTTP/1.1" 200 1024 "https://app.example.com" "curl/7.68.0"
203.0.113.10 - - [15/Jan/2024:10:31:01 +0000] "DELETE /api/orders/123 HTTP/1.1" 204 0 "-" "axios/0.21.1"
```

### 애플리케이션 로그 형식 (JSON)
```bash
# /tmp/streaming-logs/application/app.log
{"timestamp": "2024-01-15T10:30:45.123Z", "level": "INFO", "logger": "com.example.UserService", "message": "User login successful", "user_id": 12345, "request_id": "req-001"}
{"timestamp": "2024-01-15T10:30:46.456Z", "level": "WARN", "logger": "com.example.OrderService", "message": "Order processing delayed", "order_id": 67890, "request_id": "req-002"}
```

## Step 3: 로그 스트리머 실행

### 기본 스트리밍 실행
```bash
# 로그 파일 설정
LOG_CONFIG='[
    {
        "path": "/tmp/streaming-logs/apache/access.log",
        "type": "apache"
    },
    {
        "path": "/tmp/streaming-logs/nginx/access.log", 
        "type": "nginx"
    },
    {
        "path": "/tmp/streaming-logs/application/app.log",
        "type": "application"
    }
]'

# 스트리머 실행
python streaming-log-data.py \
    --stream-name log-streaming-demo \
    --region us-east-1 \
    --log-files "$LOG_CONFIG"
```

### 테스트 로그 생성과 함께 실행
```bash
# 테스트 로그 자동 생성 모드
python streaming-log-data.py \
    --stream-name log-streaming-demo \
    --region us-east-1 \
    --log-files "$LOG_CONFIG" \
    --generate-test-logs
```

## Step 4: 실시간 로그 모니터링

### Kinesis 레코드 확인
```bash
# 스트림에서 레코드 읽기
SHARD_ITERATOR=$(aws kinesis get-shard-iterator \
    --stream-name log-streaming-demo \
    --shard-id shardId-000000000000 \
    --shard-iterator-type LATEST \
    --query 'ShardIterator' --output text)

# 레코드 가져오기
aws kinesis get-records \
    --shard-iterator $SHARD_ITERATOR \
    --query 'Records[*].Data' \
    --output text | base64 -d
```

### CloudWatch 메트릭 확인
```bash
# Kinesis 스트림 메트릭 조회
aws cloudwatch get-metric-statistics \
    --namespace AWS/Kinesis \
    --metric-name IncomingRecords \
    --dimensions Name=StreamName,Value=log-streaming-demo \
    --start-time $(date -u -d '10 minutes ago' +%Y-%m-%dT%H:%M:%S) \
    --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
    --period 300 \
    --statistics Sum
```

## Step 5: 로그 파싱 검증

### 파싱된 데이터 구조 확인
```python
# Python에서 직접 확인
import json

# Apache 로그 파싱 결과 예시
apache_parsed = {
    'timestamp': '2024-01-15T10:30:45+00:00',
    'ip_address': '192.168.1.100',
    'method': 'GET',
    'path': '/index.html',
    'protocol': 'HTTP/1.1',
    'status_code': 200,
    'response_size': 2435,
    'referer': None,
    'user_agent': 'Mozilla/5.0...',
    'log_type': 'apache_access'
}

print(json.dumps(apache_parsed, indent=2))
```

### 로그 타입별 필드 매핑

| 로그 타입 | 공통 필드 | 특화 필드 |
|---------|----------|----------|
| Apache | timestamp, ip_address, log_type | method, path, status_code, response_size |
| Nginx | timestamp, ip_address, log_type | method, path, status_code, response_size |
| Application | timestamp, log_type | level, logger, message, user_id, request_id |

## Step 6: 성능 측정 및 최적화

### 스트리밍 통계 확인
```bash
# 스트리밍 중 통계 로그 예시
=== 스트리밍 통계 ===
처리된 라인: 1,250
전송된 레코드: 1,180
오류: 12
처리율: 25.3 라인/초
전송율: 23.8 레코드/초
큐 크기: 15
```

### 성능 튜닝 파라미터
```python
# streaming-log-data.py 설정 최적화
streamer_config = {
    'batch_size': 500,        # 배치 크기 (기본: 500)
    'batch_timeout': 5,       # 배치 타임아웃 (초)
    'queue_maxsize': 1000,    # 내부 큐 최대 크기
    'parse_workers': 4        # 파싱 워커 스레드 수
}
```

## Step 7: 오류 처리 및 복구

### 일반적인 오류 상황
* **파일 접근 권한 오류**
    ```bash
    chmod 644 /tmp/streaming-logs/*/access.log
    ```
* **AWS 권한 오류**
    ```bash
    # Kinesis 권한 확인
    aws iam get-user-policy --user-name streaming-user --policy-name KinesisAccess
    ```
* **네트워크 연결 오류**
    ```bash
    # AWS 연결 테스트
    aws kinesis list-streams --region us-east-1
    ```

### 재시작 및 복구
```bash
# 스트리머 프로세스 강제 종료
pkill -f streaming-log-data.py

# 안전한 재시작
python streaming-log-data.py \
    --stream-name log-streaming-demo \
    --log-files "$LOG_CONFIG" \
    --resume-from-checkpoint
```

## Step 8: 실시간 데이터 검증

### Kinesis Analytics를 통한 실시간 집계
```sql
-- 실시간 집계 쿼리 예시
CREATE STREAM aggregated_logs AS 
SELECT 
    log_type,
    COUNT(*) as event_count,
    COUNT(DISTINCT ip_address) as unique_ips,
    AVG(response_size) as avg_response_size
FROM SOURCE_SQL_STREAM_001
WHERE log_type IS NOT NULL
GROUP BY log_type, 
         ROWTIME RANGE INTERVAL '1' MINUTE;
```

### 실시간 대시보드 데이터
* **초당 로그 처리량**: 평균 25-30 라인/초
* **파싱 성공률**: 95% 이상
* **전송 지연시간**: 평균 2-3초
* **오류율**: 1% 미만

## 문제 해결 가이드

### 파싱 실패 문제
```python
# 파싱 실패한 로그 라인 디버깅
def debug_parsing_failure(log_line, log_type):
    print(f"Failed to parse {log_type} log: {log_line}")
    # 정규식 패턴 확인
    # 로그 형식 맞는지 검증
```

### 메모리 사용량 최적화
```python
# 메모리 효율적인 파일 읽기
def efficient_tail_file(file_path):
    # 버퍼 크기 조정
    # 큐 크기 제한
    # 가비지 컬렉션 최적화
```

### 배치 전송 최적화
```python
# 최적 배치 크기 자동 조정
def adaptive_batch_size():
    # 처리량 기반 동적 조정
    # 지연시간 임계값 관리
    # 오류율 기반 백오프
```

## 실습 결과 확인

### 성공 기준
- [ ] 3가지 로그 타입 모두 성공적으로 파싱
- [ ] Kinesis에 초당 20개 이상 레코드 전송  
- [ ] 파싱 성공률 95% 이상 달성
- [ ] 오류 처리 및 복구 메커니즘 동작 확인

### 추가 실습 과제
* **로그 로테이션 처리**: logrotate와 연동하여 로그 파일 회전 처리
* **다중 서버 로그**: 여러 서버의 로그를 통합 스트리밍
* **실시간 알람**: 특정 로그 패턴 감지 시 즉시 알림

## 정리 및 리소스 정리

### 리소스 정리
```bash
# Kinesis 스트림 삭제
aws kinesis delete-stream --stream-name log-streaming-demo

# 테스트 파일 정리
rm -rf /tmp/streaming-logs

# 프로세스 종료
pkill -f streaming-log-data.py
```

### 다음 단계
* **2.2.5 실습**: Kinesis → S3 → EMR 파이프라인 구축
* **2.3.1**: Kinesis Data Firehose 변환 기능 활용
* **3.2**: CloudWatch 기반 로그 모니터링 설정
