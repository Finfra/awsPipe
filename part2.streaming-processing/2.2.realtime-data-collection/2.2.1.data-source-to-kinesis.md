# 2.2.1 다양한 소스에서 Kinesis로 데이터 전송

## 개요
실시간 데이터 파이프라인에서는 다양한 소스로부터 데이터를 수집해야 합니다. 웹 로그, IoT 센서, 데이터베이스 변경 사항, 애플리케이션 이벤트 등 서로 다른 특성을 가진 데이터 소스들을 효율적으로 Kinesis로 통합하는 방법을 다룹니다.

## 데이터 소스 유형별 전략

### 1. 웹 애플리케이션 로그
웹 서버와 애플리케이션에서 생성되는 로그는 높은 빈도로 발생하며 실시간 처리가 중요합니다.

#### Apache/Nginx 액세스 로그
```python
import re
import json
from datetime import datetime

class WebLogParser:
    def __init__(self):
        # Combined 로그 형식 정규식
        self.log_pattern = re.compile(
            r'(\S+) \S+ \S+ \[([\w:/]+\s[+\-]\d{4})\] '
            r'"(\S+) (\S+) (\S+)" (\d{3}) (\d+|-) "([^"]*)" "([^"]*)"'
        )
    
    def parse_log_line(self, log_line):
        """로그 라인을 구조화된 데이터로 변환"""
        match = self.log_pattern.match(log_line)
        if not match:
            return None
        
        ip, timestamp, method, path, protocol, status, size, referer, user_agent = match.groups()
        
        return {
            'timestamp': self.convert_timestamp(timestamp),
            'ip_address': ip,
            'method': method,
            'path': path,
            'protocol': protocol,
            'status_code': int(status),
            'response_size': int(size) if size.isdigit() else 0,
            'referer': referer if referer != '-' else None,
            'user_agent': user_agent,
            'log_type': 'web_access'
        }
    
    def convert_timestamp(self, timestamp_str):
        """로그 타임스탬프를 ISO 형식으로 변환"""
        try:
            dt = datetime.strptime(timestamp_str, '%d/%b/%Y:%H:%M:%S %z')
            return dt.isoformat()
        except:
            return datetime.utcnow().isoformat()

# 사용 예시
parser = WebLogParser()
log_line = '192.168.1.1 - - [26/May/2024:10:30:45 +0000] "GET /api/users HTTP/1.1" 200 1234 "https://example.com" "Mozilla/5.0"'
parsed = parser.parse_log_line(log_line)
print(json.dumps(parsed, indent=2))
```

### 2. 데이터베이스 변경 데이터 캡처 (CDC)
```python
class DatabaseChangeCapture:
    def __init__(self):
        self.operation_types = ['INSERT', 'UPDATE', 'DELETE']
        self.tables = {
            'users': {
                'primary_key': 'user_id',
                'fields': ['user_id', 'username', 'email', 'created_at', 'status']
            },
            'orders': {
                'primary_key': 'order_id', 
                'fields': ['order_id', 'user_id', 'total_amount', 'status', 'created_at']
            }
        }
    
    def create_change_event(self, table_name: str, operation: str, before_data: dict = None, after_data: dict = None):
        """데이터베이스 변경 이벤트 생성"""
        
        change_event = {
            'timestamp': datetime.utcnow().isoformat(),
            'database': 'ecommerce_db',
            'table': table_name,
            'operation': operation,
            'transaction_id': self.generate_transaction_id(),
            'before': before_data if operation in ['UPDATE', 'DELETE'] else None,
            'after': after_data if operation in ['INSERT', 'UPDATE'] else None,
            'change_type': self.get_change_type(operation, before_data, after_data)
        }
        
        return change_event
    
    def generate_transaction_id(self):
        """트랜잭션 ID 생성"""
        import uuid
        return str(uuid.uuid4())
    
    def get_change_type(self, operation, before_data, after_data):
        """변경 유형 분류"""
        if operation == 'INSERT':
            return 'new_record'
        elif operation == 'DELETE':
            return 'record_deletion'
        elif operation == 'UPDATE':
            # 중요 필드 변경 여부 확인
            if before_data and after_data:
                critical_fields = ['status', 'email', 'total_amount']
                for field in critical_fields:
                    if field in before_data and field in after_data:
                        if before_data[field] != after_data[field]:
                            return 'critical_update'
            return 'standard_update'
        return 'unknown'

# 사용 예시
cdc = DatabaseChangeCapture()
change_event = cdc.create_change_event(
    table_name='orders',
    operation='UPDATE',
    before_data={'order_id': 12345, 'status': 'pending', 'total_amount': 100.0},
    after_data={'order_id': 12345, 'status': 'completed', 'total_amount': 100.0}
)
print(json.dumps(change_event, indent=2))
```

## 통합 데이터 전송 클래스

### MultiSourceKinesisProducer
```python
import boto3
import json
import time
import threading
from queue import Queue, Empty
from datetime import datetime
import logging

class MultiSourceKinesisProducer:
    """다중 소스 데이터를 Kinesis로 전송하는 통합 클래스"""
    
    def __init__(self, stream_name, region='us-east-1', batch_size=500):
        self.kinesis = boto3.client('kinesis', region_name=region)
        self.stream_name = stream_name
        self.batch_size = batch_size
        
        # 데이터 큐
        self.data_queue = Queue(maxsize=10000)
        
        # 통계
        self.stats = {
            'total_sent': 0,
            'total_failed': 0,
            'by_source': {},
            'start_time': time.time()
        }
        
        # 스레드 관리
        self.running = False
        self.sender_thread = None
        
        # 로깅 설정
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def start(self):
        """배치 전송 스레드 시작"""
        self.running = True
        self.sender_thread = threading.Thread(target=self._batch_sender)
        self.sender_thread.daemon = True
        self.sender_thread.start()
        self.logger.info("Kinesis producer started")
    
    def stop(self):
        """전송 스레드 중지"""
        self.running = False
        if self.sender_thread:
            self.sender_thread.join(timeout=30)
        self.logger.info("Kinesis producer stopped")
    
    def send_data(self, data, data_source, partition_key=None):
        """데이터를 큐에 추가"""
        try:
            # 파티션 키 자동 생성
            if not partition_key:
                partition_key = self._generate_partition_key(data, data_source)
            
            # 메타데이터 추가
            enriched_data = {
                **data,
                'source': data_source,
                'ingestion_timestamp': datetime.utcnow().isoformat()
            }
            
            record = {
                'data': enriched_data,
                'partition_key': partition_key,
                'source': data_source
            }
            
            self.data_queue.put(record, timeout=1)
            
        except Exception as e:
            self.logger.error(f"Failed to queue data: {e}")
    
    def _generate_partition_key(self, data, data_source):
        """데이터 소스별 파티션 키 생성"""
        if data_source == 'web_logs':
            return data.get('session_id', data.get('ip_address', 'default'))
        elif data_source == 'iot_sensors':
            return data.get('device_id', 'unknown_device')
        elif data_source == 'database_cdc':
            return f"{data.get('table', 'unknown')}_{data.get('transaction_id', 'unknown')}"
        else:
            return f"{data_source}_{hash(str(data)) % 1000}"
    
    def _batch_sender(self):
        """배치 전송 메인 루프"""
        batch = []
        last_send_time = time.time()
        
        while self.running:
            try:
                # 큐에서 데이터 가져오기
                try:
                    record = self.data_queue.get(timeout=1)
                    batch.append(record)
                except Empty:
                    pass
                
                current_time = time.time()
                
                # 배치 크기 또는 시간 기준으로 전송
                should_send = (
                    len(batch) >= self.batch_size or
                    (batch and (current_time - last_send_time) >= 5)  # 5초 타임아웃
                )
                
                if should_send:
                    self._send_batch(batch)
                    batch = []
                    last_send_time = current_time
                    
            except Exception as e:
                self.logger.error(f"Batch sender error: {e}")
        
        # 종료 시 남은 배치 전송
        if batch:
            self._send_batch(batch)
    
    def _send_batch(self, batch):
        """배치 데이터 전송"""
        if not batch:
            return
        
        try:
            # Kinesis 레코드 형식으로 변환
            kinesis_records = []
            for record in batch:
                kinesis_records.append({
                    'Data': json.dumps(record['data']),
                    'PartitionKey': record['partition_key']
                })
            
            # 배치 전송
            response = self.kinesis.put_records(
                Records=kinesis_records,
                StreamName=self.stream_name
            )
            
            # 통계 업데이트
            success_count = len(kinesis_records) - response['FailedRecordCount']
            self.stats['total_sent'] += success_count
            self.stats['total_failed'] += response['FailedRecordCount']
            
            # 소스별 통계
            for record in batch:
                source = record['source']
                if source not in self.stats['by_source']:
                    self.stats['by_source'][source] = {'sent': 0, 'failed': 0}
                
                self.stats['by_source'][source]['sent'] += 1
            
            if response['FailedRecordCount'] > 0:
                self.logger.warning(f"Batch send: {success_count} success, {response['FailedRecordCount']} failed")
                # 실패한 레코드 재시도 로직 추가 가능
            else:
                self.logger.debug(f"Batch send successful: {success_count} records")
                
        except Exception as e:
            self.logger.error(f"Failed to send batch: {e}")
            self.stats['total_failed'] += len(batch)
    
    def get_stats(self):
        """통계 정보 반환"""
        elapsed_time = time.time() - self.stats['start_time']
        total_records = self.stats['total_sent'] + self.stats['total_failed']
        
        return {
            'total_sent': self.stats['total_sent'],
            'total_failed': self.stats['total_failed'],
            'success_rate': (self.stats['total_sent'] / max(total_records, 1)) * 100,
            'throughput_per_sec': total_records / max(elapsed_time, 1),
            'elapsed_time': elapsed_time,
            'queue_size': self.data_queue.qsize(),
            'by_source': self.stats['by_source']
        }
```

## 실제 사용 예시

### 통합 데이터 수집 시스템
```python
import time
import random
from concurrent.futures import ThreadPoolExecutor

def run_integrated_data_collection():
    """통합 데이터 수집 시스템 실행"""
    
    # Kinesis Producer 초기화
    producer = MultiSourceKinesisProducer('multi-source-stream')
    producer.start()
    
    # 데이터 생성기들 초기화
    web_parser = WebLogParser()
    iot_generator = IoTDataGenerator()
    cdc_processor = DatabaseChangeCapture()
    
    def simulate_web_logs():
        """웹 로그 시뮬레이션"""
        for i in range(100):
            # 가상 로그 라인 생성
            log_line = f'192.168.1.{i%255} - - [{datetime.now().strftime("%d/%b/%Y:%H:%M:%S +0000")}] "GET /api/data HTTP/1.1" 200 {random.randint(100, 5000)} "-" "Mozilla/5.0"'
            
            parsed_log = web_parser.parse_log_line(log_line)
            if parsed_log:
                producer.send_data(parsed_log, 'web_logs')
            
            time.sleep(0.1)
    
    def simulate_iot_data():
        """IoT 데이터 시뮬레이션"""
        devices = ['sensor_001', 'sensor_002', 'sensor_003']
        
        for i in range(100):
            for device_id in devices:
                sensor_data = iot_generator.generate_sensor_reading(
                    device_id=device_id,
                    device_type='temperature',
                    location=f'building_a_floor_{i%3+1}'
                )
                producer.send_data(sensor_data, 'iot_sensors')
            
            time.sleep(2)
    
    def simulate_database_changes():
        """데이터베이스 변경 시뮬레이션"""
        for i in range(50):
            # 주문 상태 변경 시뮬레이션
            change_event = cdc_processor.create_change_event(
                table_name='orders',
                operation='UPDATE',
                before_data={'order_id': 1000+i, 'status': 'pending'},
                after_data={'order_id': 1000+i, 'status': 'completed'}
            )
            producer.send_data(change_event, 'database_cdc')
            
            time.sleep(5)
    
    # 병렬 실행
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [
            executor.submit(simulate_web_logs),
            executor.submit(simulate_iot_data),
            executor.submit(simulate_database_changes)
        ]
        
        # 30초간 실행
        time.sleep(30)
    
    # 통계 출력
    stats = producer.get_stats()
    print("=== 전송 통계 ===")
    print(f"총 전송: {stats['total_sent']:,}")
    print(f"실패: {stats['total_failed']:,}")
    print(f"성공률: {stats['success_rate']:.1f}%")
    print(f"처리량: {stats['throughput_per_sec']:.1f} records/sec")
    print("\n소스별 통계:")
    for source, source_stats in stats['by_source'].items():
        print(f"  {source}: {source_stats['sent']} sent")
    
    # 정리
    producer.stop()

if __name__ == "__main__":
    run_integrated_data_collection()
```

## 데이터 변환 및 검증

### 스키마 검증
```python
from typing import Dict, List, Any
import jsonschema

class DataValidator:
    """데이터 스키마 검증"""
    
    def __init__(self):
        self.schemas = {
            'web_logs': {
                'type': 'object',
                'required': ['timestamp', 'ip_address', 'method', 'path', 'status_code'],
                'properties': {
                    'timestamp': {'type': 'string'},
                    'ip_address': {'type': 'string'},
                    'method': {'type': 'string'},
                    'path': {'type': 'string'},
                    'status_code': {'type': 'integer'},
                    'response_size': {'type': 'integer'},
                    'log_type': {'type': 'string'}
                }
            },
            'iot_sensors': {
                'type': 'object',
                'required': ['timestamp', 'device_id', 'device_type', 'value'],
                'properties': {
                    'timestamp': {'type': 'string'},
                    'device_id': {'type': 'string'},
                    'device_type': {'type': 'string'},
                    'value': {'type': 'number'},
                    'unit': {'type': 'string'},
                    'location': {'type': 'string'}
                }
            }
        }
    
    def validate(self, data: Dict[str, Any], data_source: str) -> tuple[bool, List[str]]:
        """데이터 검증"""
        if data_source not in self.schemas:
            return False, [f"Unknown data source: {data_source}"]
        
        try:
            jsonschema.validate(data, self.schemas[data_source])
            return True, []
        except jsonschema.ValidationError as e:
            return False, [str(e)]
        except Exception as e:
            return False, [f"Validation error: {e}"]

# 사용 예시
validator = DataValidator()
test_data = {
    'timestamp': '2024-05-26T10:30:00Z',
    'ip_address': '192.168.1.1',
    'method': 'GET',
    'path': '/api/test',
    'status_code': 200
}

is_valid, errors = validator.validate(test_data, 'web_logs')
if is_valid:
    print("✅ Data validation passed")
else:
    print(f"❌ Validation errors: {errors}")
```

## 성능 최적화 팁

### 1. 배치 크기 조정
- 작은 배치: 낮은 지연시간, 높은 API 호출 비용
- 큰 배치: 높은 처리량, 메모리 사용량 증가
- 권장: 500개 레코드 또는 5MB 중 먼저 도달하는 기준

### 2. 파티션 키 전략
```python
def optimize_partition_key(data, data_source):
    """최적화된 파티션 키 생성"""
    
    if data_source == 'web_logs':
        # 세션 ID가 있으면 사용, 없으면 IP + 시간 기반
        if 'session_id' in data:
            return data['session_id']
        else:
            # IP 주소와 시간을 조합하여 분산
            ip_hash = hash(data.get('ip_address', '')) % 1000
            hour = datetime.now().hour
            return f"ip_{ip_hash}_h{hour}"
    
    elif data_source == 'iot_sensors':
        # 디바이스 ID와 위치 조합
        device_id = data.get('device_id', 'unknown')
        location = data.get('location', 'unknown')
        return f"{device_id}_{hash(location) % 100}"
    
    return f"{data_source}_{random.randint(0, 999)}"
```

### 3. 에러 처리 및 재시도
```python
import time
from typing import Optional

class RetryHandler:
    """재시도 처리 클래스"""
    
    def __init__(self, max_retries=3, base_delay=1.0, max_delay=60.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
    
    def retry_with_backoff(self, func, *args, **kwargs):
        """지수 백오프를 사용한 재시도"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    break
                
                # 지수 백오프 계산
                delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay}s...")
                time.sleep(delay)
        
        raise last_exception

# 사용 예시
retry_handler = RetryHandler()

def send_with_retry(kinesis_client, stream_name, records):
    return retry_handler.retry_with_backoff(
        kinesis_client.put_records,
        Records=records,
        StreamName=stream_name
    )
```

## 모니터링 및 알림

### CloudWatch 메트릭 생성
```python
import boto3

class CustomMetrics:
    """커스텀 메트릭 전송"""
    
    def __init__(self, namespace='DataIngestion'):
        self.cloudwatch = boto3.client('cloudwatch')
        self.namespace = namespace
    
    def put_metric(self, metric_name, value, unit='Count', dimensions=None):
        """메트릭 전송"""
        try:
            metric_data = {
                'MetricName': metric_name,
                'Value': value,
                'Unit': unit
            }
            
            if dimensions:
                metric_data['Dimensions'] = [
                    {'Name': k, 'Value': v} for k, v in dimensions.items()
                ]
            
            self.cloudwatch.put_metric_data(
                Namespace=self.namespace,
                MetricData=[metric_data]
            )
        except Exception as e:
            print(f"Failed to put metric: {e}")
    
    def put_ingestion_metrics(self, stats):
        """데이터 수집 메트릭 전송"""
        # 전체 처리량
        self.put_metric('RecordsProcessed', stats['total_sent'])
        self.put_metric('RecordsFailed', stats['total_failed'])
        self.put_metric('SuccessRate', stats['success_rate'], 'Percent')
        
        # 소스별 메트릭
        for source, source_stats in stats['by_source'].items():
            self.put_metric(
                'RecordsProcessed', 
                source_stats['sent'],
                dimensions={'DataSource': source}
            )

# 사용 예시
metrics = CustomMetrics()
stats = producer.get_stats()
metrics.put_ingestion_metrics(stats)
```

## 권장사항

### 데이터 소스별 최적화
* **웹 로그**: 로그 파싱 성능 최적화, 세션 기반 파티셔닝
* **IoT 센서**: 디바이스별 배치 처리, 데이터 품질 검증
* **데이터베이스**: 트랜잭션 기반 그룹화, 중요도별 우선순위

### 운영 고려사항
* **백프레셔 처리**: 큐 크기 모니터링 및 제한
* **장애 복구**: DLQ(Dead Letter Queue) 활용
* **스키마 진화**: 하위 호환성 유지
* **비용 최적화**: 배치 크기와 전송 빈도 균형
