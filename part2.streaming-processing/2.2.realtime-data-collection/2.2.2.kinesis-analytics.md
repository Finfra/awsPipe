# 2.2.2 Kinesis Analytics를 통한 실시간 분석

## 개요
Amazon Kinesis Data Analytics는 실시간 스트리밍 데이터를 SQL 또는 Apache Flink를 사용하여 분석할 수 있는 완전 관리형 서비스입니다. 복잡한 인프라 관리 없이 실시간으로 데이터를 변환, 집계, 필터링할 수 있습니다.

## Kinesis Analytics 아키텍처

### 핵심 구성요소
```
┌─────────────────┐        ┌──────────────────┐        ┌─────────────────┐
│   Data Source                    │───▶│ Kinesis Analytics                  │───▶│  Destination                     │
│                                  │        │                                    │        │                                  │
│ • Kinesis                       │        │ • SQL Engine                      │        │ • Kinesis                       │
│ • S3                            │        │ • Flink Runtime                   │        │ • S3                            │  
│ • MSK                           │        │ • Lambda                          │        │ • Redshift                      │
└─────────────────┘        └──────────────────┘        └─────────────────┘    
```

## SQL 기반 실시간 분석

### 1. 기본 집계 분석
```sql
-- 입력 스트림 스키마 정의
CREATE OR REPLACE STREAM "SOURCE_SQL_STREAM_001" (
    timestamp         VARCHAR(32),
    user_id           INTEGER,
    event_type        VARCHAR(32),
    value             DOUBLE,
    session_id        VARCHAR(64),
    ip_address        VARCHAR(16),
    category          VARCHAR(32)
);

-- 5분 윈도우 기반 실시간 집계
CREATE OR REPLACE STREAM "REALTIME_METRICS" (
    window_start      TIMESTAMP,
    window_end        TIMESTAMP,
    event_type        VARCHAR(32),
    event_count       BIGINT,
    unique_users      BIGINT,
    total_value       DOUBLE,
    avg_value         DOUBLE
);

CREATE OR REPLACE PUMP "METRICS_PUMP" AS INSERT INTO "REALTIME_METRICS"
SELECT STREAM
    ROWTIME_TO_TIMESTAMP(ROWTIME) AS window_start,
    ROWTIME_TO_TIMESTAMP(ROWTIME + INTERVAL '5' MINUTE) AS window_end,
    event_type,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_id) AS unique_users,
    SUM(value) AS total_value,
    AVG(value) AS avg_value
FROM SOURCE_SQL_STREAM_001
WHERE value IS NOT NULL
GROUP BY 
    event_type,
    RANGE_TIMESTAMP(ROWTIME RANGE INTERVAL '5' MINUTE);
```

### 2. 이상 탐지 분석
```sql
-- 이동 평균 기반 이상값 탐지
CREATE OR REPLACE STREAM "ANOMALY_DETECTION_STREAM" (
    timestamp         VARCHAR(32),
    user_id           INTEGER,
    event_type        VARCHAR(32),
    current_value     DOUBLE,
    moving_average    DOUBLE,
    anomaly_score     DOUBLE,
    is_anomaly        BOOLEAN
);

CREATE OR REPLACE PUMP "ANOMALY_PUMP" AS INSERT INTO "ANOMALY_DETECTION_STREAM"
SELECT STREAM
    timestamp,
    user_id,
    event_type,
    value AS current_value,
    AVG(value) OVER (
        PARTITION BY event_type 
        RANGE INTERVAL '1' HOUR PRECEDING
    ) AS moving_average,
    ABS(value - AVG(value) OVER (
        PARTITION BY event_type 
        RANGE INTERVAL '1' HOUR PRECEDING
    )) / NULLIF(STDDEV_SAMP(value) OVER (
        PARTITION BY event_type 
        RANGE INTERVAL '1' HOUR PRECEDING
    ), 0) AS anomaly_score,
    CASE 
        WHEN ABS(value - AVG(value) OVER (
            PARTITION BY event_type 
            RANGE INTERVAL '1' HOUR PRECEDING
        )) / NULLIF(STDDEV_SAMP(value) OVER (
            PARTITION BY event_type 
            RANGE INTERVAL '1' HOUR PRECEDING
        ), 0) > 3 THEN true
        ELSE false
    END AS is_anomaly
FROM SOURCE_SQL_STREAM_001
WHERE value IS NOT NULL;
```

### 3. 복잡한 이벤트 패턴 분석
```sql
-- 사용자 행동 패턴 분석 (세션별)
CREATE OR REPLACE STREAM "USER_BEHAVIOR_PATTERN" (
    session_id        VARCHAR(64),
    user_id           INTEGER,
    event_sequence    VARCHAR(1024),
    session_duration  BIGINT,
    total_value       DOUBLE,
    conversion_flag   BOOLEAN
);

CREATE OR REPLACE PUMP "BEHAVIOR_PUMP" AS INSERT INTO "USER_BEHAVIOR_PATTERN"
SELECT STREAM
    session_id,
    user_id,
    LISTAGG(event_type, ' -> ') WITHIN GROUP (ORDER BY ROWTIME) AS event_sequence,
    MAX(ROWTIME) - MIN(ROWTIME) AS session_duration,
    SUM(value) AS total_value,
    CASE 
        WHEN LISTAGG(event_type, ' -> ') WITHIN GROUP (ORDER BY ROWTIME) 
             LIKE '%view%click%purchase%' THEN true
        ELSE false
    END AS conversion_flag
FROM SOURCE_SQL_STREAM_001
GROUP BY session_id, user_id, 
         RANGE_TIMESTAMP(ROWTIME RANGE INTERVAL '30' MINUTE);
```

## Apache Flink 기반 고급 분석

### 1. Flink 애플리케이션 기본 구조
```python
# Python Flink 애플리케이션 예시
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.table.window import Tumble
from pyflink.table.expressions import col, lit
import json

def create_kinesis_analytics_app():
    # 실행 환경 생성
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(2)
    
    # 테이블 환경 생성
    table_env = StreamTableEnvironment.create(env)
    
    # Kinesis 소스 테이블 정의
    source_ddl = """
    CREATE TABLE kinesis_source (
        event_time TIMESTAMP(3),
        user_id BIGINT,
        event_type STRING,
        value DOUBLE,
        session_id STRING,
        ip_address STRING,
        WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
    ) WITH (
        'connector' = 'kinesis',
        'stream' = 'input-stream',
        'aws.region' = 'us-east-1',
        'scan.stream.initpos' = 'LATEST',
        'format' = 'json'
    )
    """
    
    # 출력 테이블 정의  
    sink_ddl = """
    CREATE TABLE kinesis_sink (
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        event_type STRING,
        event_count BIGINT,
        unique_users BIGINT,
        avg_value DOUBLE
    ) WITH (
        'connector' = 'kinesis',
        'stream' = 'output-stream',
        'aws.region' = 'us-east-1',
        'format' = 'json'
    )
    """
    
    table_env.execute_sql(source_ddl)
    table_env.execute_sql(sink_ddl)
    
    # 분석 쿼리 실행
    analysis_query = """
    INSERT INTO kinesis_sink
    SELECT 
        TUMBLE_START(event_time, INTERVAL '5' MINUTE) as window_start,
        TUMBLE_END(event_time, INTERVAL '5' MINUTE) as window_end,
        event_type,
        COUNT(*) as event_count,
        COUNT(DISTINCT user_id) as unique_users,
        AVG(value) as avg_value
    FROM kinesis_source
    WHERE value IS NOT NULL
    GROUP BY 
        TUMBLE(event_time, INTERVAL '5' MINUTE),
        event_type
    """
    
    table_env.execute_sql(analysis_query)

if __name__ == "__main__":
    create_kinesis_analytics_app()
```

### 2. 상태 관리 및 복잡한 이벤트 처리
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types
from pyflink.common.time import Time
import json

class SessionAnalyzer(KeyedProcessFunction):
    """사용자 세션 분석을 위한 Flink 함수"""
    
    def __init__(self):
        self.session_state = None
        self.timer_state = None
    
    def open(self, runtime_context: RuntimeContext):
        # 세션 상태 초기화
        session_descriptor = ValueStateDescriptor(
            "session_info",
            Types.STRING()
        )
        self.session_state = runtime_context.get_state(session_descriptor)
        
        # 타이머 상태 초기화
        timer_descriptor = ValueStateDescriptor(
            "timer_timestamp", 
            Types.LONG()
        )
        self.timer_state = runtime_context.get_state(timer_descriptor)
    
    def process_element(self, value, ctx, out):
        """이벤트 처리"""
        current_time = ctx.timestamp()
        
        # 현재 세션 정보 조회
        current_session = self.session_state.value()
        if current_session:
            session_data = json.loads(current_session)
        else:
            session_data = {
                'start_time': current_time,
                'events': [],
                'total_value': 0
            }
        
        # 새 이벤트 추가
        event_data = json.loads(value)
        session_data['events'].append({
            'event_type': event_data['event_type'],
            'timestamp': current_time,
            'value': event_data.get('value', 0)
        })
        session_data['total_value'] += event_data.get('value', 0)
        
        # 세션 상태 업데이트
        self.session_state.update(json.dumps(session_data))
        
        # 세션 타임아웃 타이머 설정 (30분)
        timeout_time = current_time + 30 * 60 * 1000  # 30분
        ctx.timer_service().register_event_time_timer(timeout_time)
        self.timer_state.update(timeout_time)
    
    def on_timer(self, timestamp, ctx, out):
        """타이머 콜백 - 세션 종료"""
        session_data = json.loads(self.session_state.value())
        
        # 세션 요약 생성
        session_summary = {
            'user_id': ctx.get_current_key(),
            'session_start': session_data['start_time'],
            'session_end': timestamp,
            'duration_minutes': (timestamp - session_data['start_time']) / (60 * 1000),
            'event_count': len(session_data['events']),
            'total_value': session_data['total_value'],
            'event_types': list(set(e['event_type'] for e in session_data['events']))
        }
        
        # 결과 출력
        out.collect(json.dumps(session_summary))
        
        # 상태 정리
        self.session_state.clear()
        self.timer_state.clear()

def create_session_analysis_app():
    """세션 분석 애플리케이션"""
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(2)
    
    # Kinesis 소스 연결
    kinesis_source = env.add_source(
        # Kinesis 소스 구성
    )
    
    # 사용자별로 키 분할하여 세션 분석
    session_analysis = kinesis_source \
        .key_by(lambda x: json.loads(x)['user_id']) \
        .process(SessionAnalyzer())
    
    # 결과를 Kinesis로 출력
    session_analysis.add_sink(
        # Kinesis 싱크 구성
    )
    
    env.execute("Session Analysis Application")
```

## 실시간 대시보드 연동

### 1. Lambda를 통한 실시간 알림
```python
import boto3
import json
from datetime import datetime

def lambda_handler(event, context):
    """Kinesis Analytics 결과 처리"""
    
    sns = boto3.client('sns')
    cloudwatch = boto3.client('cloudwatch')
    
    for record in event['Records']:
        # Kinesis 레코드 디코딩
        payload = json.loads(record['kinesis']['data'])
        
        # 이상 탐지 결과 처리
        if payload.get('is_anomaly', False):
            # SNS 알림 전송
            alert_message = {
                'timestamp': datetime.utcnow().isoformat(),
                'alert_type': 'ANOMALY_DETECTED',
                'event_type': payload.get('event_type'),
                'anomaly_score': payload.get('anomaly_score'),
                'current_value': payload.get('current_value'),
                'expected_range': payload.get('moving_average')
            }
            
            sns.publish(
                TopicArn='arn:aws:sns:us-east-1:123456789012:anomaly-alerts',
                Message=json.dumps(alert_message),
                Subject=f'Anomaly Detected: {payload.get("event_type")}'
            )
        
        # CloudWatch 커스텀 메트릭 전송
        if 'event_count' in payload:
            cloudwatch.put_metric_data(
                Namespace='RealTimeAnalytics',
                MetricData=[
                    {
                        'MetricName': 'EventCount',
                        'Value': payload['event_count'],
                        'Unit': 'Count',
                        'Dimensions': [
                            {
                                'Name': 'EventType',
                                'Value': payload.get('event_type', 'unknown')
                            }
                        ]
                    }
                ]
            )
    
    return {'statusCode': 200, 'body': 'Success'}
```

### 2. ElasticSearch 연동
```python
from elasticsearch import Elasticsearch
import json
import boto3
from datetime import datetime

class ElasticsearchSink:
    """Elasticsearch 연동 클래스"""
    
    def __init__(self, es_endpoint, index_name):
        self.es = Elasticsearch([es_endpoint])
        self.index_name = index_name
    
    def process_analytics_results(self, kinesis_records):
        """Analytics 결과를 Elasticsearch에 저장"""
        
        bulk_data = []
        
        for record in kinesis_records:
            # 문서 ID 생성
            doc_id = f"{record.get('event_type', 'unknown')}_{int(datetime.utcnow().timestamp())}"
            
            # 인덱스 액션 정의
            index_action = {
                "index": {
                    "_index": self.index_name,
                    "_id": doc_id
                }
            }
            
            # 문서 데이터
            doc_data = {
                **record,
                '@timestamp': datetime.utcnow().isoformat(),
                'processed_by': 'kinesis_analytics'
            }
            
            bulk_data.extend([index_action, doc_data])
        
        # 벌크 인덱싱 실행
        if bulk_data:
            try:
                response = self.es.bulk(body=bulk_data)
                
                # 오류 확인
                if response['errors']:
                    print(f"Elasticsearch bulk indexing errors: {response['items']}")
                else:
                    print(f"Successfully indexed {len(kinesis_records)} documents")
                    
            except Exception as e:
                print(f"Elasticsearch indexing failed: {e}")

# 사용 예시
def lambda_handler(event, context):
    es_sink = ElasticsearchSink(
        es_endpoint='https://search-analytics-domain.us-east-1.es.amazonaws.com',
        index_name='realtime-analytics'
    )
    
    # Kinesis 레코드 처리
    analytics_results = []
    for record in event['Records']:
        data = json.loads(record['kinesis']['data'])
        analytics_results.append(data)
    
    # Elasticsearch에 저장
    es_sink.process_analytics_results(analytics_results)
    
    return {'statusCode': 200}
```

## 성능 최적화

### 1. 쿼리 최적화
```sql
-- 비효율적인 쿼리 (전체 스캔)
SELECT COUNT(*) 
FROM SOURCE_SQL_STREAM_001 
WHERE user_id > 1000;

-- 최적화된 쿼리 (윈도우 사용)
SELECT STREAM
    COUNT(*) as user_count
FROM SOURCE_SQL_STREAM_001
WHERE user_id > 1000
GROUP BY RANGE_TIMESTAMP(ROWTIME RANGE INTERVAL '1' MINUTE);
```

### 2. 병렬 처리 설정
```python
# Flink 애플리케이션 병렬 처리 설정
def configure_parallelism():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # 전체 병렬도 설정
    env.set_parallelism(4)
    
    # 개별 오퍼레이터 병렬도 설정
    source_stream = env.add_source(kinesis_source).set_parallelism(2)
    
    processed_stream = source_stream \
        .map(transform_function).set_parallelism(4) \
        .key_by(lambda x: x['user_id']) \
        .window(TumblingEventTimeWindows.of(Time.minutes(5))) \
        .reduce(reduce_function).set_parallelism(2)
    
    processed_stream.add_sink(kinesis_sink).set_parallelism(1)
```

### 3. 메모리 및 체크포인트 최적화
```python
def configure_checkpointing():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # 체크포인트 활성화
    env.enable_checkpointing(60000)  # 1분마다
    
    # 체크포인트 설정
    checkpoint_config = env.get_checkpoint_config()
    checkpoint_config.set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)
    checkpoint_config.set_min_pause_between_checkpoints(30000)  # 30초
    checkpoint_config.set_checkpoint_timeout(600000)  # 10분
    checkpoint_config.enable_external_checkpoints(
        ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
    )
```

## 모니터링 및 알림

### CloudWatch 대시보드 구성
```python
import boto3

def create_analytics_dashboard():
    """Kinesis Analytics 모니터링 대시보드 생성"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    dashboard_body = {
        "widgets": [
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/KinesisAnalytics", "InputRecords", "Application", "realtime-analytics-app"],
                        [".", "OutputRecords", ".", "."],
                        [".", "KPUs", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Application Metrics"
                }
            },
            {
                "type": "metric", 
                "properties": {
                    "metrics": [
                        ["AWS/KinesisAnalytics", "Downtime", "Application", "realtime-analytics-app"],
                        [".", "UpTime", ".", "."]
                    ],
                    "period": 300,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Application Health"
                }
            }
        ]
    }
    
    response = cloudwatch.put_dashboard(
        DashboardName='KinesisAnalyticsDashboard',
        DashboardBody=json.dumps(dashboard_body)
    )
    
    return response
```

## 실제 사용 사례

### 1. 실시간 추천 시스템
```sql
-- 사용자 행동 기반 실시간 점수 계산
CREATE OR REPLACE STREAM "USER_BEHAVIOR_SCORE" (
    user_id           INTEGER,
    category          VARCHAR(32),
    behavior_score    DOUBLE,
    last_activity     TIMESTAMP
);

CREATE OR REPLACE PUMP "BEHAVIOR_SCORE_PUMP" AS INSERT INTO "USER_BEHAVIOR_SCORE"
SELECT STREAM
    user_id,
    category,
    SUM(
        CASE event_type
            WHEN 'view' THEN 1.0
            WHEN 'click' THEN 2.0 
            WHEN 'purchase' THEN 5.0
            ELSE 0.0
        END * EXP(-1.0 * (UNIX_TIMESTAMP(CURRENT_TIMESTAMP) - UNIX_TIMESTAMP(TO_TIMESTAMP(timestamp))) / 3600.0)
    ) AS behavior_score,
    MAX(TO_TIMESTAMP(timestamp)) AS last_activity
FROM SOURCE_SQL_STREAM_001
WHERE category IS NOT NULL
GROUP BY 
    user_id, 
    category,
    RANGE_TIMESTAMP(ROWTIME RANGE INTERVAL '1' HOUR);
```

### 2. 부정 거래 탐지
```sql
-- 실시간 부정 거래 패턴 탐지
CREATE OR REPLACE STREAM "FRAUD_DETECTION" (
    user_id              INTEGER,
    suspicious_activity  VARCHAR(256),
    risk_score          DOUBLE,
    alert_level         VARCHAR(16)
);

CREATE OR REPLACE PUMP "FRAUD_PUMP" AS INSERT INTO "FRAUD_DETECTION"
SELECT STREAM
    user_id,
    'High frequency transactions' AS suspicious_activity,
    COUNT(*) * 1.0 AS risk_score,
    CASE 
        WHEN COUNT(*) > 50 THEN 'HIGH'
        WHEN COUNT(*) > 20 THEN 'MEDIUM'
        ELSE 'LOW'
    END AS alert_level
FROM SOURCE_SQL_STREAM_001
WHERE event_type = 'purchase'
GROUP BY 
    user_id,
    RANGE_TIMESTAMP(ROWTIME RANGE INTERVAL '5' MINUTE)
HAVING COUNT(*) > 10;  -- 5분 내 10회 이상 거래
```

## 권장사항

### 개발 단계
* **프로토타입**: SQL Analytics로 빠른 검증
* **복잡한 로직**: Flink 애플리케이션으로 확장
* **테스트**: 작은 데이터셋으로 로직 검증
* **모니터링**: CloudWatch 메트릭 및 알람 설정

### 운영 고려사항
* **비용 최적화**: KPU 사용량 모니터링
* **지연시간**: 워터마크 및 윈도우 크기 조정
* **장애 복구**: 체크포인트 및 백업 전략
* **확장성**: 병렬도 및 리소스 할당 최적화

### 보안
* **네트워크**: VPC 내 프라이빗 서브넷 배치
* **권한**: 최소 권한 원칙 IAM 역할
* **암호화**: 전송 중 및 저장 중 데이터 암호화
* **감사**: CloudTrail 로그 활성화
