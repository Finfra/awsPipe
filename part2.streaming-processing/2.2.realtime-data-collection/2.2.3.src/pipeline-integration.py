#!/usr/bin/env python3
"""
Kinesis â†’ S3 â†’ EMR ì™„ì „ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•˜ì—¬ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ë¶€í„° ë°°ì¹˜ ì²˜ë¦¬ê¹Œì§€
"""

import boto3
import json
import time
import threading
from datetime import datetime, timedelta
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IntegratedPipeline:
    """í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config):
        self.config = config
        self.region = config['region']
        
        # AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.kinesis = boto3.client('kinesis', region_name=self.region)
        self.firehose = boto3.client('firehose', region_name=self.region)
        self.s3 = boto3.client('s3', region_name=self.region)
        self.emr = boto3.client('emr', region_name=self.region)
        self.lambda_client = boto3.client('lambda', region_name=self.region)
        self.cloudwatch = boto3.client('cloudwatch', region_name=self.region)
        self.events = boto3.client('events', region_name=self.region)
        
        # íŒŒì´í”„ë¼ì¸ ìƒíƒœ
        self.pipeline_status = {
            'kinesis_stream': 'NOT_CREATED',
            'firehose_stream': 'NOT_CREATED',
            'emr_cluster': 'NOT_CREATED',
            's3_bucket': 'NOT_CREATED',
            'lambda_functions': 'NOT_CREATED'
        }
    
    def create_transform_lambda(self):
        """ë°ì´í„° ë³€í™˜ìš© Lambda í•¨ìˆ˜ ìƒì„±"""
        
        function_name = f"{self.config['project_name']}-data-transform"
        
        logger.info(f"ë³€í™˜ Lambda í•¨ìˆ˜ ìƒì„±: {function_name}")
        
        # Lambda í•¨ìˆ˜ ì½”ë“œ
        transform_code = '''
import json
import base64
from datetime import datetime
import gzip

def lambda_handler(event, context):
    """Firehose ë°ì´í„° ë³€í™˜ í•¨ìˆ˜"""
    
    output_records = []
    
    for record in event['records']:
        try:
            # ì…ë ¥ ë°ì´í„° ë””ì½”ë”©
            payload = json.loads(base64.b64decode(record['data']).decode('utf-8'))
            
            # ë°ì´í„° ë³€í™˜ ë° enrichment
            transformed_payload = {
                'timestamp': payload.get('timestamp', datetime.utcnow().isoformat()),
                'event_type': payload.get('event_type', 'unknown'),
                'user_id': payload.get('user_id'),
                'session_id': payload.get('session_id'),
                'ip_address': payload.get('ip_address'),
                'value': payload.get('value', 0),
                'processed_at': datetime.utcnow().isoformat(),
                'data_quality': 'valid' if all(k in payload for k in ['timestamp', 'event_type']) else 'incomplete'
            }
            
            # ì¶œë ¥ ë°ì´í„° ì¸ì½”ë”©
            output_data = json.dumps(transformed_payload) + '\\n'
            encoded_data = base64.b64encode(output_data.encode('utf-8')).decode('utf-8')
            
            output_records.append({
                'recordId': record['recordId'],
                'result': 'Ok',
                'data': encoded_data
            })
            
        except Exception as e:
            # ì²˜ë¦¬ ì‹¤íŒ¨í•œ ë ˆì½”ë“œ
            output_records.append({
                'recordId': record['recordId'],
                'result': 'ProcessingFailed'
            })
    
    return {'records': output_records}
'''
        
        try:
            # Lambda í•¨ìˆ˜ ìƒì„±
            response = self.lambda_client.create_function(
                FunctionName=function_name,
                Runtime='python3.9',
                Role=self.config['lambda_role_arn'],
                Handler='index.lambda_handler',
                Code={'ZipFile': transform_code.encode()},
                Timeout=300,
                MemorySize=256,
                Tags={
                    'Project': self.config['project_name'],
                    'Purpose': 'DataTransformation'
                }
            )
            
            lambda_arn = response['FunctionArn']
            logger.info(f"ë³€í™˜ Lambda í•¨ìˆ˜ ìƒì„± ì™„ë£Œ: {lambda_arn}")
            
            return lambda_arn
            
        except self.lambda_client.exceptions.ResourceConflictException:
            # í•¨ìˆ˜ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            response = self.lambda_client.get_function(FunctionName=function_name)
            logger.info("ë³€í™˜ Lambda í•¨ìˆ˜ê°€ ì´ë¯¸ ì¡´ì¬í•¨")
            return response['Configuration']['FunctionArn']
            
        except Exception as e:
            logger.error(f"ë³€í™˜ Lambda í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def create_emr_cluster(self):
        """EMR í´ëŸ¬ìŠ¤í„° ìƒì„±"""
        
        cluster_name = f"{self.config['project_name']}-streaming-cluster"
        
        logger.info(f"EMR í´ëŸ¬ìŠ¤í„° ìƒì„±: {cluster_name}")
        
        try:
            response = self.emr.run_job_flow(
                Name=cluster_name,
                ReleaseLabel='emr-6.15.0',
                Applications=[
                    {'Name': 'Spark'},
                    {'Name': 'Hadoop'},
                    {'Name': 'Hive'}
                ],
                
                Instances={
                    'InstanceGroups': [
                        {
                            'Name': 'Master',
                            'Market': 'ON_DEMAND',
                            'InstanceRole': 'MASTER',
                            'InstanceType': 'm5.xlarge',
                            'InstanceCount': 1
                        },
                        {
                            'Name': 'Core',
                            'Market': 'SPOT',
                            'BidPrice': '0.20',
                            'InstanceRole': 'CORE',
                            'InstanceType': 'm5.large',
                            'InstanceCount': 2
                        }
                    ],
                    'Ec2KeyName': self.config.get('ec2_key_name'),
                    'KeepJobFlowAliveWhenNoSteps': True,
                    'TerminationProtected': False,
                    'Ec2SubnetId': self.config.get('subnet_id')
                },
                
                ServiceRole=self.config['emr_service_role'],
                JobFlowRole=self.config['emr_ec2_instance_profile'],
                
                LogUri=f"s3://{self.config['s3_bucket_name']}/emr-logs/",
                
                # Spark ì„¤ì • ìµœì í™”
                Configurations=[
                    {
                        'Classification': 'spark-defaults',
                        'Properties': {
                            'spark.sql.adaptive.enabled': 'true',
                            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
                            'spark.hadoop.fs.s3a.multipart.size': '134217728',
                            'spark.sql.parquet.compression.codec': 'snappy',
                            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer'
                        }
                    }
                ],
                
                # ìë™ ì¢…ë£Œ ì„¤ì •
                AutoTerminationPolicy={
                    'IdleTimeout': 3600  # 1ì‹œê°„ idle í›„ ì¢…ë£Œ
                },
                
                Tags=[
                    {'Key': 'Project', 'Value': self.config['project_name']},
                    {'Key': 'Environment', 'Value': self.config.get('environment', 'dev')},
                    {'Key': 'Purpose', 'Value': 'StreamingDataProcessing'}
                ]
            )
            
            cluster_id = response['JobFlowId']
            self.config['emr_cluster_id'] = cluster_id
            
            self.pipeline_status['emr_cluster'] = 'CREATED'
            logger.info(f"EMR í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ: {cluster_id}")
            
            return cluster_id
            
        except Exception as e:
            logger.error(f"EMR í´ëŸ¬ìŠ¤í„° ìƒì„± ì‹¤íŒ¨: {e}")
            self.pipeline_status['emr_cluster'] = 'FAILED'
            raise
    
    def upload_spark_jobs(self):
        """Spark ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ë¥¼ S3ì— ì—…ë¡œë“œ"""
        
        logger.info("Spark ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ")
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
        realtime_processor = '''
#!/usr/bin/env python3
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import sys

def create_spark_session():
    return SparkSession.builder \\
        .appName("RealtimeDataProcessor") \\
        .config("spark.sql.adaptive.enabled", "true") \\
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\
        .getOrCreate()

def process_streaming_data(spark, input_path, output_path):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬"""
    
    print(f"ì²˜ë¦¬ ì‹œì‘: {input_path} -> {output_path}")
    
    # ë°ì´í„° ì½ê¸°
    df = spark.read.json(input_path)
    
    # ë°ì´í„° í’ˆì§ˆ í•„í„°ë§
    quality_df = df.filter(
        col("data_quality") == "valid"
    ).filter(
        col("timestamp").isNotNull() & 
        col("event_type").isNotNull()
    )
    
    # ì‹¤ì‹œê°„ ì§‘ê³„
    # 1. ì‚¬ìš©ìë³„ í™œë™ ìš”ì•½
    user_activity = quality_df \\
        .groupBy("user_id") \\
        .agg(
            count("*").alias("total_events"),
            sum("value").alias("total_value"),
            countDistinct("session_id").alias("unique_sessions"),
            max("timestamp").alias("last_activity"),
            collect_set("event_type").alias("event_types")
        )
    
    # 2. ì‹œê°„ë³„ íŠ¸ë Œë“œ
    hourly_trends = quality_df \\
        .withColumn("hour", date_format("timestamp", "yyyy-MM-dd HH:00:00")) \\
        .groupBy("hour", "event_type") \\
        .agg(
            count("*").alias("event_count"),
            countDistinct("user_id").alias("unique_users"),
            avg("value").alias("avg_value"),
            sum("value").alias("total_value")
        )
    
    # 3. ì´ìƒ íƒì§€
    window_spec = Window.partitionBy("event_type")
    
    anomalies = quality_df \\
        .withColumn("avg_value", avg("value").over(window_spec)) \\
        .withColumn("stddev_value", stddev("value").over(window_spec)) \\
        .withColumn("z_score", 
            abs(col("value") - col("avg_value")) / col("stddev_value")
        ) \\
        .filter(col("z_score") > 3) \\
        .select("timestamp", "user_id", "event_type", "value", "z_score")
    
    # ê²°ê³¼ ì €ì¥
    user_activity.coalesce(1).write.mode("overwrite").parquet(f"{output_path}/user-activity/")
    hourly_trends.coalesce(1).write.mode("overwrite").parquet(f"{output_path}/hourly-trends/")
    anomalies.coalesce(1).write.mode("overwrite").parquet(f"{output_path}/anomalies/")
    
    print("ì²˜ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: spark-submit script.py <input_path> <output_path>")
        sys.exit(1)
    
    input_path = sys.argv[1]
    output_path = sys.argv[2]
    
    spark = create_spark_session()
    try:
        process_streaming_data(spark, input_path, output_path)
    finally:
        spark.stop()
'''
        
        try:
            # S3ì— ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
            self.s3.put_object(
                Bucket=self.config['s3_bucket_name'],
                Key='scripts/realtime_processor.py',
                Body=realtime_processor,
                ContentType='text/x-python'
            )
            
            logger.info("Spark ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Spark ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def create_s3_trigger_lambda(self):
        """S3 ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜ ìƒì„±"""
        
        function_name = f"{self.config['project_name']}-s3-trigger"
        
        logger.info(f"S3 íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜ ìƒì„±: {function_name}")
        
        trigger_code = f'''
import json
import boto3
import os
from urllib.parse import unquote_plus
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    emr = boto3.client('emr')
    
    cluster_id = os.environ.get('EMR_CLUSTER_ID')
    if not cluster_id:
        logger.error("EMR_CLUSTER_ID not set")
        return {{'statusCode': 500}}
    
    for record in event['Records']:
        try:
            bucket = record['s3']['bucket']['name']
            key = unquote_plus(record['s3']['object']['key'])
            
            logger.info(f"Processing S3 object: s3://{{bucket}}/{{key}}")
            
            # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° íŒŒì¼ë§Œ ì²˜ë¦¬
            if key.startswith('streaming-data/') and not key.endswith('/'):
                # íŒŒí‹°ì…˜ ê²½ë¡œì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                path_parts = key.split('/')
                if len(path_parts) >= 5:
                    year = path_parts[1].split('=')[1]
                    month = path_parts[2].split('=')[1]
                    day = path_parts[3].split('=')[1]
                    hour = path_parts[4].split('=')[1]
                    
                    input_path = f"s3://{{bucket}}/streaming-data/year={{year}}/month={{month}}/day={{day}}/hour={{hour}}/"
                    output_path = f"s3://{{bucket}}/processed-data/year={{year}}/month={{month}}/day={{day}}/hour={{hour}}/"
                    
                    # EMR Step ì œì¶œ
                    response = emr.add_job_flow_steps(
                        JobFlowId=cluster_id,
                        Steps=[{{
                            'Name': f'Process-{{year}}-{{month}}-{{day}}-{{hour}}',
                            'ActionOnFailure': 'CONTINUE',
                            'HadoopJarStep': {{
                                'Jar': 'command-runner.jar',
                                'Args': [
                                    'spark-submit',
                                    '--deploy-mode', 'cluster',
                                    '--driver-memory', '2g',
                                    '--executor-memory', '2g',
                                    '--executor-cores', '2',
                                    '--num-executors', '4',
                                    f"s3://{{bucket}}/scripts/realtime_processor.py",
                                    input_path,
                                    output_path
                                ]
                            }}
                        }}]
                    )
                    
                    logger.info(f"EMR step submitted: {{response['StepIds'][0]}}")
                    
        except Exception as e:
            logger.error(f"Error processing S3 event: {{e}}")
    
    return {{'statusCode': 200}}
'''
        
        try:
            # Lambda í•¨ìˆ˜ ìƒì„±
            response = self.lambda_client.create_function(
                FunctionName=function_name,
                Runtime='python3.9',
                Role=self.config['lambda_role_arn'],
                Handler='index.lambda_handler',
                Code={'ZipFile': trigger_code.encode()},
                Environment={
                    'Variables': {
                        'EMR_CLUSTER_ID': self.config.get('emr_cluster_id', '')
                    }
                },
                Timeout=300,
                Tags={
                    'Project': self.config['project_name'],
                    'Purpose': 'S3EventTrigger'
                }
            )
            
            lambda_arn = response['FunctionArn']
            logger.info(f"S3 íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜ ìƒì„± ì™„ë£Œ: {lambda_arn}")
            
            return lambda_arn
            
        except self.lambda_client.exceptions.ResourceConflictException:
            # í•¨ìˆ˜ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì—…ë°ì´íŠ¸
            self.lambda_client.update_function_code(
                FunctionName=function_name,
                ZipFile=trigger_code.encode()
            )
            
            self.lambda_client.update_function_configuration(
                FunctionName=function_name,
                Environment={
                    'Variables': {
                        'EMR_CLUSTER_ID': self.config.get('emr_cluster_id', '')
                    }
                }
            )
            
            response = self.lambda_client.get_function(FunctionName=function_name)
            logger.info("S3 íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            return response['Configuration']['FunctionArn']
            
        except Exception as e:
            logger.error(f"S3 íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def setup_monitoring(self):
        """íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        
        logger.info("ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ ì„¤ì •")
        
        try:
            # Kinesis ìŠ¤íŠ¸ë¦¼ ëª¨ë‹ˆí„°ë§
            self.cloudwatch.put_metric_alarm(
                AlarmName=f"{self.config['project_name']}-kinesis-high-incoming-records",
                ComparisonOperator='GreaterThanThreshold',
                EvaluationPeriods=2,
                MetricName='IncomingRecords',
                Namespace='AWS/Kinesis',
                Period=300,
                Statistic='Sum',
                Threshold=10000,
                ActionsEnabled=True,
                AlarmActions=[self.config.get('sns_topic_arn', '')],
                Dimensions=[
                    {
                        'Name': 'StreamName',
                        'Value': self.config['kinesis_stream_name']
                    }
                ]
            )
            
            # EMR í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§
            if 'emr_cluster_id' in self.config:
                self.cloudwatch.put_metric_alarm(
                    AlarmName=f"{self.config['project_name']}-emr-failed-steps",
                    ComparisonOperator='GreaterThanThreshold',
                    EvaluationPeriods=1,
                    MetricName='StepsFailed',
                    Namespace='AWS/ElasticMapReduce',
                    Period=300,
                    Statistic='Sum',
                    Threshold=0,
                    ActionsEnabled=True,
                    AlarmActions=[self.config.get('sns_topic_arn', '')],
                    Dimensions=[
                        {
                            'Name': 'JobFlowId',
                            'Value': self.config['emr_cluster_id']
                        }
                    ]
                )
            
            logger.info("ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
    
    def build_complete_pipeline(self):
        """ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"""
        
        logger.info("=== í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹œì‘ ===")
        
        try:
            # 1. S3 ë²„í‚· ìƒì„±
            s3_bucket_arn = self.create_s3_bucket()
            
            # 2. Kinesis ìŠ¤íŠ¸ë¦¼ ìƒì„±
            kinesis_stream_arn = self.create_kinesis_stream()
            
            # 3. Spark ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
            self.upload_spark_jobs()
            
            # 4. Firehose ìŠ¤íŠ¸ë¦¼ ìƒì„±
            self.create_firehose_stream(kinesis_stream_arn, s3_bucket_arn)
            
            # 5. EMR í´ëŸ¬ìŠ¤í„° ìƒì„±
            cluster_id = self.create_emr_cluster()
            
            # 6. S3 ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±° Lambda ìƒì„±
            trigger_lambda_arn = self.create_s3_trigger_lambda()
            
            # 7. ëª¨ë‹ˆí„°ë§ ì„¤ì •
            self.setup_monitoring()
            
            # 8. íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¶œë ¥
            self.print_pipeline_status()
            
            logger.info("=== í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ ===")
            
            return {
                'status': 'SUCCESS',
                'kinesis_stream': self.config['kinesis_stream_name'],
                'firehose_stream': self.config['firehose_stream_name'],
                's3_bucket': self.config['s3_bucket_name'],
                'emr_cluster_id': cluster_id,
                'lambda_trigger': trigger_lambda_arn
            }
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
            self.print_pipeline_status()
            raise
    
    def print_pipeline_status(self):
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì¶œë ¥"""
        
        logger.info("=== íŒŒì´í”„ë¼ì¸ ìƒíƒœ ===")
        for component, status in self.pipeline_status.items():
            status_icon = "âœ…" if status in ['CREATED', 'EXISTS'] else "âŒ" if status == 'FAILED' else "â³"
            logger.info(f"{status_icon} {component}: {status}")
    
    def test_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        
        logger.info("íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_records = [
            {
                'timestamp': datetime.utcnow().isoformat(),
                'event_type': 'page_view',
                'user_id': 12345,
                'session_id': 'test_session_1',
                'value': 100,
                'ip_address': '192.168.1.100'
            },
            {
                'timestamp': datetime.utcnow().isoformat(),
                'event_type': 'click',
                'user_id': 12346,
                'session_id': 'test_session_2',
                'value': 50,
                'ip_address': '192.168.1.101'
            }
        ]
        
        try:
            # Kinesisì— í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡
            for i, record in enumerate(test_records):
                response = self.kinesis.put_record(
                    StreamName=self.config['kinesis_stream_name'],
                    Data=json.dumps(record),
                    PartitionKey=f"test_key_{i}"
                )
                
                logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡: {response['SequenceNumber']}")
            
            logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡ ì™„ë£Œ")
            logger.info("ë°ì´í„°ê°€ S3ì— ì €ì¥ë˜ê³  EMRì—ì„œ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°...")
            
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    config = {
        'region': 'us-east-1',
        'project_name': 'integrated-streaming-pipeline',
        'environment': 'dev',
        
        # Kinesis ì„¤ì •
        'kinesis_stream_name': 'integrated-data-stream',
        'shard_count': 2,
        
        # Firehose ì„¤ì •
        'firehose_stream_name': 'integrated-firehose-stream',
        'firehose_role_arn': 'arn:aws:iam::123456789012:role/firehose-delivery-role',
        
        # S3 ì„¤ì •
        's3_bucket_name': 'integrated-streaming-data-bucket',
        
        # EMR ì„¤ì •
        'emr_service_role': 'EMR_DefaultRole',
        'emr_ec2_instance_profile': 'EMR_EC2_DefaultRole',
        'ec2_key_name': 'my-key-pair',
        'subnet_id': 'subnet-12345678',
        
        # Lambda ì„¤ì •
        'lambda_role_arn': 'arn:aws:iam::123456789012:role/lambda-execution-role',
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        'sns_topic_arn': 'arn:aws:sns:us-east-1:123456789012:pipeline-alerts'
    }
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        pipeline = IntegratedPipeline(config)
        
        # ì™„ì „í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
        result = pipeline.build_complete_pipeline()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_result = pipeline.test_pipeline()
        
        if test_result:
            logger.info("ğŸ‰ í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            logger.warning("âš ï¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì„±ê³µ, í…ŒìŠ¤íŠ¸ ë¶€ë¶„ì  ì„±ê³µ")
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("=== êµ¬ì¶• ê²°ê³¼ ===")
        for key, value in result.items():
            logger.info(f"{key}: {value}")
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()
