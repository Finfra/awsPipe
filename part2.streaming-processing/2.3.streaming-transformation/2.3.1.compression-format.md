# 2.3.1 압축 및 포맷 변환

## Kinesis Firehose 데이터 변환

### 지원되는 압축 형식
| 압축 형식  | 압축률    | 속도      | CPU 사용량 | 호환성    | 권장 용도     |
| ---------- | --------- | --------- | ---------- | --------- | ------------- |
| **GZIP**   | 높음      | 느림      | 높음       | 매우 좋음 | 장기 저장     |
| **Snappy** | 중간      | 빠름      | 낮음       | 좋음      | 실시간 처리   |
| **LZ4**    | 낮음      | 매우 빠름 | 매우 낮음  | 보통      | 고성능 요구   |
| **ZSTD**   | 매우 높음 | 중간      | 중간       | 보통      | 균형잡힌 성능 |

### 포맷 변환 전략

#### JSON → Parquet 변환
Firehose Lambda 함수를 통한 실시간 데이터 변환

**핵심 변환 과정**
* 입력 JSON 데이터 디코딩
* 스키마 정규화 및 필드 검증
* 데이터 타입 변환 (문자열 → 숫자)
* 품질 점수 계산 및 메타데이터 추가
* Parquet 호환 형식으로 출력

**변환 함수 예시**
```python
def lambda_handler(event, context):
    output_records = []
    
    for record in event['records']:
        try:
            # JSON 디코딩
            payload = json.loads(base64.b64decode(record['data']))
            
            # 스키마 정규화
            normalized_data = normalize_schema(payload)
            
            # 품질 검증
            quality_score = calculate_quality_score(normalized_data)
            normalized_data['data_quality_score'] = quality_score
            
            # 출력 레코드 생성
            output_records.append({
                'recordId': record['recordId'],
                'result': 'Ok',
                'data': base64.b64encode(json.dumps(normalized_data).encode())
            })
            
        except Exception as e:
            output_records.append({
                'recordId': record['recordId'],
                'result': 'ProcessingFailed'
            })
    
    return {'records': output_records}
```

### 압축 성능 비교

#### 실제 데이터 압축 테스트 결과
| 압축 형식 | 압축 시간 | 압축 크기 | 압축률 | 공간 절약 |
| --------- | --------- | --------- | ------ | --------- |
| GZIP      | 0.245초   | 2.1MB     | 0.21   | 79%       |
| BZ2       | 0.890초   | 1.8MB     | 0.18   | 82%       |
| LZMA      | 1.250초   | 1.6MB     | 0.16   | 84%       |
| 원본      | -         | 10MB      | 1.00   | 0%        |

**압축 선택 가이드**
* **실시간 처리**: Snappy (빠른 압축/해제)
* **저장 최적화**: GZIP (압축률과 속도 균형)
* **최고 압축**: LZMA (저장 공간 최소화)

## 동적 파티셔닝

### 시간 기반 파티셔닝
```python
# 파티션 키 생성 예시
def generate_partition_key(record):
    timestamp = record.get('timestamp')
    if timestamp:
        dt = datetime.fromisoformat(timestamp)
        return f"year={dt.year}/month={dt.month:02d}/day={dt.day:02d}/hour={dt.hour:02d}"
    return "year=unknown/month=unknown/day=unknown/hour=unknown"
```

### Firehose 동적 파티셔닝 설정
```json
{
    "DynamicPartitioning": {
        "Enabled": true,
        "Processors": [{
            "Type": "MetadataExtraction",
            "Parameters": [{
                "ParameterName": "MetadataExtractionQuery",
                "ParameterValue": "{year:.timestamp | strftime(\"%Y\"), month:.timestamp | strftime(\"%m\")}"
            }]
        }]
    }
}
```

### 지리적 파티셔닝
IP 주소 기반 지역별 파티셔닝으로 데이터 지역화

**파티션 구조**
```
s3://bucket/data/
├── country=kr/region=asia-pacific/
├── country=us/region=north-america/
└── country=unknown/region=unknown/
```

## 데이터 형식 최적화

### Columnar 포맷 성능 비교
| 포맷         | 쓰기 시간 | 읽기 시간 | 파일 크기 | 압축률 |
| ------------ | --------- | --------- | --------- | ------ |
| CSV          | 0.450초   | 0.320초   | 45MB      | -      |
| Parquet+Snappy | 0.280초   | 0.180초   | 12MB      | 73%    |
| Parquet+GZIP | 0.420초   | 0.200초   | 8MB       | 82%    |

**포맷 선택 기준**
* **CSV**: 호환성 최우선
* **Parquet+Snappy**: 실시간 분석
* **Parquet+GZIP**: 장기 저장

## 스키마 진화 관리

### 버전 관리 전략
스키마 변경 시 하위 호환성 유지를 위한 버전 관리

**스키마 버전 예시**
* **v1.0**: 기본 필드 (timestamp, event_type, user_id, value)
* **v1.1**: session_id 필드 추가
* **v1.2**: value 타입 변경 (float → double), properties 필드 추가

**마이그레이션 전략**
```python
def migrate_record(record, from_version, to_version):
    if from_version == 'v1.0' and to_version == 'v1.1':
        record['session_id'] = record.get('session_id', 'unknown')
    elif from_version == 'v1.1' and to_version == 'v1.2':
        record['value'] = float(record.get('value', 0))
        record['properties'] = json.dumps(record.get('properties', {}))
    return record
```

**검증 규칙**
* 필수 필드 존재 여부
* 데이터 타입 일치성
* 값 범위 및 형식 검증

## 실시간 데이터 품질 모니터링

### 품질 메트릭 정의
| 메트릭            | 설명                    | 계산 방식                |
| ----------------- | ----------------------- | ------------------------ |
| 스키마 준수율     | 필수 필드 존재 비율     | 유효 레코드 / 전체 레코드 |
| 데이터 타입 정확성 | 타입 일치 비율          | 정확한 타입 / 전체 필드   |
| 중복 레코드율     | 중복 데이터 비율        | 중복 레코드 / 전체 레코드 |
| 품질 점수         | 종합 품질 점수 (0-100)  | 가중 평균                |

### 품질 모니터링 프로세스
1. **실시간 검증**: 각 레코드별 품질 점수 계산
2. **메트릭 수집**: CloudWatch 커스텀 메트릭 전송
3. **알람 설정**: 품질 임계값 기반 알림
4. **대시보드**: 실시간 품질 현황 시각화

### 품질 점수 계산 로직
```python
def calculate_quality_score(record):
    base_score = 100
    
    # 필수 필드 검증 (40점)
    required_fields = ['timestamp', 'event_type', 'user_id']
    missing_fields = [f for f in required_fields if not record.get(f)]
    base_score -= len(missing_fields) * 13
    
    # 데이터 타입 검증 (30점)
    type_errors = validate_data_types(record)
    base_score -= len(type_errors) * 10
    
    # 비즈니스 규칙 검증 (30점)
    business_errors = validate_business_rules(record)
    base_score -= len(business_errors) * 10
    
    return max(0, base_score)
```

## 권장사항

### 압축 형식 선택 가이드
* **실시간 처리**: Snappy (빠른 압축/해제)
* **장기 저장**: GZIP (높은 압축률)
* **균형 잡힌 성능**: ZSTD (압축률과 속도 균형)
* **최고 성능**: LZ4 (CPU 사용량 최소)

### 파티셔닝 전략
* **시간 기반**: 시계열 데이터 분석에 최적
* **지역 기반**: 글로벌 서비스의 지역별 분석
* **이벤트 타입**: 다양한 이벤트가 혼재된 경우
* **하이브리드**: 시간 + 카테고리 조합

### 스키마 관리
* **하위 호환성**: 필드 추가는 가능, 삭제는 신중히
* **버전 관리**: 명확한 버전 체계 구축
* **마이그레이션**: 점진적 스키마 변경
* **검증**: 자동화된 스키마 검증 도구 활용

### 운영 모범 사례
* **모니터링**: 압축률, 변환 시간, 오류율 추적
* **알람**: 품질 임계값 기반 자동 알림
* **백업**: 변환 실패 데이터 별도 저장
* **테스트**: 스키마 변경 전 충분한 검증

## 확장 고려사항
* **대용량 처리**: Lambda 동시성 및 메모리 최적화
* **비용 효율성**: 압축률과 처리 비용의 균형
* **보안**: 데이터 암호화 및 접근 제어
* **복구**: 변환 실패 시 재처리 메커니즘
