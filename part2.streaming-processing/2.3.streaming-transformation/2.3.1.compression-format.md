# 3.2 압축 및 포맷 변환

## Kinesis Firehose 데이터 변환

### 지원되는 압축 형식
| 압축 형식 | 압축률 | 속도 | CPU 사용량 | 호환성 | 권장 용도 |
|-----------|--------|------|------------|--------|-----------|
| **GZIP** | 높음 | 느림 | 높음 | 매우 좋음 | 장기 저장 |
| **Snappy** | 중간 | 빠름 | 낮음 | 좋음 | 실시간 처리 |
| **LZ4** | 낮음 | 매우 빠름 | 매우 낮음 | 보통 | 고성능 요구 |
| **ZSTD** | 매우 높음 | 중간 | 중간 | 보통 | 균형잡힌 성능 |

### 포맷 변환 전략

#### JSON → Parquet 변환
```python
# Firehose 데이터 변환 Lambda 함수
import json
import base64
import boto3
from datetime import datetime
import uuid
import re

def lambda_handler(event, context):
    """JSON 데이터를 Parquet 호환 형식으로 변환"""
    
    output_records = []
    
    for record in event['records']:
        try:
            # 입력 데이터 디코딩
            payload = json.loads(base64.b64decode(record['data']).decode('utf-8'))
            
            # 스키마 정규화
            normalized_payload = {
                # 필수 필드
                'timestamp': payload.get('timestamp', datetime.utcnow().isoformat()),
                'event_id': payload.get('event_id', str(uuid.uuid4())),
                'event_type': payload.get('event_type', 'unknown'),
                
                # 사용자 정보
                'user_id': ensure_integer(payload.get('user_id')),
                'session_id': payload.get('session_id'),
                'ip_address': payload.get('ip_address'),
                
                # 이벤트 데이터
                'value': ensure_float(payload.get('value', 0.0)),
                'category': payload.get('category', 'uncategorized'),
                'properties': json.dumps(payload.get('properties', {})),
                
                # 메타데이터
                'processed_at': datetime.utcnow().isoformat(),
                'data_version': '1.0',
                'source_system': payload.get('source_system', 'unknown')
            }
            
            # 데이터 품질 검증
            quality_score = calculate_quality_score(normalized_payload)
            normalized_payload['data_quality_score'] = quality_score
            
            # 출력 데이터 인코딩
            output_data = json.dumps(normalized_payload) + '\n'
            encoded_data = base64.b64encode(output_data.encode('utf-8')).decode('utf-8')
            
            output_records.append({
                'recordId': record['recordId'],
                'result': 'Ok',
                'data': encoded_data
            })
            
        except Exception as e:
            # 변환 실패한 레코드는 에러 처리
            print(f"Record transformation failed: {e}")
            output_records.append({
                'recordId': record['recordId'],
                'result': 'ProcessingFailed'
            })
    
    return {'records': output_records}

def ensure_integer(value):
    """안전한 정수 변환"""
    if value is None:
        return None
    try:
        return int(float(value))
    except (ValueError, TypeError):
        return None

def ensure_float(value):
    """안전한 실수 변환"""
    if value is None:
        return 0.0
    try:
        return float(value)
    except (ValueError, TypeError):
        return 0.0

def calculate_quality_score(payload):
    """데이터 품질 점수 계산"""
    score = 0
    max_score = 10
    
    # 필수 필드 존재 여부 (40%)
    required_fields = ['timestamp', 'event_type', 'user_id']
    for field in required_fields:
        if payload.get(field) is not None:
            score += 4 / len(required_fields)
    
    # 데이터 타입 정확성 (30%)
    if isinstance(payload.get('user_id'), int):
        score += 1
    if isinstance(payload.get('value'), (int, float)):
        score += 1
    if payload.get('timestamp') and validate_timestamp(payload['timestamp']):
        score += 1
    
    # 데이터 일관성 (30%)
    if payload.get('ip_address') and validate_ip(payload['ip_address']):
        score += 1
    if payload.get('event_type') and payload['event_type'] in get_valid_event_types():
        score += 1
    if payload.get('session_id') and len(payload['session_id']) > 0:
        score += 1
    
    return min(score, max_score)

def validate_timestamp(timestamp_str):
    """타임스탬프 유효성 검증"""
    try:
        datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        return True
    except:
        return False

def validate_ip(ip_address):
    """IP 주소 유효성 검증"""
    pattern = r'^(\d{1,3}\.){3}\d{1,3}$'
    return bool(re.match(pattern, ip_address))

def get_valid_event_types():
    """유효한 이벤트 타입 목록"""
    return [
        'page_view', 'click', 'purchase', 'signup', 'login', 
        'logout', 'add_to_cart', 'checkout', 'search'
    ]
```

### 압축 성능 비교

#### 실제 데이터 압축 테스트
```python
import gzip
import bz2
import lzma
import time
import os

def compression_benchmark(data, iterations=10):
    """압축 성능 벤치마크"""
    
    results = {}
    original_size = len(data.encode('utf-8'))
    
    # GZIP 압축
    gzip_times = []
    for _ in range(iterations):
        start = time.time()
        compressed = gzip.compress(data.encode('utf-8'))
        gzip_times.append(time.time() - start)
    
    results['gzip'] = {
        'avg_compression_time': sum(gzip_times) / len(gzip_times),
        'compressed_size': len(compressed),
        'compression_ratio': len(compressed) / original_size,
        'space_savings': (1 - len(compressed) / original_size) * 100
    }
    
    # BZ2 압축
    bz2_times = []
    for _ in range(iterations):
        start = time.time()
        compressed = bz2.compress(data.encode('utf-8'))
        bz2_times.append(time.time() - start)
    
    results['bz2'] = {
        'avg_compression_time': sum(bz2_times) / len(bz2_times),
        'compressed_size': len(compressed),
        'compression_ratio': len(compressed) / original_size,
        'space_savings': (1 - len(compressed) / original_size) * 100
    }
    
    # LZMA 압축
    lzma_times = []
    for _ in range(iterations):
        start = time.time()
        compressed = lzma.compress(data.encode('utf-8'))
        lzma_times.append(time.time() - start)
    
    results['lzma'] = {
        'avg_compression_time': sum(lzma_times) / len(lzma_times),
        'compressed_size': len(compressed),
        'compression_ratio': len(compressed) / original_size,
        'space_savings': (1 - len(compressed) / original_size) * 100
    }
    
    return results

# 테스트 데이터 생성
test_data = json.dumps([
    {
        'timestamp': '2024-05-26T10:30:00Z',
        'event_type': 'page_view',
        'user_id': i,
        'session_id': f'session_{i % 100}',
        'value': 100 + (i % 500),
        'properties': {'page': f'/page_{i % 50}', 'referrer': 'google.com'}
    }
    for i in range(10000)
])

# 압축 벤치마크 실행
benchmark_results = compression_benchmark(test_data)

print("=== 압축 성능 비교 ===")
for compression_type, stats in benchmark_results.items():
    print(f"\n{compression_type.upper()}:")
    print(f"  압축 시간: {stats['avg_compression_time']:.4f}초")
    print(f"  압축 크기: {stats['compressed_size']:,} bytes")
    print(f"  압축률: {stats['compression_ratio']:.3f}")
    print(f"  공간 절약: {stats['space_savings']:.1f}%")
```

## 동적 파티셔닝

### 시간 기반 파티셔닝
```python
def generate_partition_key(record):
    """동적 파티션 키 생성"""
    
    try:
        timestamp = record.get('timestamp')
        if timestamp:
            dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            
            # 년/월/일/시간 기반 파티셔닝
            partition_key = f"year={dt.year}/month={dt.month:02d}/day={dt.day:02d}/hour={dt.hour:02d}"
            
            # 이벤트 타입별 추가 파티셔닝
            event_type = record.get('event_type', 'unknown')
            partition_key += f"/event_type={event_type}"
            
            return partition_key
    except:
        pass
    
    # 기본 파티션
    return "year=unknown/month=unknown/day=unknown/hour=unknown/event_type=unknown"

# Firehose 동적 파티셔닝 설정
dynamic_partitioning_config = {
    'Enabled': True,
    'Processors': [
        {
            'Type': 'MetadataExtraction',
            'Parameters': [
                {
                    'ParameterName': 'MetadataExtractionQuery',
                    'ParameterValue': '{year:.timestamp | strftime("%Y"), month:.timestamp | strftime("%m"), day:.timestamp | strftime("%d"), hour:.timestamp | strftime("%H"), event_type:.event_type}'
                },
                {
                    'ParameterName': 'JsonParsingEngine',
                    'ParameterValue': 'JQ-1.6'
                }
            ]
        }
    ]
}
```

### 지리적 파티셔닝
```python
def geo_partition_key(record):
    """지리적 위치 기반 파티셔닝"""
    
    ip_address = record.get('ip_address')
    if ip_address:
        # IP 주소에서 지역 정보 추출 (간단한 예시)
        country = get_country_from_ip(ip_address)
        region = get_region_from_ip(ip_address)
        
        return f"country={country}/region={region}"
    
    return "country=unknown/region=unknown"

def get_country_from_ip(ip_address):
    """IP 주소에서 국가 정보 추출 (실제로는 GeoIP 라이브러리 사용)"""
    # 간단한 예시 - 실제로는 MaxMind GeoIP2 등 사용
    ip_parts = ip_address.split('.')
    if ip_parts[0] in ['192', '10']:
        return 'private'
    elif ip_parts[0] in ['203', '202']:
        return 'kr'
    elif ip_parts[0] in ['208', '209']:
        return 'us'
    else:
        return 'unknown'

def get_region_from_ip(ip_address):
    """IP 주소에서 지역 정보 추출"""
    country = get_country_from_ip(ip_address)
    if country == 'kr':
        return 'asia-pacific'
    elif country == 'us':
        return 'north-america'
    else:
        return 'unknown'
```

## 데이터 형식 최적화

### Columnar 포맷 비교
```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import time

def format_comparison_test(df):
    """다양한 파일 포맷 성능 비교"""
    
    results = {}
    
    # CSV 포맷
    start_time = time.time()
    df.to_csv('test_data.csv', index=False)
    csv_write_time = time.time() - start_time
    csv_size = os.path.getsize('test_data.csv')
    
    start_time = time.time()
    pd.read_csv('test_data.csv')
    csv_read_time = time.time() - start_time
    
    results['csv'] = {
        'write_time': csv_write_time,
        'read_time': csv_read_time,
        'file_size': csv_size
    }
    
    # Parquet 포맷 (Snappy 압축)
    start_time = time.time()
    df.to_parquet('test_data_snappy.parquet', compression='snappy', index=False)
    parquet_snappy_write_time = time.time() - start_time
    parquet_snappy_size = os.path.getsize('test_data_snappy.parquet')
    
    start_time = time.time()
    pd.read_parquet('test_data_snappy.parquet')
    parquet_snappy_read_time = time.time() - start_time
    
    results['parquet_snappy'] = {
        'write_time': parquet_snappy_write_time,
        'read_time': parquet_snappy_read_time,
        'file_size': parquet_snappy_size
    }
    
    # Parquet 포맷 (GZIP 압축)
    start_time = time.time()
    df.to_parquet('test_data_gzip.parquet', compression='gzip', index=False)
    parquet_gzip_write_time = time.time() - start_time
    parquet_gzip_size = os.path.getsize('test_data_gzip.parquet')
    
    start_time = time.time()
    pd.read_parquet('test_data_gzip.parquet')
    parquet_gzip_read_time = time.time() - start_time
    
    results['parquet_gzip'] = {
        'write_time': parquet_gzip_write_time,
        'read_time': parquet_gzip_read_time,
        'file_size': parquet_gzip_size
    }
    
    # 결과 정리
    os.remove('test_data.csv')
    os.remove('test_data_snappy.parquet')
    os.remove('test_data_gzip.parquet')
    
    return results

# 테스트 데이터 생성
test_df = pd.DataFrame({
    'timestamp': pd.date_range('2024-01-01', periods=100000, freq='1min'),
    'user_id': range(100000),
    'event_type': ['click', 'view', 'purchase'] * 33334,
    'value': range(100000),
    'session_id': [f'session_{i}' for i in range(100000)]
})

# 포맷 비교 테스트 실행
format_results = format_comparison_test(test_df)

print("=== 파일 포맷 성능 비교 ===")
for format_name, metrics in format_results.items():
    print(f"\n{format_name.upper()}:")
    print(f"  쓰기 시간: {metrics['write_time']:.3f}초")
    print(f"  읽기 시간: {metrics['read_time']:.3f}초")
    print(f"  파일 크기: {metrics['file_size'] / 1024 / 1024:.2f} MB")
```

## 스키마 진화 관리

### 버전 관리 전략
```python
class SchemaVersionManager:
    """스키마 버전 관리"""
    
    def __init__(self):
        self.schemas = {
            'v1.0': {
                'timestamp': 'string',
                'event_type': 'string',
                'user_id': 'integer',
                'value': 'float'
            },
            'v1.1': {
                'timestamp': 'string',
                'event_type': 'string',
                'user_id': 'integer',
                'value': 'float',
                'session_id': 'string'  # 새 필드 추가
            },
            'v1.2': {
                'timestamp': 'string',
                'event_type': 'string',
                'user_id': 'integer',
                'value': 'double',  # 타입 변경
                'session_id': 'string',
                'properties': 'string'  # JSON 문자열
            }
        }
    
    def migrate_record(self, record, from_version, to_version):
        """레코드 스키마 마이그레이션"""
        
        if from_version == 'v1.0' and to_version == 'v1.1':
            # v1.0 → v1.1: session_id 추가
            record['session_id'] = record.get('session_id', 'unknown')
        
        elif from_version == 'v1.1' and to_version == 'v1.2':
            # v1.1 → v1.2: value 타입 변경, properties 추가
            record['value'] = float(record.get('value', 0))
            record['properties'] = json.dumps(record.get('properties', {}))
        
        elif from_version == 'v1.0' and to_version == 'v1.2':
            # v1.0 → v1.2: 직접 변환
            record['session_id'] = record.get('session_id', 'unknown')
            record['value'] = float(record.get('value', 0))
            record['properties'] = json.dumps(record.get('properties', {}))
        
        return record
    
    def validate_record(self, record, version):
        """레코드 스키마 검증"""
        
        schema = self.schemas.get(version, {})
        errors = []
        
        for field, expected_type in schema.items():
            if field not in record:
                errors.append(f"Missing field: {field}")
                continue
            
            value = record[field]
            if not self.validate_type(value, expected_type):
                errors.append(f"Invalid type for {field}: expected {expected_type}")
        
        return len(errors) == 0, errors
    
    def validate_type(self, value, expected_type):
        """타입 검증"""
        
        if expected_type == 'string':
            return isinstance(value, str)
        elif expected_type == 'integer':
            return isinstance(value, int)
        elif expected_type == 'float' or expected_type == 'double':
            return isinstance(value, (int, float))
        else:
            return True

# 사용 예시
schema_manager = SchemaVersionManager()

# 기존 v1.0 레코드
old_record = {
    'timestamp': '2024-05-26T10:30:00Z',
    'event_type': 'click',
    'user_id': 12345,
    'value': 100
}

# v1.2로 마이그레이션
migrated_record = schema_manager.migrate_record(old_record, 'v1.0', 'v1.2')
is_valid, errors = schema_manager.validate_record(migrated_record, 'v1.2')

print(f"Migration result: {migrated_record}")
print(f"Validation: {is_valid}, Errors: {errors}")
```

## 실시간 데이터 품질 모니터링

### 품질 메트릭 계산
```python
class DataQualityMonitor:
    """실시간 데이터 품질 모니터링"""
    
    def __init__(self):
        self.metrics = {
            'total_records': 0,
            'valid_records': 0,
            'invalid_records': 0,
            'schema_errors': 0,
            'null_values': 0,
            'duplicate_records': 0
        }
        self.seen_records = set()
    
    def process_record(self, record):
        """레코드 품질 검사 및 메트릭 업데이트"""
        
        self.metrics['total_records'] += 1
        quality_issues = []
        
        # 1. 스키마 검증
        required_fields = ['timestamp', 'event_type', 'user_id']
        for field in required_fields:
            if field not in record or record[field] is None:
                quality_issues.append(f'missing_{field}')
                self.metrics['null_values'] += 1
        
        # 2. 데이터 타입 검증
        if 'user_id' in record:
            try:
                int(record['user_id'])
            except (ValueError, TypeError):
                quality_issues.append('invalid_user_id_type')
        
        # 3. 중복 검사
        record_hash = self.calculate_record_hash(record)
        if record_hash in self.seen_records:
            quality_issues.append('duplicate_record')
            self.metrics['duplicate_records'] += 1
        else:
            self.seen_records.add(record_hash)
        
        # 4. 비즈니스 규칙 검증
        if record.get('value', 0) < 0:
            quality_issues.append('negative_value')
        
        # 메트릭 업데이트
        if quality_issues:
            self.metrics['invalid_records'] += 1
            if any('missing_' in issue for issue in quality_issues):
                self.metrics['schema_errors'] += 1
        else:
            self.metrics['valid_records'] += 1
        
        # 품질 점수 계산
        quality_score = self.calculate_quality_score(record, quality_issues)
        
        return {
            'record': record,
            'quality_score': quality_score,
            'quality_issues': quality_issues,
            'is_valid': len(quality_issues) == 0
        }
    
    def calculate_record_hash(self, record):
        """레코드 해시 계산 (중복 감지용)"""
        
        # 주요 필드만 사용하여 해시 계산
        key_fields = ['timestamp', 'user_id', 'event_type']
        hash_input = ''.join(str(record.get(field, '')) for field in key_fields)
        return hash(hash_input)
    
    def calculate_quality_score(self, record, issues):
        """품질 점수 계산 (0-100)"""
        
        base_score = 100
        
        # 이슈별 감점
        penalty_map = {
            'missing_timestamp': 30,
            'missing_event_type': 20,
            'missing_user_id': 25,
            'invalid_user_id_type': 15,
            'duplicate_record': 50,
            'negative_value': 10
        }
        
        for issue in issues:
            base_score -= penalty_map.get(issue, 5)
        
        return max(0, base_score)
    
    def get_quality_report(self):
        """품질 리포트 생성"""
        
        total = self.metrics['total_records']
        if total == 0:
            return {'message': 'No records processed'}
        
        return {
            'total_records': total,
            'data_quality_percentage': (self.metrics['valid_records'] / total) * 100,
            'schema_compliance_percentage': ((total - self.metrics['schema_errors']) / total) * 100,
            'duplicate_percentage': (self.metrics['duplicate_records'] / total) * 100,
            'null_value_percentage': (self.metrics['null_values'] / total) * 100,
            'detailed_metrics': self.metrics
        }

# 실시간 품질 모니터링 사용 예시
quality_monitor = DataQualityMonitor()

# 샘플 레코드들 처리
sample_records = [
    {
        'timestamp': '2024-05-26T10:30:00Z',
        'event_type': 'click',
        'user_id': 12345,
        'value': 100
    },
    {
        'timestamp': '2024-05-26T10:31:00Z',
        'event_type': 'view',
        'user_id': 'invalid',  # 잘못된 타입
        'value': 50
    },
    {
        'event_type': 'purchase',  # timestamp 누락
        'user_id': 12347,
        'value': -10  # 음수값
    }
]

for record in sample_records:
    result = quality_monitor.process_record(record)
    print(f"Record quality: {result['quality_score']}, Issues: {result['quality_issues']}")

# 최종 품질 리포트
quality_report = quality_monitor.get_quality_report()
print(f"\n품질 리포트: {quality_report}")
```

## 권장사항

### 압축 형식 선택 가이드
* **실시간 처리**: Snappy (빠른 압축/해제)
* **장기 저장**: GZIP (높은 압축률)
* **균형 잡힌 성능**: ZSTD (압축률과 속도 균형)
* **최고 성능**: LZ4 (CPU 사용량 최소)

### 파티셔닝 전략
* **시간 기반**: 시계열 데이터 분석에 최적
* **지역 기반**: 글로벌 서비스의 지역별 분석
* **이벤트 타입**: 다양한 이벤트가 혼재된 경우
* **하이브리드**: 시간 + 카테고리 조합

### 스키마 관리
* **하위 호환성**: 필드 추가는 가능, 삭제는 신중히
* **버전 관리**: 명확한 버전 체계 구축
* **마이그레이션**: 점진적 스키마 변경
* **검증**: 자동화된 스키마 검증 도구 활용
