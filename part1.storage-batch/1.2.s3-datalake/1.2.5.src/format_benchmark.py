import pandas as pd
import time
import os
import json
from pathlib import Path

def benchmark_file_formats():
    """íŒŒì¼ í¬ë§·ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
    df = pd.read_pickle('/home/ec2-user/test_dataset.pkl')
    
    # í¬ë§·ë³„ ì„¤ì •
    formats = {
        'csv': {
            'save': lambda df, path: df.to_csv(path, index=False),
            'load': lambda path: pd.read_csv(path, parse_dates=['timestamp']),
            'ext': '.csv'
        },
        'json': {
            'save': lambda df, path: df.to_json(path, orient='records', date_format='iso'),
            'load': lambda path: pd.read_json(path, orient='records'),
            'ext': '.json'
        },
        'parquet_snappy': {
            'save': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        },
        'parquet_gzip': {
            'save': lambda df, path: df.to_parquet(path, compression='gzip', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        }
    }
    
    # ORC ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        formats['orc'] = {
            'save': lambda df, path: df.to_orc(path, compression='snappy'),
            'load': lambda path: pd.read_orc(path),
            'ext': '.orc'
        }
    except:
        print("âš ï¸ ORC í¬ë§· ìŠ¤í‚µ (pyarrow ë²„ì „ í™•ì¸ í•„ìš”)")
    
    print(f"\n{'í¬ë§·':<20} {'ì €ì¥(ì´ˆ)':<10} {'ë¡œë“œ(ì´ˆ)':<10} {'í¬ê¸°(MB)':<12} {'ì••ì¶•ë¥ ':<12}")
    print("=" * 75)
    
    results = {}
    baseline_size = None
    
    for fmt_name, fmt_config in formats.items():
        file_path = f"test_data_{fmt_name}{fmt_config['ext']}"
        
        try:
            # 1. ì €ì¥ ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            fmt_config['save'](df, file_path)
            save_time = time.time() - start_time
            
            # 2. íŒŒì¼ í¬ê¸° ì¸¡ì •
            file_size_mb = os.path.getsize(file_path) / 1024 / 1024
            
            # 3. ë¡œë”© ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            loaded_df = fmt_config['load'](file_path)
            load_time = time.time() - start_time
            
            # 4. ì••ì¶•ë¥  ê³„ì‚°
            if fmt_name == 'csv':
                baseline_size = file_size_mb
                compression = "ê¸°ì¤€"
            else:
                reduction = ((baseline_size - file_size_mb) / baseline_size) * 100
                compression = f"{reduction:+.1f}%"
            
            # 5. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
            assert len(loaded_df) == len(df), f"{fmt_name}: ë ˆì½”ë“œ ìˆ˜ ë¶ˆì¼ì¹˜"
            
            # ê²°ê³¼ ì €ì¥
            results[fmt_name] = {
                'save_time': save_time,
                'load_time': load_time,
                'file_size_mb': file_size_mb,
                'compression': compression
            }
            
            print(f"{fmt_name:<20} {save_time:<10.2f} {load_time:<10.2f} {file_size_mb:<12.2f} {compression:<12}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(file_path)
            
        except Exception as e:
            print(f"{fmt_name:<20} âŒ ì˜¤ë¥˜: {str(e)[:40]}")
            results[fmt_name] = {'error': str(e)}
    
    return results

def query_performance_test():
    """ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ"""
    
    print("\n=== ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    df = pd.read_pickle('/home/ec2-user/test_dataset.pkl')
    
    # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ìƒì„±
    df.to_csv('query_test.csv', index=False)
    df.to_parquet('query_test.parquet', compression='snappy', index=False)
    
    print(f"{'ì¿¼ë¦¬ ìœ í˜•':<30} {'CSV(ì´ˆ)':<10} {'Parquet(ì´ˆ)':<12} {'ì„±ëŠ¥í–¥ìƒ':<10}")
    print("-" * 70)
    
    # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    queries = [
        {
            'name': 'ì „ì²´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ',
            'csv_func': lambda: len(pd.read_csv('query_test.csv')),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet'))
        },
        {
            'name': 'ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§',
            'csv_func': lambda: len(pd.read_csv('query_test.csv').query("category == 'electronics'")),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet').query("category == 'electronics'"))
        },
        {
            'name': 'ê¸ˆì•¡ í‰ê·  ê³„ì‚°',
            'csv_func': lambda: pd.read_csv('query_test.csv')['amount'].mean(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet')['amount'].mean()
        },
        {
            'name': 'ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„',
            'csv_func': lambda: pd.read_csv('query_test.csv').groupby('category')['amount'].sum().to_dict(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet').groupby('category')['amount'].sum().to_dict()
        }
    ]
    
    for query in queries:
        # CSV ì„±ëŠ¥
        start = time.time()
        csv_result = query['csv_func']()
        csv_time = time.time() - start
        
        # Parquet ì„±ëŠ¥
        start = time.time()
        parquet_result = query['parquet_func']()
        parquet_time = time.time() - start
        
        # ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
        speedup = f"{csv_time/parquet_time:.1f}ë°°" if parquet_time > 0 else "N/A"
        
        print(f"{query['name']:<30} {csv_time:<10.3f} {parquet_time:<12.3f} {speedup:<10}")
    
    # ì •ë¦¬
    os.remove('query_test.csv')
    os.remove('query_test.parquet')

def analyze_results_and_recommendations(results):
    """ê²°ê³¼ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­"""
    
    print("\n=== ì„±ëŠ¥ ë¶„ì„ ===")
    
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if not valid_results:
        print("ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
    fastest_save = min(valid_results.items(), key=lambda x: x[1]['save_time'])
    fastest_load = min(valid_results.items(), key=lambda x: x[1]['load_time'])
    smallest_file = min(valid_results.items(), key=lambda x: x[1]['file_size_mb'])
    
    print(f"ğŸš€ ê°€ì¥ ë¹ ë¥¸ ì €ì¥: {fastest_save[0]} ({fastest_save[1]['save_time']:.2f}ì´ˆ)")
    print(f"âš¡ ê°€ì¥ ë¹ ë¥¸ ë¡œë“œ: {fastest_load[0]} ({fastest_load[1]['load_time']:.2f}ì´ˆ)")
    print(f"ğŸ’¾ ê°€ì¥ ì‘ì€ íŒŒì¼: {smallest_file[0]} ({smallest_file[1]['file_size_mb']:.2f}MB)")
    
    print("\n=== ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ì‚¬í•­ ===")
    
    scenarios = {
        "ğŸ”„ ë°ì´í„° êµí™˜/í˜¸í™˜ì„±": {
            "ê¶Œì¥": "CSV",
            "ì´ìœ ": "ëª¨ë“  ë„êµ¬ì—ì„œ ì§€ì›, ì‚¬ëŒì´ ì½ê¸° ê°€ëŠ¥",
            "ìš©ë„": "ì†Œê·œëª¨ ë°ì´í„°, ìˆ˜ë™ ê²€í†  í•„ìš”í•œ ê²½ìš°"
        },
        "ğŸŒ ì›¹ API/NoSQL": {
            "ê¶Œì¥": "JSON",
            "ì´ìœ ": "ì›¹ í‘œì¤€, ì¤‘ì²© êµ¬ì¡° ì§€ì›",
            "ìš©ë„": "API ì‘ë‹µ, ì„¤ì • íŒŒì¼, ë°˜êµ¬ì¡°í™” ë°ì´í„°"
        },
        "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„": {
            "ê¶Œì¥": "Parquet (Snappy)",
            "ì´ìœ ": "ë¹ ë¥¸ ì••ì¶•/í•´ì œ, ì»¬ëŸ¼í˜• ìµœì í™”",
            "ìš©ë„": "BI ë„êµ¬, ë¹ˆë²ˆí•œ ì¿¼ë¦¬, ëŒ€í™”í˜• ë¶„ì„"
        },
        "ğŸ’° ì¥ê¸° ë³´ê´€/ì•„ì¹´ì´ë¹™": {
            "ê¶Œì¥": "Parquet (GZIP)",
            "ì´ìœ ": "ìµœê³  ì••ì¶•ë¥ , ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½",
            "ìš©ë„": "ê³¼ê±° ë°ì´í„°, ê·œì • ì¤€ìˆ˜, ë°±ì—…"
        },
        "ğŸ­ Hadoop ìƒíƒœê³„": {
            "ê¶Œì¥": "ORC",
            "ì´ìœ ": "Hive ìµœì í™”, íŠ¸ëœì­ì…˜ ì§€ì›",
            "ìš©ë„": "Hive ì¿¼ë¦¬, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬"
        }
    }
    
    for scenario, info in scenarios.items():
        print(f"\n{scenario}")
        print(f"  ê¶Œì¥ í¬ë§·: {info['ê¶Œì¥']}")
        print(f"  ì„ íƒ ì´ìœ : {info['ì´ìœ ']}")
        print(f"  ì í•© ìš©ë„: {info['ìš©ë„']}")

if __name__ == "__main__":
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸
    if not os.path.exists('/home/ec2-user/test_dataset.pkl'):
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € load_test_data.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    
    # 2. í¬ë§· ì„±ëŠ¥ ë¹„êµ
    results = benchmark_file_formats()
    
    # 3. ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    query_performance_test()
    
    # 4. ê²°ê³¼ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
    analyze_results_and_recommendations(results)
    
    # 5. ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    with open('~/format_benchmark_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“ˆ ìƒì„¸ ê²°ê³¼ ì €ì¥: ~/format_benchmark_results.json")