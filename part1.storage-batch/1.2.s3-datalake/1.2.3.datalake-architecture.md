# 2.3 데이터 레이크 아키텍처 설계

## 3계층 데이터 레이크 아키텍처

### Bronze Layer (Raw Zone)
* **목적**: 원본 데이터를 변경 없이 저장
* **특징**: 
    - 데이터 불변성 보장
    - 모든 원본 데이터 보존
    - 최소한의 메타데이터만 추가
* **저장 형식**: 원본 형식 유지 (JSON, CSV, 로그 등)

```
s3://datalake-bucket/bronze/
├── raw-events/
│   └── year=2024/month=05/day=26/
│       ├── events_001.json.gz
│       └── events_002.json.gz
├── raw-logs/
│   └── application=web/date=2024-05-26/
│       └── access.log.gz
└── raw-files/
    └── uploads/
        ├── file1.csv
        └── file2.xlsx
```

### Silver Layer (Refined Zone)
* **목적**: 정제되고 표준화된 데이터
* **특징**:
    - 데이터 품질 검증 완료
    - 표준화된 스키마 적용
    - 중복 제거 및 데이터 정규화
* **저장 형식**: Parquet, Delta Lake

```
s3://datalake-bucket/silver/
├── events/
│   └── year=2024/month=05/
│       └── events_clean.parquet
├── user-profiles/
│   └── date=2024-05-26/
│       └── profiles_deduplicated.parquet
└── transactions/
    └── year=2024/month=05/
        └── transactions_validated.parquet
```

### Gold Layer (Curated Zone)
* **목적**: 비즈니스 로직이 적용된 최종 데이터
* **특징**:
    - 비즈니스 규칙 적용
    - 집계 및 요약 데이터
    - 리포팅 및 분석 최적화
* **저장 형식**: Parquet, 사전 집계 테이블

```
s3://datalake-bucket/gold/
├── analytics/
│   ├── daily-user-metrics/
│   │   └── date=2024-05-26/
│   └── monthly-revenue/
│       └── year=2024/month=05/
├── ml-features/
│   └── user-behavior-features/
└── reports/
    ├── executive-dashboard/
    └── operational-metrics/
```

## 데이터 플로우 설계

### ETL 파이프라인 구조
```python
# Bronze → Silver 변환
def bronze_to_silver(bronze_path, silver_path):
    """원본 데이터를 정제하여 Silver로 이동"""
    
    # 원본 데이터 읽기
    raw_df = spark.read.json(bronze_path)
    
    # 데이터 정제
    cleaned_df = raw_df \
        .dropDuplicates() \
        .filter(col("user_id").isNotNull()) \
        .withColumn("timestamp", to_timestamp("event_time")) \
        .withColumn("processed_date", current_date())
    
    # 데이터 품질 검증
    quality_check = cleaned_df \
        .agg(
            count("*").alias("total_records"),
            countDistinct("user_id").alias("unique_users"),
            sum(when(col("timestamp").isNull(), 1).otherwise(0)).alias("null_timestamps")
        ).collect()[0]
    
    print(f"품질 체크: {quality_check}")
    
    # Silver에 저장
    cleaned_df \
        .write \
        .mode("overwrite") \
        .partitionBy("year", "month") \
        .parquet(silver_path)

# Silver → Gold 변환
def silver_to_gold(silver_path, gold_path):
    """Silver 데이터를 비즈니스 로직으로 변환"""
    
    silver_df = spark.read.parquet(silver_path)
    
    # 비즈니스 메트릭 계산
    user_metrics = silver_df \
        .groupBy("user_id", "date") \
        .agg(
            count("*").alias("total_events"),
            countDistinct("event_type").alias("event_variety"),
            sum("value").alias("daily_value"),
            max("timestamp").alias("last_activity")
        ) \
        .withColumn("user_segment", 
            when(col("daily_value") > 1000, "premium")
            .when(col("daily_value") > 100, "standard")
            .otherwise("basic")
        )
    
    # Gold에 저장
    user_metrics \
        .write \
        .mode("overwrite") \
        .partitionBy("date") \
        .parquet(gold_path)
```

## 데이터 거버넌스

### 스키마 관리
```python
from pyspark.sql.types import *

# 표준 스키마 정의
EVENT_SCHEMA = StructType([
    StructField("event_id", StringType(), False),
    StructField("user_id", LongType(), False),
    StructField("event_type", StringType(), False),
    StructField("timestamp", TimestampType(), False),
    StructField("properties", MapType(StringType(), StringType()), True),
    StructField("value", DecimalType(10, 2), True)
])

# 스키마 진화 관리
def validate_schema_evolution(old_schema, new_schema):
    """스키마 변경 호환성 검증"""
    
    old_fields = {field.name: field for field in old_schema.fields}
    new_fields = {field.name: field for field in new_schema.fields}
    
    # 삭제된 필드 확인
    removed_fields = set(old_fields.keys()) - set(new_fields.keys())
    if removed_fields:
        raise Exception(f"Breaking change: Removed fields {removed_fields}")
    
    # 타입 변경 확인
    for field_name in old_fields:
        if field_name in new_fields:
            old_type = old_fields[field_name].dataType
            new_type = new_fields[field_name].dataType
            if old_type != new_type:
                print(f"Warning: Type changed for {field_name}: {old_type} -> {new_type}")
    
    return True
```

### 데이터 품질 관리
```python
class DataQualityChecker:
    def __init__(self, df):
        self.df = df
        self.quality_issues = []
    
    def check_completeness(self, required_columns):
        """필수 컬럼 완성도 검사"""
        for col_name in required_columns:
            null_count = self.df.filter(col(col_name).isNull()).count()
            total_count = self.df.count()
            completeness = (total_count - null_count) / total_count * 100
            
            if completeness < 95:  # 95% 미만시 경고
                self.quality_issues.append(f"{col_name}: {completeness:.1f}% 완성도")
    
    def check_uniqueness(self, unique_columns):
        """고유성 검사"""
        for col_name in unique_columns:
            total_count = self.df.count()
            unique_count = self.df.select(col_name).distinct().count()
            
            if total_count != unique_count:
                duplicate_count = total_count - unique_count
                self.quality_issues.append(f"{col_name}: {duplicate_count}개 중복")
    
    def check_validity(self, validation_rules):
        """유효성 검사"""
        for col_name, rule_func in validation_rules.items():
            invalid_count = self.df.filter(~rule_func(col(col_name))).count()
            
            if invalid_count > 0:
                self.quality_issues.append(f"{col_name}: {invalid_count}개 유효하지 않음")
    
    def generate_report(self):
        """품질 리포트 생성"""
        if not self.quality_issues:
            return "✅ 모든 품질 검사 통과"
        else:
            return f"⚠️ 품질 이슈 발견:\n" + "\n".join(self.quality_issues)

# 사용 예시
quality_checker = DataQualityChecker(df)
quality_checker.check_completeness(["user_id", "event_type", "timestamp"])
quality_checker.check_uniqueness(["event_id"])
quality_checker.check_validity({
    "user_id": lambda x: x > 0,
    "value": lambda x: x >= 0
})

print(quality_checker.generate_report())
```

## 보안 및 접근 제어

### 계층별 보안 정책
```python
# Terraform으로 계층별 IAM 정책 구현
resource "aws_iam_policy" "bronze_read_only" {
  name = "bronze-read-only"
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:ListBucket"
        ]
        Resource = [
          "arn:aws:s3:::${var.bucket_name}/bronze/*"
        ]
      }
    ]
  })
}

resource "aws_iam_policy" "silver_read_write" {
  name = "silver-read-write"
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket"
        ]
        Resource = [
          "arn:aws:s3:::${var.bucket_name}/silver/*"
        ]
      }
    ]
  })
}
```

### 데이터 암호화
```python
# 클라이언트 사이드 암호화
import boto3
from botocore.client import Config

def setup_encryption(bucket_name):
    s3 = boto3.client('s3')
    
    # 버킷 암호화 설정
    s3.put_bucket_encryption(
        Bucket=bucket_name,
        ServerSideEncryptionConfiguration={
            'Rules': [
                {
                    'ApplyServerSideEncryptionByDefault': {
                        'SSEAlgorithm': 'AES256'
                    },
                    'BucketKeyEnabled': True
                }
            ]
        }
    )
```

## 메타데이터 관리

### Glue Data Catalog 통합
```python
# Glue 테이블 자동 생성
def create_glue_table(database_name, table_name, s3_location, schema):
    glue = boto3.client('glue')
    
    table_input = {
        'Name': table_name,
        'StorageDescriptor': {
            'Columns': [
                {
                    'Name': field.name,
                    'Type': convert_spark_to_hive_type(field.dataType)
                }
                for field in schema.fields
            ],
            'Location': s3_location,
            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
            'SerdeInfo': {
                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
            }
        },
        'PartitionKeys': [
            {'Name': 'year', 'Type': 'string'},
            {'Name': 'month', 'Type': 'string'}
        ]
    }
    
    try:
        glue.create_table(
            DatabaseName=database_name,
            TableInput=table_input
        )
        print(f"테이블 생성 완료: {database_name}.{table_name}")
    except glue.exceptions.AlreadyExistsException:
        print(f"테이블 이미 존재: {database_name}.{table_name}")
```

### 데이터 카탈로그 자동화
```python
def auto_catalog_data(s3_path, database_name):
    """S3 데이터를 자동으로 카탈로그에 등록"""
    
    # 데이터 스키마 자동 감지
    sample_df = spark.read.parquet(s3_path).limit(1000)
    inferred_schema = sample_df.schema
    
    # 파티션 정보 추출
    partitions = extract_partitions_from_path(s3_path)
    
    # Glue 테이블 생성
    table_name = s3_path.split('/')[-2]  # 경로에서 테이블명 추출
    create_glue_table(database_name, table_name, s3_path, inferred_schema)
    
    # 파티션 추가
    for partition in partitions:
        add_partition_to_glue(database_name, table_name, partition)
```

## 성능 최적화

### 압축 및 포맷 최적화
```python
def optimize_table_format(input_path, output_path, optimization_type="balanced"):
    """테이블 포맷 최적화"""
    
    df = spark.read.parquet(input_path)
    
    if optimization_type == "storage":
        # 저장 공간 최적화 (높은 압축률)
        df.write \
          .option("compression", "gzip") \
          .mode("overwrite") \
          .parquet(output_path)
    
    elif optimization_type == "query":
        # 쿼리 성능 최적화 (빠른 읽기)
        df.write \
          .option("compression", "snappy") \
          .mode("overwrite") \
          .parquet(output_path)
    
    elif optimization_type == "balanced":
        # 균형 최적화
        df.coalesce(4) \
          .write \
          .option("compression", "snappy") \
          .mode("overwrite") \
          .parquet(output_path)
```

### 자동 최적화 스케줄러
```python
import schedule
import time

def optimize_data_lake():
    """데이터 레이크 정기 최적화"""
    
    # 1. 작은 파일 병합
    merge_small_files()
    
    # 2. 파티션 통계 업데이트
    update_partition_statistics()
    
    # 3. 데이터 품질 체크
    run_quality_checks()
    
    # 4. 스토리지 비용 최적화
    optimize_storage_classes()

# 매일 새벽 2시에 최적화 실행
schedule.every().day.at("02:00").do(optimize_data_lake)

while True:
    schedule.run_pending()
    time.sleep(3600)  # 1시간마다 체크
```

## 모니터링 및 알림

### CloudWatch 메트릭
```python
import boto3

def publish_data_lake_metrics():
    """데이터 레이크 메트릭 CloudWatch에 전송"""
    
    cloudwatch = boto3.client('cloudwatch')
    
    # 계층별 데이터 크기 측정
    layers = ['bronze', 'silver', 'gold']
    
    for layer in layers:
        size_bytes = get_s3_size(f"s3://bucket/{layer}/")
        
        cloudwatch.put_metric_data(
            Namespace='DataLake',
            MetricData=[
                {
                    'MetricName': 'StorageSize',
                    'Dimensions': [
                        {
                            'Name': 'Layer',
                            'Value': layer
                        }
                    ],
                    'Value': size_bytes,
                    'Unit': 'Bytes'
                }
            ]
        )
```

### 데이터 품질 알림
```python
import boto3

def send_quality_alert(issues):
    """데이터 품질 이슈 알림"""
    
    sns = boto3.client('sns')
    
    message = f"""
    데이터 품질 이슈 발견:
    
    {chr(10).join(issues)}
    
    확인 필요한 테이블: {table_name}
    시간: {datetime.now()}
    """
    
    sns.publish(
        TopicArn='arn:aws:sns:region:account:data-quality-alerts',
        Message=message,
        Subject='데이터 품질 알림'
    )
```

## 아키텍처 구현 예제

### Bronze Layer 구현
```python
class BronzeLayer:
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
        self.base_path = f"s3://{bucket_name}/bronze"
    
    def ingest_streaming_data(self, source_stream, target_path):
        """스트리밍 데이터 Bronze 저장"""
        
        streaming_df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", source_stream) \
            .load()
        
        query = streaming_df \
            .writeStream \
            .format("parquet") \
            .option("path", f"{self.base_path}/{target_path}") \
            .option("checkpointLocation", f"{self.base_path}/checkpoints/{target_path}") \
            .partitionBy("year", "month", "day") \
            .start()
        
        return query
    
    def ingest_batch_data(self, source_path, target_path):
        """배치 데이터 Bronze 저장"""
        
        df = spark.read.option("multiline", "true").json(source_path)
        
        # 메타데이터 추가
        df_with_metadata = df \
            .withColumn("ingestion_timestamp", current_timestamp()) \
            .withColumn("source_file", input_file_name())
        
        df_with_metadata \
            .write \
            .mode("append") \
            .partitionBy("year", "month", "day") \
            .parquet(f"{self.base_path}/{target_path}")
```

### Silver Layer 구현
```python
class SilverLayer:
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
        self.base_path = f"s3://{bucket_name}/silver"
    
    def transform_bronze_to_silver(self, bronze_path, silver_path, transformation_rules):
        """Bronze 데이터를 Silver로 변환"""
        
        bronze_df = spark.read.parquet(bronze_path)
        
        # 데이터 정제 적용
        silver_df = self.apply_transformations(bronze_df, transformation_rules)
        
        # 품질 검증
        quality_checker = DataQualityChecker(silver_df)
        quality_report = quality_checker.generate_report()
        
        if "이슈 발견" in quality_report:
            print(f"Warning: {quality_report}")
        
        # Silver에 저장
        silver_df \
            .write \
            .mode("overwrite") \
            .option("compression", "snappy") \
            .parquet(f"{self.base_path}/{silver_path}")
        
        return quality_report
    
    def apply_transformations(self, df, rules):
        """데이터 변환 규칙 적용"""
        
        transformed_df = df
        
        for rule in rules:
            if rule['type'] == 'filter':
                transformed_df = transformed_df.filter(rule['condition'])
            elif rule['type'] == 'deduplicate':
                transformed_df = transformed_df.dropDuplicates(rule['columns'])
            elif rule['type'] == 'standardize':
                transformed_df = transformed_df.withColumn(
                    rule['column'], 
                    rule['function'](col(rule['column']))
                )
        
        return transformed_df
```

### Gold Layer 구현
```python
class GoldLayer:
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
        self.base_path = f"s3://{bucket_name}/gold"
    
    def create_business_metrics(self, silver_path, metrics_config):
        """비즈니스 메트릭 생성"""
        
        silver_df = spark.read.parquet(silver_path)
        
        metrics = {}
        
        for metric_name, config in metrics_config.items():
            if config['type'] == 'aggregation':
                metric_df = silver_df \
                    .groupBy(*config['group_by']) \
                    .agg(*config['aggregations'])
                
                metrics[metric_name] = metric_df
        
        return metrics
    
    def save_analytics_table(self, df, table_name, partition_cols=None):
        """분석 테이블 저장"""
        
        writer = df.write.mode("overwrite").option("compression", "snappy")
        
        if partition_cols:
            writer = writer.partitionBy(*partition_cols)
        
        writer.parquet(f"{self.base_path}/analytics/{table_name}")
```

## 실제 구현 예제

### 전체 파이프라인 구현
```python
def run_daily_pipeline():
    """일일 데이터 파이프라인 실행"""
    
    # 1. Bronze Layer: 원본 데이터 수집
    bronze = BronzeLayer("my-datalake")
    bronze.ingest_batch_data(
        "s3://raw-data/events/", 
        "events"
    )
    
    # 2. Silver Layer: 데이터 정제
    silver = SilverLayer("my-datalake")
    transformation_rules = [
        {'type': 'filter', 'condition': col('user_id').isNotNull()},
        {'type': 'deduplicate', 'columns': ['event_id']},
        {'type': 'standardize', 'column': 'email', 'function': lower}
    ]
    
    quality_report = silver.transform_bronze_to_silver(
        "s3://my-datalake/bronze/events/",
        "events",
        transformation_rules
    )
    
    # 3. Gold Layer: 비즈니스 메트릭 생성
    gold = GoldLayer("my-datalake")
    metrics_config = {
        'daily_user_activity': {
            'type': 'aggregation',
            'group_by': ['user_id', 'date'],
            'aggregations': [
                count('*').alias('event_count'),
                sum('value').alias('total_value')
            ]
        }
    }
    
    metrics = gold.create_business_metrics(
        "s3://my-datalake/silver/events/",
        metrics_config
    )
    
    # 결과 저장
    for metric_name, metric_df in metrics.items():
        gold.save_analytics_table(
            metric_df, 
            metric_name, 
            partition_cols=['date']
        )
    
    print("파이프라인 실행 완료")
    return quality_report

# 실행
if __name__ == "__main__":
    report = run_daily_pipeline()
    print(report)
```

이 아키텍처 설계는 확장 가능하고 유지보수하기 쉬운 데이터 레이크를 구축하는 실전 가이드임. 각 계층의 역할이 명확하고, 데이터 품질과 거버넌스를 보장하면서도 성능을 최적화할 수 있음.

## 참고 링크
- [AWS Data Lake 아키텍처 센터](https://aws.amazon.com/ko/architecture/data-lake/)
- [AWS Glue 공식 문서](https://docs.aws.amazon.com/ko_kr/glue/index.html)
- [AWS Lake Formation 공식 문서](https://docs.aws.amazon.com/ko_kr/lake-formation/latest/dg/what-is-lake-formation.html)
- [데이터 거버넌스와 보안](https://aws.amazon.com/ko/big-data/datalakes-and-analytics/data-governance/)
