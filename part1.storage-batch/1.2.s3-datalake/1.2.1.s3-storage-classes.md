# 1.2.1. S3 버킷 및 스토리지 클래스


## S3 버킷 기본 개념
* 주소 : https://console.aws.amazon.com/s3/
* S3란? AWS S3는 인터넷을 통해 언제 어디서나 데이터를 저장하고 검색할 수 있는 객체 스토리지 서비스임.

### 버킷 구조
```
S3 버킷 = 글로벌 네임스페이스의 컨테이너
├── 객체(Object) = 실제 데이터 파일
├── 키(Key) = 객체의 고유 식별자 (파일 경로)
└── 메타데이터 = 객체 속성 정보
```

### 네이밍 규칙
* **전역 고유성**: 모든 AWS 계정에서 유일해야 함
* **명명 규칙**: 소문자, 숫자, 하이픈(-) 사용
* **길이 제한**: 3-63자
* **DNS 호환**: 점(.) 사용 시 SSL 인증서 문제 가능

### 리전 및 가용성
* 버킷은 특정 리전에 생성
* 데이터는 해당 리전 내에서만 저장
* 교차 리전 복제(CRR) 설정 가능

## 스토리지 클래스 개요

### 스토리지 클래스 분류
```
접근 빈도별 분류:
├── 빈번한 액세스: Standard
├── 가끔 액세스: Standard-IA, One Zone-IA
├── 아카이브: Glacier, Deep Archive
└── 지능형: Intelligent-Tiering
```

## 주요 스토리지 클래스

### 1. Standard (기본)
* **특징**: 높은 내구성(99.999999999%), 가용성(99.99%)
* **용도**: 빈번히 액세스되는 데이터
* **비용**: 저장 비용 높음, 검색 비용 없음
* **SLA**: 99.9% 가용성

### 2. Standard-Infrequent Access (IA)
* **특징**: Standard와 동일한 내구성, 낮은 가용성(99.9%)
* **용도**: 월 1회 미만 액세스하는 데이터
* **비용**: 저장 비용 낮음, 검색 시 요금 발생
* **최소 저장 기간**: 30일

### 3. One Zone-Infrequent Access
* **특징**: 단일 가용 영역에 저장
* **용도**: 재생성 가능한 비중요 데이터
* **비용**: IA 대비 20% 저렴
* **위험**: 가용 영역 장애 시 데이터 손실 가능

### 4. Glacier
* **특징**: 장기 아카이브용
* **검색 시간**: 1분 ~ 12시간
* **용도**: 백업, 규정 준수 데이터
* **비용**: 매우 저렴한 저장 비용
* **최소 저장 기간**: 90일

### 5. Glacier Deep Archive
* **특징**: 최장기 아카이브용
* **검색 시간**: 12 ~ 48시간
* **용도**: 7-10년 이상 보관하는 데이터
* **비용**: 가장 저렴한 저장 비용
* **최소 저장 기간**: 180일

### 6. Intelligent-Tiering
* **특징**: 액세스 패턴에 따른 자동 계층 이동
* **계층**:
  - 빈번한 액세스: Standard 비용
  - 가끔 액세스: IA 비용 (30일 후)
  - 아카이브: Glacier 비용 (90일 후)
  - 딥 아카이브: Deep Archive 비용 (180일 후)
* **모니터링 비용**: 객체당 월 $0.0025

## 빅데이터 관점에서의 활용

### 데이터 생명주기별 전략
```
Hot Data (실시간 분석)
├── Standard 클래스
├── 빈번한 ETL 작업
└── 실시간 대시보드

Warm Data (정기 분석)
├── Standard-IA 클래스
├── 월/주간 리포트
└── 과거 데이터 분석

Cold Data (장기 보관)
├── Glacier 클래스
├── 규정 준수
└── 백업 데이터
```

### 데이터 레이크 계층화
| 계층 | 스토리지 클래스 | 용도 | 보관 기간 |
|------|---------------|------|-----------|
| Raw Data | Standard | 원본 데이터 수집 | 30일 |
| Processed Data | Standard-IA | 변환된 데이터 | 1년 |
| Historical Data | Glacier | 과거 데이터 | 7년 |
| Archive Data | Deep Archive | 법적 요구사항 | 10년+ |

## 비용 최적화 전략

### 라이프사이클 정책
```json
{
  "Rules": [{
    "ID": "DataLakeLifecycle",
    "Status": "Enabled",
    "Transitions": [
      {
        "Days": 30,
        "StorageClass": "STANDARD_IA"
      },
      {
        "Days": 90, 
        "StorageClass": "GLACIER"
      },
      {
        "Days": 365,
        "StorageClass": "DEEP_ARCHIVE"
      }
    ]
  }]
}
```

### 비용 구성 요소
* **저장 비용**: GB당 월 요금
* **요청 비용**: PUT, GET, DELETE 요청 당 비용
* **검색 비용**: IA, Glacier에서 데이터 가져올 때
* **전송 비용**: 인터넷으로 데이터 전송 시

### 최적화 가이드라인
1. **접근 패턴 분석**: CloudWatch 메트릭 활용
2. **적절한 클래스 선택**: 비용 대비 성능 고려
3. **압축 활용**: GZIP, Snappy 등으로 용량 절약
4. **파티셔닝**: 불필요한 스캔 방지

## 성능 고려사항

### 처리량 최적화
* **멀티파트 업로드**: 100MB 이상 파일
* **병렬 처리**: 프리픽스 분산으로 핫스팟 방지
* **전송 가속**: CloudFront 활용

### 일관성 모델
* **읽기 후 쓰기 일관성**: 새 객체
* **최종 일관성**: 기존 객체 업데이트/삭제

## 보안 및 규정 준수

### 액세스 제어
* **IAM 정책**: 사용자/역할 기반 권한
* **버킷 정책**: 리소스 기반 권한
* **ACL**: 객체 레벨 권한 (레거시)

### 암호화
* **서버 측 암호화**: SSE-S3, SSE-KMS, SSE-C
* **클라이언트 측 암호화**: 업로드 전 암호화
* **전송 중 암호화**: HTTPS/TLS

### 규정 준수
* **버전 관리**: 데이터 변경 이력 추적
* **MFA 삭제**: 중요 데이터 보호
* **액세스 로깅**: 모든 요청 기록

## 모니터링 및 관리

### CloudWatch 메트릭
* **스토리지 사용량**: BucketSizeBytes
* **객체 수**: NumberOfObjects
* **요청 메트릭**: GetRequests, PutRequests

### 비용 관리
* **Cost Explorer**: 스토리지 클래스별 비용 분석
* **S3 Storage Lens**: 사용 패턴 인사이트
* **S3 Analytics**: 액세스 패턴 분석

## 모범 사례

### 네이밍 규칙
```
회사명-환경-목적-리전-계정ID
예: mycompany-prod-datalake-us-east-1-123456789
```

### 태깅 전략
```json
{
  "Environment": "Production",
  "Project": "DataLake",
  "Owner": "DataTeam",
  "CostCenter": "Engineering"
}
```

### 백업 전략
* **교차 리전 복제**: 재해 복구
* **버전 관리**: 데이터 복구
* **정기 백업**: 중요 데이터 별도 보관

## 참고 자료
* [S3 스토리지 클래스](https://docs.aws.amazon.com/s3/latest/userguide/storage-class-intro.html)
* [S3 요금](https://aws.amazon.com/s3/pricing/)
* [S3 성능 가이드라인](https://docs.aws.amazon.com/s3/latest/userguide/optimizing-performance.html)
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/s3/index.html)
- [Hadoop에서 S3로 데이터 마이그레이션](https://aws.amazon.com/ko/blogs/big-data/migrating-hadoop-workloads-to-amazon-emr/)
- [S3 API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html)
