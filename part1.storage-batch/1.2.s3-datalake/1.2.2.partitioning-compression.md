# 2.2 파티셔닝 및 압축 전략

## 파티셔닝 전략

### 파티셔닝 설계 원칙
* **쿼리 패턴 기반**: 자주 사용하는 필터 조건을 파티션 키로 선택
* **카디널리티 고려**: 너무 적거나 많은 파티션 생성 방지
* **균등 분산**: 파티션별 데이터 크기가 비슷하도록 설계
* **미래 확장성**: 향후 데이터 증가를 고려한 설계

### 시간 기반 파티셔닝
```
# 계층적 시간 파티셔닝
s3://my-datalake/
├── events/
│   └── year=2024/
│       └── month=05/
│           └── day=26/
│               └── hour=14/
│                   ├── part-00000.parquet
│                   └── part-00001.parquet
```

### 카테고리 기반 파티셔닝
```
# 비즈니스 로직 기반 파티셔닝
s3://my-datalake/
├── sales/
│   └── region=us-west/
│       └── department=electronics/
│           └── year=2024/
│               └── month=05/
```

### 하이브리드 파티셔닝
```python
# 복합 파티셔닝 예제
df.write \
  .mode("overwrite") \
  .partitionBy("year", "month", "region") \
  .parquet("s3://bucket/sales-data/")
```

## 압축 전략

### 압축 포맷 비교
| 포맷 | 압축률 | 압축속도 | 해제속도 | CPU 사용량 | 사용 사례 |
|------|--------|----------|----------|------------|-----------|
| Snappy | 중간 | 빠름 | 빠름 | 낮음 | 일반적 사용 |
| GZIP | 높음 | 느림 | 중간 | 높음 | 저장 공간 중요 |
| LZ4 | 낮음 | 매우빠름 | 매우빠름 | 매우낮음 | 실시간 처리 |
| ZSTD | 높음 | 중간 | 빠름 | 중간 | 균형잡힌 성능 |

### Parquet 압축 설정
```python
# Spark에서 Parquet 압축 설정
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

# 다양한 압축 코덱 성능 비교
compression_codecs = ["snappy", "gzip", "lz4", "zstd"]

for codec in compression_codecs:
    df.write \
      .option("compression", codec) \
      .mode("overwrite") \
      .parquet(f"s3://bucket/data-{codec}/")
```

### 압축 성능 벤치마크

> **실습 소스 위치:** `1.2.2.src/compression_benchmark.py`

압축 성능 벤치마크 코드는 별도 소스 파일로 분리되어 있습니다. 아래와 같이 활용하세요.

#### 실행 예시
```python
from compression_benchmark import benchmark_compression

# 예시 DataFrame과 codec 지정
result = benchmark_compression(df, 'snappy')
print(result)
```

## 파티션 크기 최적화

### 최적 파티션 크기
* **권장 크기**: 128MB ~ 1GB per partition
* **최소 크기**: 64MB (너무 작으면 메타데이터 오버헤드)
* **최대 크기**: 2GB (메모리 제한 고려)

### 파티션 수 계산

> **실습 소스 위치:** `1.2.2.src/partition_calculator.py`

파티션 수 계산 코드는 별도 소스 파일로 분리되어 있습니다. 아래와 같이 활용하세요.

#### 실행 예시
```python
from partition_calculator import calculate_optimal_partitions

data_size_gb = 10
optimal_partitions = calculate_optimal_partitions(data_size_gb)
print(f"권장 파티션 수: {optimal_partitions}")

# DataFrame 리파티셔닝 예시
df_repartitioned = df.repartition(optimal_partitions)
```

### 파티션 병합 최적화
```python
# 작은 파티션들을 병합
df.coalesce(optimal_partitions) \
  .write \
  .mode("overwrite") \
  .parquet("s3://bucket/optimized-data/")

# 동적 파티션 조정
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

## 데이터 레이아웃 최적화

### 컬럼 순서 최적화
```python
# 자주 필터링되는 컬럼을 앞에 배치
optimized_df = df.select(
    "date",           # 파티션 키
    "category",       # 자주 필터링
    "user_id",        # 조인 키
    "event_type",     # 자주 그룹화
    "value",          # 집계 대상
    "details"         # 덜 중요한 정보
)
```

### 데이터 타입 최적화
```python
from pyspark.sql.types import *

# 메모리 효율적인 데이터 타입 사용
optimized_schema = StructType([
    StructField("user_id", IntegerType(), True),      # Long → Int
    StructField("category", StringType(), True),
    StructField("value", DecimalType(10,2), True),    # Double → Decimal
    StructField("timestamp", TimestampType(), True),
    StructField("active", BooleanType(), True)        # String → Boolean
])

df_typed = spark.read.schema(optimized_schema).parquet("input/")
```

## 스토리지 클래스 자동화

### Lifecycle 정책 구현

> **실습 소스 위치:** `1.2.2.src/lifecycle_policy.py`

Lifecycle 정책 구현 코드는 별도 소스 파일로 분리되어 있습니다. 아래와 같이 활용하세요.

#### 실행 예시
```python
from lifecycle_policy import setup_intelligent_tiering

setup_intelligent_tiering('your-bucket-name')
```

## 성능 모니터링

### 파티션 효율성 체크
```python
def analyze_partition_efficiency(table_path):
    """파티션 효율성 분석"""
    
    # 파티션 정보 수집
    partitions = spark.sql(f"SHOW PARTITIONS {table_path}")
    partition_count = partitions.count()
    
    # 각 파티션 크기 확인
    partition_sizes = []
    for row in partitions.collect():
        partition_path = f"{table_path}/{row.partition}"
        size = get_s3_object_size(partition_path)
        partition_sizes.append(size)
    
    # 통계 계산
    avg_size = sum(partition_sizes) / len(partition_sizes)
    min_size = min(partition_sizes)
    max_size = max(partition_sizes)
    
    print(f"파티션 수: {partition_count}")
    print(f"평균 크기: {avg_size/1024/1024:.2f} MB")
    print(f"최소/최대 크기: {min_size/1024/1024:.2f} / {max_size/1024/1024:.2f} MB")
    
    # 권장사항
    if avg_size < 64 * 1024 * 1024:  # 64MB
        print("⚠️  파티션이 너무 작습니다. 병합을 고려하세요.")
    elif avg_size > 1024 * 1024 * 1024:  # 1GB
        print("⚠️  파티션이 너무 큽니다. 분할을 고려하세요.")
    else:
        print("✅ 파티션 크기가 적절합니다.")
```

### 압축률 모니터링
```python
def monitor_compression_ratio(original_path, compressed_path):
    """압축률 모니터링"""
    
    original_size = get_s3_object_size(original_path)
    compressed_size = get_s3_object_size(compressed_path)
    
    compression_ratio = (1 - compressed_size / original_size) * 100
    
    print(f"원본 크기: {original_size/1024/1024:.2f} MB")
    print(f"압축 크기: {compressed_size/1024/1024:.2f} MB")
    print(f"압축률: {compression_ratio:.1f}%")
    
    return compression_ratio
```

## 실전 최적화 사례

### 사례 1: 로그 데이터 최적화
```python
# Before: 비효율적인 구조
# s3://logs/2024/05/26/server1/app.log.gz
# s3://logs/2024/05/26/server2/app.log.gz

# After: 효율적인 파티셔닝
# s3://logs/year=2024/month=05/day=26/hour=14/part-00000.parquet

# 최적화 코드
logs_df = spark.read.json("s3://raw-logs/")

# 타임스탬프 파싱 및 파티션 컬럼 추가
processed_logs = logs_df \
    .withColumn("timestamp", to_timestamp("@timestamp")) \
    .withColumn("year", year("timestamp")) \
    .withColumn("month", month("timestamp")) \
    .withColumn("day", dayofmonth("timestamp")) \
    .withColumn("hour", hour("timestamp"))

# 최적화된 저장
processed_logs \
    .coalesce(4) \
    .write \
    .mode("overwrite") \
    .partitionBy("year", "month", "day", "hour") \
    .option("compression", "snappy") \
    .parquet("s3://optimized-logs/")
```

### 사례 2: 센서 데이터 최적화
```python
# IoT 센서 데이터 최적화
sensor_df = spark.read.json("s3://sensor-data/")

# 디바이스 타입별 + 시간별 파티셔닝
optimized_sensor = sensor_df \
    .withColumn("device_type", col("device_id").substr(1, 3)) \
    .withColumn("date", to_date("timestamp")) \
    .repartition("device_type", "date")

# 압축률이 높은 코덱 사용 (센서 데이터는 반복이 많음)
optimized_sensor \
    .write \
    .mode("overwrite") \
    .partitionBy("device_type", "date") \
    .option("compression", "gzip") \
    .parquet("s3://optimized-sensors/")
```

## 권장사항 요약

### Do's ✅
* 쿼리 패턴에 맞는 파티셔닝 적용
* 적절한 파티션 크기 유지 (128MB-1GB)
* 압축 포맷 성능 테스트 후 선택
* Lifecycle 정책으로 비용 최적화
* 정기적인 파티션 효율성 모니터링

### Don'ts ❌
* 과도한 파티셔닝 (소량 파일 대량 생성)
* 카디널리티 무시한 파티션 키 선택
* 압축 없이 원본 데이터 저장
* 한 번 설정 후 방치
* 모든 데이터에 동일한 전략 적용

## 참고 링크
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/s3/index.html)
- [S3 데이터 파티셔닝 전략](https://docs.aws.amazon.com/ko_kr/athena/latest/ug/partitions.html)
- [Parquet, ORC, Avro 포맷 비교](https://aws.amazon.com/ko/big-data/datalakes-and-analytics/what-is-a-data-lake/)
- [S3 성능 최적화](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/optimizing-performance.html)

---

