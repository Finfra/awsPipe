# 2.2 파티셔닝 및 압축 전략

## 파티셔닝 전략

### 파티셔닝 설계 원칙
* **쿼리 패턴 기반**: 자주 사용하는 필터 조건을 파티션 키로 선택
* **카디널리티 고려**: 너무 적거나 많은 파티션 생성 방지
* **균등 분산**: 파티션별 데이터 크기가 비슷하도록 설계
* **미래 확장성**: 향후 데이터 증가를 고려한 설계

### 시간 기반 파티셔닝
```
# 계층적 시간 파티셔닝
s3://my-datalake/
├── events/
│   └── year=2024/
│       └── month=05/
│           └── day=26/
│               └── hour=14/
│                   ├── part-00000.parquet
│                   └── part-00001.parquet
```

### 카테고리 기반 파티셔닝
```
# 비즈니스 로직 기반 파티셔닝
s3://my-datalake/
├── sales/
│   └── region=us-west/
│       └── department=electronics/
│           └── year=2024/
│               └── month=05/
```

### 하이브리드 파티셔닝
```python
# 복합 파티셔닝 예제
df.write \
  .mode("overwrite") \
  .partitionBy("year", "month", "region") \
  .parquet("s3://bucket/sales-data/")
```

## 압축 전략

### 압축 포맷 비교
| 포맷   | 압축률 | 압축속도 | 해제속도 | CPU 사용량 | 사용 사례      |
| ------ | ------ | -------- | -------- | ---------- | -------------- |
| Snappy | 중간   | 빠름     | 빠름     | 낮음       | 일반적 사용    |
| GZIP   | 높음   | 느림     | 중간     | 높음       | 저장 공간 중요 |
| LZ4    | 낮음   | 매우빠름 | 매우빠름 | 매우낮음   | 실시간 처리    |
| ZSTD   | 높음   | 중간     | 빠름     | 중간       | 균형잡힌 성능  |

### Parquet 압축 설정
```python
# Spark에서 Parquet 압축 설정
spark.conf.set("spark.sql.parquet.compression.codec", "snappy")

# 다양한 압축 코덱 성능 비교
compression_codecs = ["snappy", "gzip", "lz4", "zstd"]

for codec in compression_codecs:
    df.write \
      .option("compression", codec) \
      .mode("overwrite") \
      .parquet(f"s3://bucket/data-{codec}/")
```

### 압축 성능 벤치마크

```
python3 1.2.2.src/compression_benchmark.py
```

압축 성능 벤치마크 코드는 별도 소스 파일로 분리되어 있습니다. 아래와 같이 활용하세요.

#### 실행 예시
```python
from compression_benchmark import benchmark_compression

# 예시 DataFrame과 codec 지정
result = benchmark_compression(df, 'snappy')
print(result)
```

## 파티션 크기 최적화

### 최적 파티션 크기
* **권장 크기**: 128MB ~ 1GB per partition
* **최소 크기**: 64MB (너무 작으면 메타데이터 오버헤드)
* **최대 크기**: 2GB (메모리 제한 고려)

### 파티션 수 계산
```
python3 1.2.2.src/partition_calculator.py
```


### 파티션 병합 최적화
```python
# 작은 파티션들을 병합
df.coalesce(optimal_partitions) \
  .write \
  .mode("overwrite") \
  .parquet("s3://bucket/optimized-data/")

# 동적 파티션 조정
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

## 데이터 레이아웃 최적화

### 컬럼 순서 최적화
```python
# 자주 필터링되는 컬럼을 앞에 배치
optimized_df = df.select(
    "date",           # 파티션 키
    "category",       # 자주 필터링
    "user_id",        # 조인 키
    "event_type",     # 자주 그룹화
    "value",          # 집계 대상
    "details"         # 덜 중요한 정보
)
```

### 데이터 타입 최적화
```python
from pyspark.sql.types import *

# 메모리 효율적인 데이터 타입 사용
optimized_schema = StructType([
    StructField("user_id", IntegerType(), True),      # Long → Int
    StructField("category", StringType(), True),
    StructField("value", DecimalType(10,2), True),    # Double → Decimal
    StructField("timestamp", TimestampType(), True),
    StructField("active", BooleanType(), True)        # String → Boolean
])

df_typed = spark.read.schema(optimized_schema).parquet("input/")
```

## 스토리지 클래스 자동화

### Lifecycle 정책 구현
```
python3 1.2.2.src/lifecycle_policy.py <버킷이름>
```

## 성능 모니터링

### 파티션 효율성 체크
```python
python3 1.2.2.src/compression_benchmark.py 
```


## 권장사항 요약

### Do's ✅
* 쿼리 패턴에 맞는 파티셔닝 적용
* 적절한 파티션 크기 유지 (128MB-1GB)
* 압축 포맷 성능 테스트 후 선택
* Lifecycle 정책으로 비용 최적화
* 정기적인 파티션 효율성 모니터링

### Don'ts ❌
* 과도한 파티셔닝 (소량 파일 대량 생성)
* 카디널리티 무시한 파티션 키 선택
* 압축 없이 원본 데이터 저장
* 한 번 설정 후 방치
* 모든 데이터에 동일한 전략 적용

## 참고 링크
- [Amazon S3 공식 문서](https://docs.aws.amazon.com/ko_kr/s3/index.html)
- [S3 데이터 파티셔닝 전략](https://docs.aws.amazon.com/ko_kr/athena/latest/ug/partitions.html)
- [Parquet, ORC, Avro 포맷 비교](https://aws.amazon.com/ko/big-data/datalakes-and-analytics/what-is-a-data-lake/)
- [S3 성능 최적화](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/optimizing-performance.html)

---

