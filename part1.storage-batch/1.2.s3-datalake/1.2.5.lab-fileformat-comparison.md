# 1.2.5. 실습: 다양한 파일 포맷 성능 비교

## 실습 목표
* 1.2.4에서 생성한 데이터를 다양한 포맷으로 변환
* CSV, JSON, Parquet, ORC 포맷 간 성능 비교
* 저장 용량, 처리 속도, 압축률 실측 분석

## 사전 준비

### 환경 설정
```bash
# 필요한 라이브러리 설치
pip install pandas pyarrow fastparquet boto3

# 1.2.4에서 생성한 버킷 이름 확인
echo "버킷 이름: $BUCKET_NAME"
```

## Step 1: 테스트 데이터 준비

### S3에서 JSON 데이터 다운로드
```bash
# 1.2.4에서 업로드한 데이터 다운로드
mkdir -p test-data
aws s3 cp s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/data.jsonl test-data/

# 데이터 확인
echo "=== 다운로드한 데이터 샘플 ==="
head -3 test-data/data.jsonl
```

### DataFrame으로 데이터 로드
# 소스 위치: ./1.2.5.src/load_test_data.py
```python
# 1.2.5.src/load_test_data.py
import pandas as pd
import json

def load_jsonl_data(file_path):
    """JSON Lines 데이터를 DataFrame으로 로드"""
    
    records = []
    with open(file_path, 'r') as f:
        for line in f:
            records.append(json.loads(line.strip()))
    
    df = pd.DataFrame(records)
    print(f"데이터 로드 완료: {len(df):,} 레코드")
    print(f"컬럼: {list(df.columns)}")
    print(f"메모리 사용량: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    return df

if __name__ == "__main__":
    df = load_jsonl_data('test-data/data.jsonl')
    
    # 타입 최적화
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['user_id'] = df['user_id'].astype('int32')
    df['amount'] = df['amount'].astype('float32')
    
    # 테스트용으로 저장
    df.to_pickle('test_dataset.pkl')
    print("테스트 데이터셋 저장: test_dataset.pkl")
```

## Step 2: 포맷별 성능 벤치마크

### 성능 비교 스크립트
# 소스 위치: ./1.2.5.src/format_benchmark.py
```python
# 1.2.5.src/format_benchmark.py
import pandas as pd
import time
import os
import json
from pathlib import Path

def benchmark_file_formats():
    """파일 포맷별 성능 벤치마크"""
    
    # 테스트 데이터 로드
    print("테스트 데이터 로딩...")
    df = pd.read_pickle('test_dataset.pkl')
    
    # 포맷별 설정
    formats = {
        'csv': {
            'save': lambda df, path: df.to_csv(path, index=False),
            'load': lambda path: pd.read_csv(path, parse_dates=['timestamp']),
            'ext': '.csv'
        },
        'json': {
            'save': lambda df, path: df.to_json(path, orient='records', date_format='iso'),
            'load': lambda path: pd.read_json(path, orient='records'),
            'ext': '.json'
        },
        'parquet_snappy': {
            'save': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        },
        'parquet_gzip': {
            'save': lambda df, path: df.to_parquet(path, compression='gzip', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        }
    }
    
    # ORC 추가 (가능한 경우)
    try:
        formats['orc'] = {
            'save': lambda df, path: df.to_orc(path, compression='snappy'),
            'load': lambda path: pd.read_orc(path),
            'ext': '.orc'
        }
    except:
        print("⚠️ ORC 포맷 스킵 (pyarrow 버전 확인 필요)")
    
    print(f"\n{'포맷':<20} {'저장(초)':<10} {'로드(초)':<10} {'크기(MB)':<12} {'압축률':<12}")
    print("=" * 75)
    
    results = {}
    baseline_size = None
    
    for fmt_name, fmt_config in formats.items():
        file_path = f"test_data_{fmt_name}{fmt_config['ext']}"
        
        try:
            # 1. 저장 성능 측정
            start_time = time.time()
            fmt_config['save'](df, file_path)
            save_time = time.time() - start_time
            
            # 2. 파일 크기 측정
            file_size_mb = os.path.getsize(file_path) / 1024 / 1024
            
            # 3. 로딩 성능 측정
            start_time = time.time()
            loaded_df = fmt_config['load'](file_path)
            load_time = time.time() - start_time
            
            # 4. 압축률 계산
            if fmt_name == 'csv':
                baseline_size = file_size_mb
                compression = "기준"
            else:
                reduction = ((baseline_size - file_size_mb) / baseline_size) * 100
                compression = f"{reduction:+.1f}%"
            
            # 5. 데이터 무결성 확인
            assert len(loaded_df) == len(df), f"{fmt_name}: 레코드 수 불일치"
            
            # 결과 저장
            results[fmt_name] = {
                'save_time': save_time,
                'load_time': load_time,
                'file_size_mb': file_size_mb,
                'compression': compression
            }
            
            print(f"{fmt_name:<20} {save_time:<10.2f} {load_time:<10.2f} {file_size_mb:<12.2f} {compression:<12}")
            
            # 임시 파일 정리
            os.remove(file_path)
            
        except Exception as e:
            print(f"{fmt_name:<20} ❌ 오류: {str(e)[:40]}")
            results[fmt_name] = {'error': str(e)}
    
    return results

def query_performance_test():
    """쿼리 성능 비교"""
    
    print("\n=== 쿼리 성능 테스트 ===")
    
    df = pd.read_pickle('test_dataset.pkl')
    
    # 테스트용 파일 생성
    df.to_csv('query_test.csv', index=False)
    df.to_parquet('query_test.parquet', compression='snappy', index=False)
    
    print(f"{'쿼리 유형':<30} {'CSV(초)':<10} {'Parquet(초)':<12} {'성능향상':<10}")
    print("-" * 70)
    
    # 쿼리 테스트
    queries = [
        {
            'name': '전체 레코드 수 조회',
            'csv_func': lambda: len(pd.read_csv('query_test.csv')),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet'))
        },
        {
            'name': '카테고리별 필터링',
            'csv_func': lambda: len(pd.read_csv('query_test.csv').query("category == 'electronics'")),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet').query("category == 'electronics'"))
        },
        {
            'name': '금액 평균 계산',
            'csv_func': lambda: pd.read_csv('query_test.csv')['amount'].mean(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet')['amount'].mean()
        },
        {
            'name': '카테고리별 집계',
            'csv_func': lambda: pd.read_csv('query_test.csv').groupby('category')['amount'].sum().to_dict(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet').groupby('category')['amount'].sum().to_dict()
        }
    ]
    
    for query in queries:
        # CSV 성능
        start = time.time()
        csv_result = query['csv_func']()
        csv_time = time.time() - start
        
        # Parquet 성능
        start = time.time()
        parquet_result = query['parquet_func']()
        parquet_time = time.time() - start
        
        # 성능 향상 계산
        speedup = f"{csv_time/parquet_time:.1f}배" if parquet_time > 0 else "N/A"
        
        print(f"{query['name']:<30} {csv_time:<10.3f} {parquet_time:<12.3f} {speedup:<10}")
    
    # 정리
    os.remove('query_test.csv')
    os.remove('query_test.parquet')

def analyze_results_and_recommendations(results):
    """결과 분석 및 권장사항"""
    
    print("\n=== 성능 분석 ===")
    
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if not valid_results:
        print("유효한 결과가 없습니다.")
        return
    
    # 최고 성능 찾기
    fastest_save = min(valid_results.items(), key=lambda x: x[1]['save_time'])
    fastest_load = min(valid_results.items(), key=lambda x: x[1]['load_time'])
    smallest_file = min(valid_results.items(), key=lambda x: x[1]['file_size_mb'])
    
    print(f"🚀 가장 빠른 저장: {fastest_save[0]} ({fastest_save[1]['save_time']:.2f}초)")
    print(f"⚡ 가장 빠른 로드: {fastest_load[0]} ({fastest_load[1]['load_time']:.2f}초)")
    print(f"💾 가장 작은 파일: {smallest_file[0]} ({smallest_file[1]['file_size_mb']:.2f}MB)")
    
    print("\n=== 사용 시나리오별 권장사항 ===")
    
    scenarios = {
        "🔄 데이터 교환/호환성": {
            "권장": "CSV",
            "이유": "모든 도구에서 지원, 사람이 읽기 가능",
            "용도": "소규모 데이터, 수동 검토 필요한 경우"
        },
        "🌐 웹 API/NoSQL": {
            "권장": "JSON",
            "이유": "웹 표준, 중첩 구조 지원",
            "용도": "API 응답, 설정 파일, 반구조화 데이터"
        },
        "📊 실시간 분석": {
            "권장": "Parquet (Snappy)",
            "이유": "빠른 압축/해제, 컬럼형 최적화",
            "용도": "BI 도구, 빈번한 쿼리, 대화형 분석"
        },
        "💰 장기 보관/아카이빙": {
            "권장": "Parquet (GZIP)",
            "이유": "최고 압축률, 스토리지 비용 절약",
            "용도": "과거 데이터, 규정 준수, 백업"
        },
        "🏭 Hadoop 생태계": {
            "권장": "ORC",
            "이유": "Hive 최적화, 트랜잭션 지원",
            "용도": "Hive 쿼리, 대용량 배치 처리"
        }
    }
    
    for scenario, info in scenarios.items():
        print(f"\n{scenario}")
        print(f"  권장 포맷: {info['권장']}")
        print(f"  선택 이유: {info['이유']}")
        print(f"  적합 용도: {info['용도']}")

if __name__ == "__main__":
    # 1. 테스트 데이터 확인
    if not os.path.exists('test_dataset.pkl'):
        print("❌ 테스트 데이터가 없습니다. 먼저 load_test_data.py를 실행하세요.")
        exit(1)
    
    # 2. 포맷 성능 비교
    results = benchmark_file_formats()
    
    # 3. 쿼리 성능 테스트
    query_performance_test()
    
    # 4. 결과 분석 및 권장사항
    analyze_results_and_recommendations(results)
    
    # 5. 결과를 JSON으로 저장
    with open('format_benchmark_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\n📈 상세 결과 저장: format_benchmark_results.json")
```

## Step 3: S3 업로드/다운로드 성능 테스트

### S3 전송 성능 비교
# 소스 위치: ./1.2.5.src/s3_transfer_benchmark.py
```python
# 1.2.5.src/s3_transfer_benchmark.py
import boto3
import pandas as pd
import time
import tempfile
import os

def s3_transfer_performance(bucket_name):
    """S3 전송 성능 비교"""
    
    print("=== S3 전송 성능 테스트 ===")
    
    s3 = boto3.client('s3')
    df = pd.read_pickle('test_dataset.pkl')
    
    formats = {
        'csv': lambda df, path: df.to_csv(path, index=False),
        'parquet_snappy': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
        'parquet_gzip': lambda df, path: df.to_parquet(path, compression='gzip', index=False)
    }
    
    print(f"{'포맷':<20} {'크기(MB)':<12} {'업로드(초)':<12} {'다운로드(초)':<12} {'총시간(초)':<12}")
    print("-" * 80)
    
    for fmt_name, save_func in formats.items():
        with tempfile.NamedTemporaryFile(suffix=f'.{fmt_name.split("_")[0]}', delete=False) as tmp:
            try:
                # 로컬 파일 생성
                save_func(df, tmp.name)
                file_size_mb = os.path.getsize(tmp.name) / 1024 / 1024
                
                # S3 업로드 성능
                s3_key = f"benchmark/{fmt_name}_test"
                start = time.time()
                s3.upload_file(tmp.name, bucket_name, s3_key)
                upload_time = time.time() - start
                
                # S3 다운로드 성능
                download_path = f"downloaded_{fmt_name}"
                start = time.time()
                s3.download_file(bucket_name, s3_key, download_path)
                download_time = time.time() - start
                
                total_time = upload_time + download_time
                
                print(f"{fmt_name:<20} {file_size_mb:<12.2f} {upload_time:<12.2f} {download_time:<12.2f} {total_time:<12.2f}")
                
                # 정리
                os.unlink(tmp.name)
                os.unlink(download_path)
                s3.delete_object(Bucket=bucket_name, Key=s3_key)
                
            except Exception as e:
                print(f"{fmt_name:<20} ❌ 오류: {e}")
                if os.path.exists(tmp.name):
                    os.unlink(tmp.name)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("사용법: python s3_transfer_benchmark.py <bucket-name>")
        print("예시: python s3_transfer_benchmark.py datalake-demo-1234567890")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    s3_transfer_performance(bucket_name)
```

## Step 4: 전체 테스트 실행

### 실습 실행 순서
# 소스 위치: ./1.2.5.src/load_test_data.py, ./1.2.5.src/format_benchmark.py, ./1.2.5.src/s3_transfer_benchmark.py
```bash
# 1. 테스트 데이터 다운로드 및 준비
python 1.2.5.src/load_test_data.py

# 2. 포맷 성능 벤치마크 실행
python 1.2.5.src/format_benchmark.py

# 3. S3 전송 성능 테스트 (선택사항)
python 1.2.5.src/s3_transfer_benchmark.py $BUCKET_NAME

# 4. 결과 확인
echo "=== 벤치마크 결과 ==="
cat format_benchmark_results.json | python -m json.tool
```

## Step 5: 결과 분석

### 예상 성능 결과 (참고)
```
포맷                   저장(초)     로드(초)     크기(MB)      압축률      
===========================================================================
csv                  0.15      0.28      2.45       기준       
json                 0.22      0.35      3.20       -30.6%     
parquet_snappy       0.08      0.12      0.87       +64.5%     
parquet_gzip         0.12      0.13      0.65       +73.5%     
orc                  0.10      0.15      0.72       +70.6%     
```

### 쿼리 성능 결과
```
쿼리 유형                        CSV(초)    Parquet(초)   성능향상   
----------------------------------------------------------------------
전체 레코드 수 조회                0.045      0.012        3.8배     
카테고리별 필터링                  0.052      0.018        2.9배     
금액 평균 계산                     0.048      0.015        3.2배     
카테고리별 집계                    0.065      0.022        3.0배     
```

## Step 6: 정리

### 리소스 정리
```bash
# 생성된 파일 정리
rm -f test_dataset.pkl
rm -f format_benchmark_results.json
rm -f test-data/*
rmdir test-data

# 생성한 스크립트 정리 (선택사항)
# rm -f load_test_data.py format_benchmark.py s3_transfer_benchmark.py

echo "정리 완료"
```

## 실습 결과 확인

### 성공 기준
- [ ] 1.2.4에서 생성한 데이터를 성공적으로 활용
- [ ] 다양한 포맷 간 성능 차이 확인
- [ ] 쿼리 성능에서 Parquet 우위 확인
- [ ] 압축률과 처리 속도 트레이드오프 이해

### 주요 학습 포인트
* **포맷별 특성**: CSV(호환성), JSON(구조화), Parquet(성능), ORC(Hadoop)
* **성능 트레이드오프**: 압축률 vs 처리 속도
* **실무 선택 기준**: 사용 시나리오에 따른 최적 포맷
* **S3 최적화**: 전송 시간과 저장 비용 고려

### 실제 권장사항
```
💡 빅데이터 분석: Parquet (Snappy) - 빠른 쿼리 성능
💰 장기 보관: Parquet (GZIP) - 스토리지 비용 절약  
🔄 데이터 교환: CSV - 범용 호환성
🌐 API 데이터: JSON - 웹 표준
```

이 실습을 통해 실제 데이터로 포맷별 성능을 비교하고, 상황에 맞는 최적 포맷을 선택하는 기준을 배웠습니다!
