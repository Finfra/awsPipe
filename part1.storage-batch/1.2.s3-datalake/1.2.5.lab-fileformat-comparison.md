# 1.2.5. ì‹¤ìŠµ: ë‹¤ì–‘í•œ íŒŒì¼ í¬ë§· ì„±ëŠ¥ ë¹„êµ

## ì‹¤ìŠµ ëª©í‘œ
* 1.2.4ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜
* CSV, JSON, Parquet, ORC í¬ë§· ê°„ ì„±ëŠ¥ ë¹„êµ
* ì €ì¥ ìš©ëŸ‰, ì²˜ë¦¬ ì†ë„, ì••ì¶•ë¥  ì‹¤ì¸¡ ë¶„ì„

## ì‚¬ì „ ì¤€ë¹„

### í™˜ê²½ ì„¤ì •
```bash
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
sudo pip install pandas pyarrow fastparquet boto3 
sudo python3 -m pip install python-dateutil
# dateutil ì„¤ì¹˜ ì‹¤íŒ¨ì‹œ.
#                 sudo yum remove awscli -y
#                 sudo pip3 install awscli
#                 pip3 install --user awscli
#                 export PATH=$PATH:~/.local/bin      (~/.bashrcì—ë„ ì¶”ê°€)

# 1.2.4ì—ì„œ ìƒì„±í•œ ë²„í‚· ì´ë¦„ í™•ì¸
export BUCKET_NAME=$(cat ~/BUCKET_NAME)
echo "ë²„í‚· ì´ë¦„: $BUCKET_NAME"

```

## Step 1: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

### S3ì—ì„œ JSON ë°ì´í„° ë‹¤ìš´ë¡œë“œ
```bash
# 1.2.4ì—ì„œ ì—…ë¡œë“œí•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
mkdir -p ~/test-data
aws s3 cp s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/data.jsonl ~/test-data/

# ë°ì´í„° í™•ì¸
echo "=== ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„° ìƒ˜í”Œ ==="
head -3 ~/test-data/data.jsonl
```

### DataFrameìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
```python
python3 1.2.5.src/load_test_data.py
```

## Step 2: í¬ë§·ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/format_benchmark.py
```python
python3 1.2.5.src/format_benchmark.py

```

## Step 3: S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### S3 ì „ì†¡ ì„±ëŠ¥ ë¹„êµ
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/s3_transfer_benchmark.py
```python
# 1.2.5.src/s3_transfer_benchmark.py
import boto3
import pandas as pd
import time
import tempfile
import os

def s3_transfer_performance(bucket_name):
    """S3 ì „ì†¡ ì„±ëŠ¥ ë¹„êµ"""
    
    print("=== S3 ì „ì†¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    s3 = boto3.client('s3')
    df = pd.read_pickle('test_dataset.pkl')
    
    formats = {
        'csv': lambda df, path: df.to_csv(path, index=False),
        'parquet_snappy': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
        'parquet_gzip': lambda df, path: df.to_parquet(path, compression='gzip', index=False)
    }
    
    print(f"{'í¬ë§·':<20} {'í¬ê¸°(MB)':<12} {'ì—…ë¡œë“œ(ì´ˆ)':<12} {'ë‹¤ìš´ë¡œë“œ(ì´ˆ)':<12} {'ì´ì‹œê°„(ì´ˆ)':<12}")
    print("-" * 80)
    
    for fmt_name, save_func in formats.items():
        with tempfile.NamedTemporaryFile(suffix=f'.{fmt_name.split("_")[0]}', delete=False) as tmp:
            try:
                # ë¡œì»¬ íŒŒì¼ ìƒì„±
                save_func(df, tmp.name)
                file_size_mb = os.path.getsize(tmp.name) / 1024 / 1024
                
                # S3 ì—…ë¡œë“œ ì„±ëŠ¥
                s3_key = f"benchmark/{fmt_name}_test"
                start = time.time()
                s3.upload_file(tmp.name, bucket_name, s3_key)
                upload_time = time.time() - start
                
                # S3 ë‹¤ìš´ë¡œë“œ ì„±ëŠ¥
                download_path = f"downloaded_{fmt_name}"
                start = time.time()
                s3.download_file(bucket_name, s3_key, download_path)
                download_time = time.time() - start
                
                total_time = upload_time + download_time
                
                print(f"{fmt_name:<20} {file_size_mb:<12.2f} {upload_time:<12.2f} {download_time:<12.2f} {total_time:<12.2f}")
                
                # ì •ë¦¬
                os.unlink(tmp.name)
                os.unlink(download_path)
                s3.delete_object(Bucket=bucket_name, Key=s3_key)
                
            except Exception as e:
                print(f"{fmt_name:<20} âŒ ì˜¤ë¥˜: {e}")
                if os.path.exists(tmp.name):
                    os.unlink(tmp.name)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python s3_transfer_benchmark.py <bucket-name>")
        print("ì˜ˆì‹œ: python s3_transfer_benchmark.py datalake-demo-1234567890")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    s3_transfer_performance(bucket_name)
```

## Step 4: ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ì‹¤ìŠµ ì‹¤í–‰ ìˆœì„œ
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/load_test_data.py, ./1.2.5.src/format_benchmark.py, ./1.2.5.src/s3_transfer_benchmark.py
```bash
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
python 1.2.5.src/load_test_data.py

# 2. í¬ë§· ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python 1.2.5.src/format_benchmark.py

# 3. S3 ì „ì†¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
python 1.2.5.src/s3_transfer_benchmark.py $(cat ~/BUCKET_NAME)

# 4. ê²°ê³¼ í™•ì¸
echo "=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ==="
cat format_benchmark_results.json | python -m json.tool
```

## Step 5: ê²°ê³¼ ë¶„ì„

### ì˜ˆìƒ ì„±ëŠ¥ ê²°ê³¼ (ì°¸ê³ )
```
í¬ë§·                   ì €ì¥(ì´ˆ)     ë¡œë“œ(ì´ˆ)     í¬ê¸°(MB)      ì••ì¶•ë¥       
===========================================================================
csv                  0.15      0.28      2.45       ê¸°ì¤€       
json                 0.22      0.35      3.20       -30.6%     
parquet_snappy       0.08      0.12      0.87       +64.5%     
parquet_gzip         0.12      0.13      0.65       +73.5%     
orc                  0.10      0.15      0.72       +70.6%     
```

### ì¿¼ë¦¬ ì„±ëŠ¥ ê²°ê³¼
```
ì¿¼ë¦¬ ìœ í˜•                        CSV(ì´ˆ)    Parquet(ì´ˆ)   ì„±ëŠ¥í–¥ìƒ   
----------------------------------------------------------------------
ì „ì²´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ                0.045      0.012        3.8ë°°     
ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§                  0.052      0.018        2.9ë°°     
ê¸ˆì•¡ í‰ê·  ê³„ì‚°                     0.048      0.015        3.2ë°°     
ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„                    0.065      0.022        3.0ë°°     
```

## Step 6: ì •ë¦¬

### ë¦¬ì†ŒìŠ¤ ì •ë¦¬
```bash
# ìƒì„±ëœ íŒŒì¼ ì •ë¦¬
rm -f ~/test_dataset.pkl
rm -f ~/format_benchmark_results.json
rm -rf ~/test-data

echo "ì •ë¦¬ ì™„ë£Œ"
```

## ì‹¤ìŠµ ê²°ê³¼ í™•ì¸

### ì„±ê³µ ê¸°ì¤€
- [ ] 1.2.4ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ í™œìš©
- [ ] ë‹¤ì–‘í•œ í¬ë§· ê°„ ì„±ëŠ¥ ì°¨ì´ í™•ì¸
- [ ] ì¿¼ë¦¬ ì„±ëŠ¥ì—ì„œ Parquet ìš°ìœ„ í™•ì¸
- [ ] ì••ì¶•ë¥ ê³¼ ì²˜ë¦¬ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì´í•´

### ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸
* **í¬ë§·ë³„ íŠ¹ì„±**: CSV(í˜¸í™˜ì„±), JSON(êµ¬ì¡°í™”), Parquet(ì„±ëŠ¥), ORC(Hadoop)
* **ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„**: ì••ì¶•ë¥  vs ì²˜ë¦¬ ì†ë„
* **ì‹¤ë¬´ ì„ íƒ ê¸°ì¤€**: ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¥¸ ìµœì  í¬ë§·
* **S3 ìµœì í™”**: ì „ì†¡ ì‹œê°„ê³¼ ì €ì¥ ë¹„ìš© ê³ ë ¤

### ì‹¤ì œ ê¶Œì¥ì‚¬í•­
```
ğŸ’¡ ë¹…ë°ì´í„° ë¶„ì„: Parquet (Snappy) - ë¹ ë¥¸ ì¿¼ë¦¬ ì„±ëŠ¥
ğŸ’° ì¥ê¸° ë³´ê´€: Parquet (GZIP) - ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½  
ğŸ”„ ë°ì´í„° êµí™˜: CSV - ë²”ìš© í˜¸í™˜ì„±
ğŸŒ API ë°ì´í„°: JSON - ì›¹ í‘œì¤€
```
