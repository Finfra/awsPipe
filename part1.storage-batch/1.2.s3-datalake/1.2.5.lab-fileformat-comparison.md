# 1.2.5. 실습: 다양한 파일 포맷 성능 비교

## 실습 목표
* 1.2.4에서 생성한 데이터를 다양한 포맷으로 변환
* CSV, JSON, Parquet, ORC 포맷 간 성능 비교
* 저장 용량, 처리 속도, 압축률 실측 분석

## 사전 준비

### 환경 설정
```bash
# 필요한 라이브러리 설치
sudo pip install pandas pyarrow fastparquet boto3 
sudo python3 -m pip install python-dateutil
# dateutil 설치 실패시.
#                 sudo yum remove awscli -y
#                 sudo pip3 install awscli
#                 pip3 install --user awscli
#                 export PATH=$PATH:~/.local/bin      (~/.bashrc에도 추가)

# 1.2.4에서 생성한 버킷 이름 확인
export BUCKET_NAME=$(cat ~/BUCKET_NAME)
echo "버킷 이름: $BUCKET_NAME"

```

## Step 1: 테스트 데이터 준비

### S3에서 JSON 데이터 다운로드
```bash
# 1.2.4에서 업로드한 데이터 다운로드
mkdir -p ~/test-data
aws s3 cp s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/data.jsonl ~/test-data/

# 데이터 확인
echo "=== 다운로드한 데이터 샘플 ==="
head -3 ~/test-data/data.jsonl
```

### DataFrame으로 데이터 로드
```python
python3 1.2.5.src/load_test_data.py
```

## Step 2: 포맷별 성능 벤치마크

### 성능 비교 스크립트
# 소스 위치: ./1.2.5.src/format_benchmark.py
```python
python3 1.2.5.src/format_benchmark.py

```

## Step 3: S3 업로드/다운로드 성능 테스트

### S3 전송 성능 비교
# 소스 위치: ./1.2.5.src/s3_transfer_benchmark.py
```python
# 1.2.5.src/s3_transfer_benchmark.py
import boto3
import pandas as pd
import time
import tempfile
import os

def s3_transfer_performance(bucket_name):
    """S3 전송 성능 비교"""
    
    print("=== S3 전송 성능 테스트 ===")
    
    s3 = boto3.client('s3')
    df = pd.read_pickle('test_dataset.pkl')
    
    formats = {
        'csv': lambda df, path: df.to_csv(path, index=False),
        'parquet_snappy': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
        'parquet_gzip': lambda df, path: df.to_parquet(path, compression='gzip', index=False)
    }
    
    print(f"{'포맷':<20} {'크기(MB)':<12} {'업로드(초)':<12} {'다운로드(초)':<12} {'총시간(초)':<12}")
    print("-" * 80)
    
    for fmt_name, save_func in formats.items():
        with tempfile.NamedTemporaryFile(suffix=f'.{fmt_name.split("_")[0]}', delete=False) as tmp:
            try:
                # 로컬 파일 생성
                save_func(df, tmp.name)
                file_size_mb = os.path.getsize(tmp.name) / 1024 / 1024
                
                # S3 업로드 성능
                s3_key = f"benchmark/{fmt_name}_test"
                start = time.time()
                s3.upload_file(tmp.name, bucket_name, s3_key)
                upload_time = time.time() - start
                
                # S3 다운로드 성능
                download_path = f"downloaded_{fmt_name}"
                start = time.time()
                s3.download_file(bucket_name, s3_key, download_path)
                download_time = time.time() - start
                
                total_time = upload_time + download_time
                
                print(f"{fmt_name:<20} {file_size_mb:<12.2f} {upload_time:<12.2f} {download_time:<12.2f} {total_time:<12.2f}")
                
                # 정리
                os.unlink(tmp.name)
                os.unlink(download_path)
                s3.delete_object(Bucket=bucket_name, Key=s3_key)
                
            except Exception as e:
                print(f"{fmt_name:<20} ❌ 오류: {e}")
                if os.path.exists(tmp.name):
                    os.unlink(tmp.name)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("사용법: python s3_transfer_benchmark.py <bucket-name>")
        print("예시: python s3_transfer_benchmark.py datalake-demo-1234567890")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    s3_transfer_performance(bucket_name)
```

## Step 4: 전체 테스트 실행

### 실습 실행 순서
# 소스 위치: ./1.2.5.src/load_test_data.py, ./1.2.5.src/format_benchmark.py, ./1.2.5.src/s3_transfer_benchmark.py
```bash
# 1. 테스트 데이터 다운로드 및 준비
python 1.2.5.src/load_test_data.py

# 2. 포맷 성능 벤치마크 실행
python 1.2.5.src/format_benchmark.py

# 3. S3 전송 성능 테스트 (선택사항)
python 1.2.5.src/s3_transfer_benchmark.py $(cat ~/BUCKET_NAME)

# 4. 결과 확인
echo "=== 벤치마크 결과 ==="
cat format_benchmark_results.json | python -m json.tool
```

## Step 5: 결과 분석

### 예상 성능 결과 (참고)
```
포맷                   저장(초)     로드(초)     크기(MB)      압축률      
===========================================================================
csv                  0.15      0.28      2.45       기준       
json                 0.22      0.35      3.20       -30.6%     
parquet_snappy       0.08      0.12      0.87       +64.5%     
parquet_gzip         0.12      0.13      0.65       +73.5%     
orc                  0.10      0.15      0.72       +70.6%     
```

### 쿼리 성능 결과
```
쿼리 유형                        CSV(초)    Parquet(초)   성능향상   
----------------------------------------------------------------------
전체 레코드 수 조회                0.045      0.012        3.8배     
카테고리별 필터링                  0.052      0.018        2.9배     
금액 평균 계산                     0.048      0.015        3.2배     
카테고리별 집계                    0.065      0.022        3.0배     
```

## Step 6: 정리

### 리소스 정리
```bash
# 생성된 파일 정리
rm -f ~/test_dataset.pkl
rm -f ~/format_benchmark_results.json
rm -rf ~/test-data

echo "정리 완료"
```

## 실습 결과 확인

### 성공 기준
- [ ] 1.2.4에서 생성한 데이터를 성공적으로 활용
- [ ] 다양한 포맷 간 성능 차이 확인
- [ ] 쿼리 성능에서 Parquet 우위 확인
- [ ] 압축률과 처리 속도 트레이드오프 이해

### 주요 학습 포인트
* **포맷별 특성**: CSV(호환성), JSON(구조화), Parquet(성능), ORC(Hadoop)
* **성능 트레이드오프**: 압축률 vs 처리 속도
* **실무 선택 기준**: 사용 시나리오에 따른 최적 포맷
* **S3 최적화**: 전송 시간과 저장 비용 고려

### 실제 권장사항
```
💡 빅데이터 분석: Parquet (Snappy) - 빠른 쿼리 성능
💰 장기 보관: Parquet (GZIP) - 스토리지 비용 절약  
🔄 데이터 교환: CSV - 범용 호환성
🌐 API 데이터: JSON - 웹 표준
```
