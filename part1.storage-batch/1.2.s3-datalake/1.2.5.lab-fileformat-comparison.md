# 1.2.5. ì‹¤ìŠµ: ë‹¤ì–‘í•œ íŒŒì¼ í¬ë§· ì„±ëŠ¥ ë¹„êµ

## ì‹¤ìŠµ ëª©í‘œ
* 1.2.4ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜
* CSV, JSON, Parquet, ORC í¬ë§· ê°„ ì„±ëŠ¥ ë¹„êµ
* ì €ì¥ ìš©ëŸ‰, ì²˜ë¦¬ ì†ë„, ì••ì¶•ë¥  ì‹¤ì¸¡ ë¶„ì„

## ì‚¬ì „ ì¤€ë¹„

### í™˜ê²½ ì„¤ì •
```bash
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install pandas pyarrow fastparquet boto3

# 1.2.4ì—ì„œ ìƒì„±í•œ ë²„í‚· ì´ë¦„ í™•ì¸
echo "ë²„í‚· ì´ë¦„: $BUCKET_NAME"
```

## Step 1: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„

### S3ì—ì„œ JSON ë°ì´í„° ë‹¤ìš´ë¡œë“œ
```bash
# 1.2.4ì—ì„œ ì—…ë¡œë“œí•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
mkdir -p test-data
aws s3 cp s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/data.jsonl test-data/

# ë°ì´í„° í™•ì¸
echo "=== ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„° ìƒ˜í”Œ ==="
head -3 test-data/data.jsonl
```

### DataFrameìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/load_test_data.py
```python
# 1.2.5.src/load_test_data.py
import pandas as pd
import json

def load_jsonl_data(file_path):
    """JSON Lines ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë¡œë“œ"""
    
    records = []
    with open(file_path, 'r') as f:
        for line in f:
            records.append(json.loads(line.strip()))
    
    df = pd.DataFrame(records)
    print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,} ë ˆì½”ë“œ")
    print(f"ì»¬ëŸ¼: {list(df.columns)}")
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    return df

if __name__ == "__main__":
    df = load_jsonl_data('test-data/data.jsonl')
    
    # íƒ€ì… ìµœì í™”
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['user_id'] = df['user_id'].astype('int32')
    df['amount'] = df['amount'].astype('float32')
    
    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì €ì¥
    df.to_pickle('test_dataset.pkl')
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥: test_dataset.pkl")
```

## Step 2: í¬ë§·ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/format_benchmark.py
```python
# 1.2.5.src/format_benchmark.py
import pandas as pd
import time
import os
import json
from pathlib import Path

def benchmark_file_formats():
    """íŒŒì¼ í¬ë§·ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
    df = pd.read_pickle('test_dataset.pkl')
    
    # í¬ë§·ë³„ ì„¤ì •
    formats = {
        'csv': {
            'save': lambda df, path: df.to_csv(path, index=False),
            'load': lambda path: pd.read_csv(path, parse_dates=['timestamp']),
            'ext': '.csv'
        },
        'json': {
            'save': lambda df, path: df.to_json(path, orient='records', date_format='iso'),
            'load': lambda path: pd.read_json(path, orient='records'),
            'ext': '.json'
        },
        'parquet_snappy': {
            'save': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        },
        'parquet_gzip': {
            'save': lambda df, path: df.to_parquet(path, compression='gzip', index=False),
            'load': lambda path: pd.read_parquet(path),
            'ext': '.parquet'
        }
    }
    
    # ORC ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        formats['orc'] = {
            'save': lambda df, path: df.to_orc(path, compression='snappy'),
            'load': lambda path: pd.read_orc(path),
            'ext': '.orc'
        }
    except:
        print("âš ï¸ ORC í¬ë§· ìŠ¤í‚µ (pyarrow ë²„ì „ í™•ì¸ í•„ìš”)")
    
    print(f"\n{'í¬ë§·':<20} {'ì €ì¥(ì´ˆ)':<10} {'ë¡œë“œ(ì´ˆ)':<10} {'í¬ê¸°(MB)':<12} {'ì••ì¶•ë¥ ':<12}")
    print("=" * 75)
    
    results = {}
    baseline_size = None
    
    for fmt_name, fmt_config in formats.items():
        file_path = f"test_data_{fmt_name}{fmt_config['ext']}"
        
        try:
            # 1. ì €ì¥ ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            fmt_config['save'](df, file_path)
            save_time = time.time() - start_time
            
            # 2. íŒŒì¼ í¬ê¸° ì¸¡ì •
            file_size_mb = os.path.getsize(file_path) / 1024 / 1024
            
            # 3. ë¡œë”© ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            loaded_df = fmt_config['load'](file_path)
            load_time = time.time() - start_time
            
            # 4. ì••ì¶•ë¥  ê³„ì‚°
            if fmt_name == 'csv':
                baseline_size = file_size_mb
                compression = "ê¸°ì¤€"
            else:
                reduction = ((baseline_size - file_size_mb) / baseline_size) * 100
                compression = f"{reduction:+.1f}%"
            
            # 5. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
            assert len(loaded_df) == len(df), f"{fmt_name}: ë ˆì½”ë“œ ìˆ˜ ë¶ˆì¼ì¹˜"
            
            # ê²°ê³¼ ì €ì¥
            results[fmt_name] = {
                'save_time': save_time,
                'load_time': load_time,
                'file_size_mb': file_size_mb,
                'compression': compression
            }
            
            print(f"{fmt_name:<20} {save_time:<10.2f} {load_time:<10.2f} {file_size_mb:<12.2f} {compression:<12}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(file_path)
            
        except Exception as e:
            print(f"{fmt_name:<20} âŒ ì˜¤ë¥˜: {str(e)[:40]}")
            results[fmt_name] = {'error': str(e)}
    
    return results

def query_performance_test():
    """ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ"""
    
    print("\n=== ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    df = pd.read_pickle('test_dataset.pkl')
    
    # í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ìƒì„±
    df.to_csv('query_test.csv', index=False)
    df.to_parquet('query_test.parquet', compression='snappy', index=False)
    
    print(f"{'ì¿¼ë¦¬ ìœ í˜•':<30} {'CSV(ì´ˆ)':<10} {'Parquet(ì´ˆ)':<12} {'ì„±ëŠ¥í–¥ìƒ':<10}")
    print("-" * 70)
    
    # ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    queries = [
        {
            'name': 'ì „ì²´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ',
            'csv_func': lambda: len(pd.read_csv('query_test.csv')),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet'))
        },
        {
            'name': 'ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§',
            'csv_func': lambda: len(pd.read_csv('query_test.csv').query("category == 'electronics'")),
            'parquet_func': lambda: len(pd.read_parquet('query_test.parquet').query("category == 'electronics'"))
        },
        {
            'name': 'ê¸ˆì•¡ í‰ê·  ê³„ì‚°',
            'csv_func': lambda: pd.read_csv('query_test.csv')['amount'].mean(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet')['amount'].mean()
        },
        {
            'name': 'ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„',
            'csv_func': lambda: pd.read_csv('query_test.csv').groupby('category')['amount'].sum().to_dict(),
            'parquet_func': lambda: pd.read_parquet('query_test.parquet').groupby('category')['amount'].sum().to_dict()
        }
    ]
    
    for query in queries:
        # CSV ì„±ëŠ¥
        start = time.time()
        csv_result = query['csv_func']()
        csv_time = time.time() - start
        
        # Parquet ì„±ëŠ¥
        start = time.time()
        parquet_result = query['parquet_func']()
        parquet_time = time.time() - start
        
        # ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
        speedup = f"{csv_time/parquet_time:.1f}ë°°" if parquet_time > 0 else "N/A"
        
        print(f"{query['name']:<30} {csv_time:<10.3f} {parquet_time:<12.3f} {speedup:<10}")
    
    # ì •ë¦¬
    os.remove('query_test.csv')
    os.remove('query_test.parquet')

def analyze_results_and_recommendations(results):
    """ê²°ê³¼ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­"""
    
    print("\n=== ì„±ëŠ¥ ë¶„ì„ ===")
    
    valid_results = {k: v for k, v in results.items() if 'error' not in v}
    
    if not valid_results:
        print("ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
    fastest_save = min(valid_results.items(), key=lambda x: x[1]['save_time'])
    fastest_load = min(valid_results.items(), key=lambda x: x[1]['load_time'])
    smallest_file = min(valid_results.items(), key=lambda x: x[1]['file_size_mb'])
    
    print(f"ğŸš€ ê°€ì¥ ë¹ ë¥¸ ì €ì¥: {fastest_save[0]} ({fastest_save[1]['save_time']:.2f}ì´ˆ)")
    print(f"âš¡ ê°€ì¥ ë¹ ë¥¸ ë¡œë“œ: {fastest_load[0]} ({fastest_load[1]['load_time']:.2f}ì´ˆ)")
    print(f"ğŸ’¾ ê°€ì¥ ì‘ì€ íŒŒì¼: {smallest_file[0]} ({smallest_file[1]['file_size_mb']:.2f}MB)")
    
    print("\n=== ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ì‚¬í•­ ===")
    
    scenarios = {
        "ğŸ”„ ë°ì´í„° êµí™˜/í˜¸í™˜ì„±": {
            "ê¶Œì¥": "CSV",
            "ì´ìœ ": "ëª¨ë“  ë„êµ¬ì—ì„œ ì§€ì›, ì‚¬ëŒì´ ì½ê¸° ê°€ëŠ¥",
            "ìš©ë„": "ì†Œê·œëª¨ ë°ì´í„°, ìˆ˜ë™ ê²€í†  í•„ìš”í•œ ê²½ìš°"
        },
        "ğŸŒ ì›¹ API/NoSQL": {
            "ê¶Œì¥": "JSON",
            "ì´ìœ ": "ì›¹ í‘œì¤€, ì¤‘ì²© êµ¬ì¡° ì§€ì›",
            "ìš©ë„": "API ì‘ë‹µ, ì„¤ì • íŒŒì¼, ë°˜êµ¬ì¡°í™” ë°ì´í„°"
        },
        "ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„": {
            "ê¶Œì¥": "Parquet (Snappy)",
            "ì´ìœ ": "ë¹ ë¥¸ ì••ì¶•/í•´ì œ, ì»¬ëŸ¼í˜• ìµœì í™”",
            "ìš©ë„": "BI ë„êµ¬, ë¹ˆë²ˆí•œ ì¿¼ë¦¬, ëŒ€í™”í˜• ë¶„ì„"
        },
        "ğŸ’° ì¥ê¸° ë³´ê´€/ì•„ì¹´ì´ë¹™": {
            "ê¶Œì¥": "Parquet (GZIP)",
            "ì´ìœ ": "ìµœê³  ì••ì¶•ë¥ , ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½",
            "ìš©ë„": "ê³¼ê±° ë°ì´í„°, ê·œì • ì¤€ìˆ˜, ë°±ì—…"
        },
        "ğŸ­ Hadoop ìƒíƒœê³„": {
            "ê¶Œì¥": "ORC",
            "ì´ìœ ": "Hive ìµœì í™”, íŠ¸ëœì­ì…˜ ì§€ì›",
            "ìš©ë„": "Hive ì¿¼ë¦¬, ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬"
        }
    }
    
    for scenario, info in scenarios.items():
        print(f"\n{scenario}")
        print(f"  ê¶Œì¥ í¬ë§·: {info['ê¶Œì¥']}")
        print(f"  ì„ íƒ ì´ìœ : {info['ì´ìœ ']}")
        print(f"  ì í•© ìš©ë„: {info['ìš©ë„']}")

if __name__ == "__main__":
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸
    if not os.path.exists('test_dataset.pkl'):
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € load_test_data.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    
    # 2. í¬ë§· ì„±ëŠ¥ ë¹„êµ
    results = benchmark_file_formats()
    
    # 3. ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    query_performance_test()
    
    # 4. ê²°ê³¼ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­
    analyze_results_and_recommendations(results)
    
    # 5. ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    with open('format_benchmark_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“ˆ ìƒì„¸ ê²°ê³¼ ì €ì¥: format_benchmark_results.json")
```

## Step 3: S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### S3 ì „ì†¡ ì„±ëŠ¥ ë¹„êµ
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/s3_transfer_benchmark.py
```python
# 1.2.5.src/s3_transfer_benchmark.py
import boto3
import pandas as pd
import time
import tempfile
import os

def s3_transfer_performance(bucket_name):
    """S3 ì „ì†¡ ì„±ëŠ¥ ë¹„êµ"""
    
    print("=== S3 ì „ì†¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
    
    s3 = boto3.client('s3')
    df = pd.read_pickle('test_dataset.pkl')
    
    formats = {
        'csv': lambda df, path: df.to_csv(path, index=False),
        'parquet_snappy': lambda df, path: df.to_parquet(path, compression='snappy', index=False),
        'parquet_gzip': lambda df, path: df.to_parquet(path, compression='gzip', index=False)
    }
    
    print(f"{'í¬ë§·':<20} {'í¬ê¸°(MB)':<12} {'ì—…ë¡œë“œ(ì´ˆ)':<12} {'ë‹¤ìš´ë¡œë“œ(ì´ˆ)':<12} {'ì´ì‹œê°„(ì´ˆ)':<12}")
    print("-" * 80)
    
    for fmt_name, save_func in formats.items():
        with tempfile.NamedTemporaryFile(suffix=f'.{fmt_name.split("_")[0]}', delete=False) as tmp:
            try:
                # ë¡œì»¬ íŒŒì¼ ìƒì„±
                save_func(df, tmp.name)
                file_size_mb = os.path.getsize(tmp.name) / 1024 / 1024
                
                # S3 ì—…ë¡œë“œ ì„±ëŠ¥
                s3_key = f"benchmark/{fmt_name}_test"
                start = time.time()
                s3.upload_file(tmp.name, bucket_name, s3_key)
                upload_time = time.time() - start
                
                # S3 ë‹¤ìš´ë¡œë“œ ì„±ëŠ¥
                download_path = f"downloaded_{fmt_name}"
                start = time.time()
                s3.download_file(bucket_name, s3_key, download_path)
                download_time = time.time() - start
                
                total_time = upload_time + download_time
                
                print(f"{fmt_name:<20} {file_size_mb:<12.2f} {upload_time:<12.2f} {download_time:<12.2f} {total_time:<12.2f}")
                
                # ì •ë¦¬
                os.unlink(tmp.name)
                os.unlink(download_path)
                s3.delete_object(Bucket=bucket_name, Key=s3_key)
                
            except Exception as e:
                print(f"{fmt_name:<20} âŒ ì˜¤ë¥˜: {e}")
                if os.path.exists(tmp.name):
                    os.unlink(tmp.name)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python s3_transfer_benchmark.py <bucket-name>")
        print("ì˜ˆì‹œ: python s3_transfer_benchmark.py datalake-demo-1234567890")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    s3_transfer_performance(bucket_name)
```

## Step 4: ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ì‹¤ìŠµ ì‹¤í–‰ ìˆœì„œ
# ì†ŒìŠ¤ ìœ„ì¹˜: ./1.2.5.src/load_test_data.py, ./1.2.5.src/format_benchmark.py, ./1.2.5.src/s3_transfer_benchmark.py
```bash
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
python 1.2.5.src/load_test_data.py

# 2. í¬ë§· ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python 1.2.5.src/format_benchmark.py

# 3. S3 ì „ì†¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
python 1.2.5.src/s3_transfer_benchmark.py $BUCKET_NAME

# 4. ê²°ê³¼ í™•ì¸
echo "=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ==="
cat format_benchmark_results.json | python -m json.tool
```

## Step 5: ê²°ê³¼ ë¶„ì„

### ì˜ˆìƒ ì„±ëŠ¥ ê²°ê³¼ (ì°¸ê³ )
```
í¬ë§·                   ì €ì¥(ì´ˆ)     ë¡œë“œ(ì´ˆ)     í¬ê¸°(MB)      ì••ì¶•ë¥       
===========================================================================
csv                  0.15      0.28      2.45       ê¸°ì¤€       
json                 0.22      0.35      3.20       -30.6%     
parquet_snappy       0.08      0.12      0.87       +64.5%     
parquet_gzip         0.12      0.13      0.65       +73.5%     
orc                  0.10      0.15      0.72       +70.6%     
```

### ì¿¼ë¦¬ ì„±ëŠ¥ ê²°ê³¼
```
ì¿¼ë¦¬ ìœ í˜•                        CSV(ì´ˆ)    Parquet(ì´ˆ)   ì„±ëŠ¥í–¥ìƒ   
----------------------------------------------------------------------
ì „ì²´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ                0.045      0.012        3.8ë°°     
ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§                  0.052      0.018        2.9ë°°     
ê¸ˆì•¡ í‰ê·  ê³„ì‚°                     0.048      0.015        3.2ë°°     
ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„                    0.065      0.022        3.0ë°°     
```

## Step 6: ì •ë¦¬

### ë¦¬ì†ŒìŠ¤ ì •ë¦¬
```bash
# ìƒì„±ëœ íŒŒì¼ ì •ë¦¬
rm -f test_dataset.pkl
rm -f format_benchmark_results.json
rm -f test-data/*
rmdir test-data

# ìƒì„±í•œ ìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬ (ì„ íƒì‚¬í•­)
# rm -f load_test_data.py format_benchmark.py s3_transfer_benchmark.py

echo "ì •ë¦¬ ì™„ë£Œ"
```

## ì‹¤ìŠµ ê²°ê³¼ í™•ì¸

### ì„±ê³µ ê¸°ì¤€
- [ ] 1.2.4ì—ì„œ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ í™œìš©
- [ ] ë‹¤ì–‘í•œ í¬ë§· ê°„ ì„±ëŠ¥ ì°¨ì´ í™•ì¸
- [ ] ì¿¼ë¦¬ ì„±ëŠ¥ì—ì„œ Parquet ìš°ìœ„ í™•ì¸
- [ ] ì••ì¶•ë¥ ê³¼ ì²˜ë¦¬ ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ì´í•´

### ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸
* **í¬ë§·ë³„ íŠ¹ì„±**: CSV(í˜¸í™˜ì„±), JSON(êµ¬ì¡°í™”), Parquet(ì„±ëŠ¥), ORC(Hadoop)
* **ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„**: ì••ì¶•ë¥  vs ì²˜ë¦¬ ì†ë„
* **ì‹¤ë¬´ ì„ íƒ ê¸°ì¤€**: ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¥¸ ìµœì  í¬ë§·
* **S3 ìµœì í™”**: ì „ì†¡ ì‹œê°„ê³¼ ì €ì¥ ë¹„ìš© ê³ ë ¤

### ì‹¤ì œ ê¶Œì¥ì‚¬í•­
```
ğŸ’¡ ë¹…ë°ì´í„° ë¶„ì„: Parquet (Snappy) - ë¹ ë¥¸ ì¿¼ë¦¬ ì„±ëŠ¥
ğŸ’° ì¥ê¸° ë³´ê´€: Parquet (GZIP) - ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì ˆì•½  
ğŸ”„ ë°ì´í„° êµí™˜: CSV - ë²”ìš© í˜¸í™˜ì„±
ğŸŒ API ë°ì´í„°: JSON - ì›¹ í‘œì¤€
```

ì´ ì‹¤ìŠµì„ í†µí•´ ì‹¤ì œ ë°ì´í„°ë¡œ í¬ë§·ë³„ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ , ìƒí™©ì— ë§ëŠ” ìµœì  í¬ë§·ì„ ì„ íƒí•˜ëŠ” ê¸°ì¤€ì„ ë°°ì› ìŠµë‹ˆë‹¤!
