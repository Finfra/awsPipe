# 1.2.4. 실습: S3 버킷 생성 및 파티션된 데이터 저장

## 실습 목표
* S3 버킷 생성 및 기본 설정
* 파티션 구조로 데이터 저장 및 관리
* S3 스토리지 클래스 활용

## Step 1: S3 버킷 생성

### AWS CLI로 버킷 생성
```bash
aws configure 

# 고유한 버킷 이름 설정
read -p "자기번호=" user_num
BUCKET_NAME="ligkn-$user_num"
echo $BUCKET_NAME > ~/BUCKET_NAME
echo 설정한 $BUCKET_NAME 입니다. 
# 버킷 생성
aws s3 mb s3://$BUCKET_NAME --region ap-northeast-2

# 버전 관리 활성화
aws s3api put-bucket-versioning \
    --bucket $BUCKET_NAME \
    --versioning-configuration Status=Enabled

# 퍼블릭 액세스 차단
aws s3api put-public-access-block \
    --bucket $BUCKET_NAME \
    --public-access-block-configuration \
    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

echo "S3 버킷 생성 완료: $BUCKET_NAME [향후 \$(cat ~/BUCKET_NAME) 으로 다시 읽을 수 있음.]"

```

### Terraform으로 버킷 생성 (선택사항)
```bash
cd 1.2.4.src
# Terraform으로 S3 버킷 및 파티션 구조 생성
terraform init
terraform apply
```

## Step 2: 파티션 구조 설계

### 파티션 스키마 정의
```bash
# 파티션 구조: year/month/day/hour
# s3://bucket/data/year=2024/month=01/day=15/hour=10/

# 기본 폴더 구조 생성
aws s3api put-object --bucket $BUCKET_NAME --key raw-data/
aws s3api put-object --bucket $BUCKET_NAME --key processed-data/
aws s3api put-object --bucket $BUCKET_NAME --key archived-data/
```

## Step 3: 테스트 데이터 생성 및 업로드

### 파티션된 데이터 생성 스크립트
# 소스 위치: ./1.2.4.src/generate_partitioned_data.py
```python
# 1.2.4.src/generate_partitioned_data.py
import json
import boto3
from datetime import datetime, timedelta
import random
import os

def generate_sample_data(count=1000):
    """샘플 데이터 생성"""
    data = []
    base_time = datetime(2024, 1, 15, 10, 0, 0)
    
    for i in range(count):
        record = {
            'id': i,
            'timestamp': (base_time + timedelta(minutes=i)).isoformat(),
            'user_id': random.randint(1000, 9999),
            'event_type': random.choice(['login', 'logout', 'purchase', 'view']),
            'amount': round(random.uniform(10.0, 1000.0), 2),
            'category': random.choice(['electronics', 'clothing', 'books', 'food'])
        }
        data.append(record)
    
    return data

def upload_partitioned_data(bucket_name, data):
    """파티션된 구조로 데이터 업로드"""
    s3 = boto3.client('s3')
    
    # 시간별로 데이터 그룹화
    partitioned_data = {}
    
    for record in data:
        timestamp = datetime.fromisoformat(record['timestamp'])
        partition_key = f"year={timestamp.year}/month={timestamp.month:02d}/day={timestamp.day:02d}/hour={timestamp.hour:02d}"
        
        if partition_key not in partitioned_data:
            partitioned_data[partition_key] = []
        
        partitioned_data[partition_key].append(record)
    
    # 각 파티션별로 파일 업로드
    for partition_key, records in partitioned_data.items():
        # JSON Lines 형식으로 저장
        file_content = '\n'.join(json.dumps(record) for record in records)
        
        # S3 키 생성
        s3_key = f"raw-data/{partition_key}/data.jsonl"
        
        # S3에 업로드
        s3.put_object(
            Bucket=bucket_name,
            Key=s3_key,
            Body=file_content,
            ContentType='application/jsonl'
        )
        
        print(f"업로드 완료: s3://{bucket_name}/{s3_key} ({len(records)} records)")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("사용법: python generate_partitioned_data.py <bucket-name>")
        sys.exit(1)
    
    bucket_name = sys.argv[1]
    
    # 데이터 생성 및 업로드
    print("샘플 데이터 생성 중...")
    sample_data = generate_sample_data(2000)
    
    print("파티션된 데이터 업로드 중...")
    upload_partitioned_data(bucket_name, sample_data)
    
    print("완료!")
```

### 데이터 업로드 실행
# 소스 위치: ./1.2.4.src/generate_partitioned_data.py
```bash
python 1.2.4.src/generate_partitioned_data.py $BUCKET_NAME
```

## Step 4: 스토리지 클래스 설정

### 라이프사이클 정책 생성
```json
# lifecycle-policy.json
{
    "Rules": [
        {
            "ID": "DataLakeLifecycle",
            "Status": "Enabled",
            "Filter": {
                "Prefix": "raw-data/"
            },
            "Transitions": [
                {
                    "Days": 30,
                    "StorageClass": "STANDARD_IA"
                },
                {
                    "Days": 90,
                    "StorageClass": "GLACIER"
                },
                {
                    "Days": 365,
                    "StorageClass": "DEEP_ARCHIVE"
                }
            ]
        }
    ]
}
```

### 라이프사이클 정책 적용
```bash
# 라이프사이클 정책 적용
aws s3api put-bucket-lifecycle-configuration \
    --bucket $BUCKET_NAME \
    --lifecycle-configuration file://lifecycle-policy.json

# 정책 확인
aws s3api get-bucket-lifecycle-configuration --bucket $BUCKET_NAME
```

## Step 5: 파티션 데이터 확인

### S3 구조 확인
```bash
# 버킷 내용 확인
echo "=== 버킷 구조 ==="
aws s3 ls s3://$BUCKET_NAME/ --recursive

# 특정 파티션 확인
echo "=== 2024년 1월 15일 10시 데이터 ==="
aws s3 ls s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/

# 파일 내용 샘플 확인
aws s3 cp s3://$BUCKET_NAME/raw-data/year=2024/month=01/day=15/hour=10/data.jsonl - | head -5
```

### 파티션 성능 테스트
# 소스 위치: ./1.2.4.src/partition_performance_test.py
```python
# 1.2.4.src/partition_performance_test.py
import boto3
import time
import json

def test_partition_query(bucket_name):
    """파티션 쿼리 성능 테스트"""
    s3 = boto3.client('s3')
    
    # 1. 전체 스캔 (비효율적)
    print("=== 전체 스캔 테스트 ===")
    start_time = time.time()
    
    response = s3.list_objects_v2(
        Bucket=bucket_name,
        Prefix='raw-data/'
    )
    
    total_objects = response.get('KeyCount', 0)
    scan_time = time.time() - start_time
    
    print(f"전체 객체 수: {total_objects}")
    print(f"스캔 시간: {scan_time:.2f}초")
    
    # 2. 파티션 기반 쿼리 (효율적)
    print("\n=== 파티션 쿼리 테스트 ===")
    start_time = time.time()
    
    # 특정 날짜의 데이터만 조회
    response = s3.list_objects_v2(
        Bucket=bucket_name,
        Prefix='raw-data/year=2024/month=01/day=15/'
    )
    
    partition_objects = response.get('KeyCount', 0)
    partition_time = time.time() - start_time
    
    print(f"파티션 객체 수: {partition_objects}")
    print(f"파티션 쿼리 시간: {partition_time:.2f}초")
    print(f"성능 개선: {(scan_time/partition_time):.1f}배 빠름")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("사용법: python partition_performance_test.py <bucket-name>")
        sys.exit(1)
    
    test_partition_query(sys.argv[1])
```

## Step 6: 정리

### 리소스 정리
```bash
# 버킷 내용 삭제
aws s3 rm s3://$BUCKET_NAME --recursive

# 버킷 삭제
aws s3 rb s3://$BUCKET_NAME

# 임시 파일 정리
rm -f lifecycle-policy.json
rm -f 1.2.4.src/generate_partitioned_data.py
rm -f 1.2.4.src/partition_performance_test.py

echo "정리 완료"
```

## 실습 결과 확인

### 성공 기준
- [ ] S3 버킷이 성공적으로 생성됨
- [ ] 파티션 구조로 데이터가 저장됨
- [ ] 라이프사이클 정책이 적용됨
- [ ] 파티션 쿼리 성능 개선 확인

### 학습 포인트
* **파티션 설계**: 쿼리 패턴에 맞는 파티션 구조 설계
* **스토리지 최적화**: 라이프사이클 정책을 통한 비용 절약
* **성능 개선**: 파티션 프루닝을 통한 쿼리 성능 향상
* **데이터 관리**: 체계적인 데이터 저장 및 관리 방법

파티션된 데이터 레이크 구조를 통해 효율적인 빅데이터 저장소를 구축했습니다!
