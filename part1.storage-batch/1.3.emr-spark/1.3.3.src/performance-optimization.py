# Spark DataFrame ì„±ëŠ¥ ìµœì í™”
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark import StorageLevel

def dataframe_optimization_techniques(df):
    """DataFrame ìµœì í™” ê¸°ë²•"""
    
    print("=== DataFrame ìµœì í™” ê¸°ë²• ===")
    
    optimization_results = {}
    
    # 1. íŒŒí‹°ì…”ë‹ ìµœì í™”
    current_partitions = df.rdd.getNumPartitions()
    record_count = df.count()
    
    # ì ì ˆí•œ íŒŒí‹°ì…˜ ìˆ˜ ê³„ì‚° (íŒŒí‹°ì…˜ë‹¹ 100K-1M ë ˆì½”ë“œ)
    optimal_partitions = max(min(record_count // 500000, 200), 1)
    
    print(f"ğŸ“Š íŒŒí‹°ì…˜ ì •ë³´:")
    print(f"   í˜„ì¬ íŒŒí‹°ì…˜: {current_partitions}")
    print(f"   ë ˆì½”ë“œ ìˆ˜: {record_count:,}")
    print(f"   ê¶Œì¥ íŒŒí‹°ì…˜: {optimal_partitions}")
    
    if optimal_partitions < current_partitions:
        optimized_df = df.coalesce(optimal_partitions)
        optimization_results['partitioned'] = optimized_df
        print(f"   â†’ Coalesceë¡œ {optimal_partitions}ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ì¶•ì†Œ")
    elif optimal_partitions > current_partitions:
        optimized_df = df.repartition(optimal_partitions)
        optimization_results['partitioned'] = optimized_df
        print(f"   â†’ Repartitionìœ¼ë¡œ {optimal_partitions}ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ í™•ì¥")
    else:
        optimized_df = df
        optimization_results['partitioned'] = optimized_df
        print(f"   â†’ í˜„ì¬ íŒŒí‹°ì…˜ ìˆ˜ê°€ ì ì ˆí•¨")
    
    return optimization_results

def caching_strategies(df):
    """ìºì‹± ì „ëµ"""
    
    print("=== ìºì‹± ì „ëµ ===")
    
    caching_results = {}
    
    # 1. ê¸°ë³¸ ë©”ëª¨ë¦¬ ìºì‹œ
    memory_cached_df = df.cache()
    memory_cached_df.count()  # ìºì‹œ ì›Œë°ì—…
    caching_results['memory'] = memory_cached_df
    print("âœ… ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©")
    
    # 2. ë©”ëª¨ë¦¬ + ë””ìŠ¤í¬ ìºì‹œ
    memory_disk_df = df.persist(StorageLevel.MEMORY_AND_DISK)
    caching_results['memory_disk'] = memory_disk_df
    print("âœ… ë©”ëª¨ë¦¬ + ë””ìŠ¤í¬ ìºì‹œ ì ìš©")
    
    # 3. ì§ë ¬í™”ëœ ë©”ëª¨ë¦¬ ìºì‹œ
    serialized_df = df.persist(StorageLevel.MEMORY_ONLY_SER)
    caching_results['serialized'] = serialized_df
    print("âœ… ì§ë ¬í™”ëœ ë©”ëª¨ë¦¬ ìºì‹œ ì ìš©")
    
    print("\nğŸ“‹ ìºì‹œ ì‚¬ìš© ê°€ì´ë“œë¼ì¸:")
    print("   - ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©ë˜ëŠ” DataFrameë§Œ ìºì‹œ")
    print("   - ë³€í™˜ í›„ ìºì‹œ (í•„í„°ë§, ì¡°ì¸ í›„)")
    print("   - ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ MEMORY_AND_DISK ì‚¬ìš©")
    print("   - í° DataFrameì€ ì§ë ¬í™” ìºì‹œ ê³ ë ¤")
    
    return caching_results

def schema_optimization(df):
    """ìŠ¤í‚¤ë§ˆ ìµœì í™”"""
    
    print("=== ìŠ¤í‚¤ë§ˆ ìµœì í™” ===")
    
    optimized_df = df
    changes_made = []
    
    # 1. ë°ì´í„° íƒ€ì… ìµœì í™”
    for field in df.schema.fields:
        
        # Longì„ Intë¡œ ë³€í™˜ (ê°’ì´ Int ë²”ìœ„ ë‚´ì¸ ê²½ìš°)
        if field.dataType == LongType() and field.name not in ["user_id", "event_id"]:
            try:
                max_val = df.agg(max(field.name)).collect()[0][0]
                if max_val and max_val < 2147483647:
                    optimized_df = optimized_df.withColumn(
                        field.name, 
                        col(field.name).cast(IntegerType())
                    )
                    changes_made.append(f"{field.name}: Long â†’ Int")
            except:
                pass
        
        # Stringì„ Booleanìœ¼ë¡œ ë³€í™˜
        elif field.dataType == StringType() and field.name.startswith("is_"):
            optimized_df = optimized_df.withColumn(
                field.name,
                when(col(field.name).isin(["true", "True", "1", "yes", "Y"]), True)
                .when(col(field.name).isin(["false", "False", "0", "no", "N"]), False)
                .otherwise(None).cast(BooleanType())
            )
            changes_made.append(f"{field.name}: String â†’ Boolean")
    
    # 2. ì»¬ëŸ¼ ìˆœì„œ ìµœì í™”
    priority_columns = []
    other_columns = []
    
    for col_name in optimized_df.columns:
        if col_name in ["user_id", "timestamp", "event_type", "value", "date"]:
            priority_columns.append(col_name)
        else:
            other_columns.append(col_name)
    
    if priority_columns:
        optimized_df = optimized_df.select(*(priority_columns + other_columns))
        changes_made.append("ì»¬ëŸ¼ ìˆœì„œ ìµœì í™” ì ìš©")
    
    print("ğŸ”§ ìŠ¤í‚¤ë§ˆ ìµœì í™” ë³€ê²½ì‚¬í•­:")
    for change in changes_made:
        print(f"   - {change}")
    
    return optimized_df

def performance_monitoring_and_tuning():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠœë‹"""
    
    print("=== ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠœë‹ ===")
    
    # ì„±ëŠ¥ ë³‘ëª© ì§€ì 
    bottlenecks = {
        "CPU ì§‘ì•½ì ": {
            "ì¦ìƒ": "CPU ì‚¬ìš©ë¥  ë†’ìŒ, ê¸´ ì²˜ë¦¬ ì‹œê°„",
            "í•´ê²°ì±…": ["executor ìˆ˜ ì¦ê°€", "ë³µì¡í•œ ì—°ì‚° ìµœì í™”", "íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€"]
        },
        "ë©”ëª¨ë¦¬ ë¶€ì¡±": {
            "ì¦ìƒ": "OOM ì—ëŸ¬, ìŠ¤í•„ë§ ë°œìƒ",
            "í•´ê²°ì±…": ["executor ë©”ëª¨ë¦¬ ì¦ê°€", "íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€", "ìºì‹œ ìµœì í™”"]
        },
        "I/O ë³‘ëª©": {
            "ì¦ìƒ": "ë””ìŠ¤í¬ ì½ê¸°/ì“°ê¸° ì§€ì—°",
            "í•´ê²°ì±…": ["íŒŒì¼ í¬ë§· ìµœì í™”", "ì••ì¶• ì‚¬ìš©", "íŒŒí‹°ì…”ë‹ ê°œì„ "]
        },
        "ë„¤íŠ¸ì›Œí¬ ë³‘ëª©": {
            "ì¦ìƒ": "ì…”í”Œ ë‹¨ê³„ì—ì„œ ì§€ì—°",
            "í•´ê²°ì±…": ["ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸", "íŒŒí‹°ì…˜ í‚¤ ìµœì í™”", "ë°ì´í„° ë¡œì»¬ë¦¬í‹° í–¥ìƒ"]
        }
    }
    
    print("ğŸ” ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë° í•´ê²°ì±…:")
    for bottleneck, info in bottlenecks.items():
        print(f"\n{bottleneck}:")
        print(f"   ì¦ìƒ: {info['ì¦ìƒ']}")
        print(f"   í•´ê²°ì±…: {', '.join(info['í•´ê²°ì±…'])}")

def spark_ui_analysis_guide():
    """Spark UI ë¶„ì„ ê°€ì´ë“œ"""
    
    print("=== Spark UI ë¶„ì„ ê°€ì´ë“œ ===")
    
    ui_tabs = {
        "Jobs": {
            "í™•ì¸ ì‚¬í•­": "ì‘ì—… ì‹¤í–‰ ì‹œê°„, ì‹¤íŒ¨í•œ ì‘ì—…",
            "ìµœì í™” í¬ì¸íŠ¸": "ê¸´ ì‹¤í–‰ ì‹œê°„ì˜ ì‘ì—… ë¶„ì„"
        },
        "Stages": {
            "í™•ì¸ ì‚¬í•­": "ìŠ¤í…Œì´ì§€ë³„ ì‹¤í–‰ ì‹œê°„, ì…”í”Œ ë°ì´í„° í¬ê¸°",
            "ìµœì í™” í¬ì¸íŠ¸": "ì…”í”Œì´ ë§ì€ ìŠ¤í…Œì´ì§€ ìµœì í™”"
        },
        "Storage": {
            "í™•ì¸ ì‚¬í•­": "ìºì‹œëœ ë°ì´í„° í¬ê¸°, ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ",
            "ìµœì í™” í¬ì¸íŠ¸": "ë¶ˆí•„ìš”í•œ ìºì‹œ ì œê±°"
        },
        "Environment": {
            "í™•ì¸ ì‚¬í•­": "Spark ì„¤ì •ê°’",
            "ìµœì í™” í¬ì¸íŠ¸": "ì„¤ì • íŠœë‹"
        },
        "Executors": {
            "í™•ì¸ ì‚¬í•­": "Executorë³„ ë©”ëª¨ë¦¬, CPU ì‚¬ìš©ë¥ ",
            "ìµœì í™” í¬ì¸íŠ¸": "ë¦¬ì†ŒìŠ¤ ë¶ˆê· í˜• í•´ê²°"
        },
        "SQL": {
            "í™•ì¸ ì‚¬í•­": "ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš, ì‹¤í–‰ ì‹œê°„",
            "ìµœì í™” í¬ì¸íŠ¸": "ë¹„íš¨ìœ¨ì ì¸ ì¿¼ë¦¬ ê°œì„ "
        }
    }
    
    print("ğŸ“Š Spark UI íƒ­ë³„ ë¶„ì„ í¬ì¸íŠ¸:")
    for tab, info in ui_tabs.items():
        print(f"\n{tab} íƒ­:")
        print(f"   í™•ì¸ ì‚¬í•­: {info['í™•ì¸ ì‚¬í•­']}")
        print(f"   ìµœì í™” í¬ì¸íŠ¸: {info['ìµœì í™” í¬ì¸íŠ¸']}")

def execution_plan_analysis():
    """ì‹¤í–‰ ê³„íš ë¶„ì„"""
    
    print("=== ì‹¤í–‰ ê³„íš ë¶„ì„ ===")
    
    # ì‹¤í–‰ ê³„íš ë¶„ì„ ë°©ë²•
    analysis_methods = {
        "explain()": {
            "ìš©ë„": "ê¸°ë³¸ ì‹¤í–‰ ê³„íš í™•ì¸",
            "ì‚¬ìš©ë²•": "df.explain()",
            "ì •ë³´": "ë¬¼ë¦¬ì  ì‹¤í–‰ ê³„íš"
        },
        "explain(True)": {
            "ìš©ë„": "ìƒì„¸ ì‹¤í–‰ ê³„íš í™•ì¸",
            "ì‚¬ìš©ë²•": "df.explain(True)",
            "ì •ë³´": "ë…¼ë¦¬ì  + ë¬¼ë¦¬ì  ê³„íš"
        },
        "explain('cost')": {
            "ìš©ë„": "ë¹„ìš© ê¸°ë°˜ ì‹¤í–‰ ê³„íš",
            "ì‚¬ìš©ë²•": "df.explain('cost')",
            "ì •ë³´": "ê° ë‹¨ê³„ë³„ ì˜ˆìƒ ë¹„ìš©"
        }
    }
    
    print("ğŸ” ì‹¤í–‰ ê³„íš ë¶„ì„ ë°©ë²•:")
    for method, info in analysis_methods.items():
        print(f"\n{method}:")
        print(f"   ìš©ë„: {info['ìš©ë„']}")
        print(f"   ì‚¬ìš©ë²•: {info['ì‚¬ìš©ë²•']}")
        print(f"   ì •ë³´: {info['ì •ë³´']}")
    
    # ì‹¤í–‰ ê³„íšì—ì„œ í™•ì¸í•  ì‚¬í•­
    print("\nğŸ“‹ ì‹¤í–‰ ê³„íš í™•ì¸ ì‚¬í•­:")
    check_points = [
        "ìŠ¤ìº”ë˜ëŠ” íŒŒì¼ ìˆ˜ì™€ í¬ê¸°",
        "í•„í„° ì¡°ê±´ì˜ í‘¸ì‹œë‹¤ìš´ ì—¬ë¶€",
        "ì¡°ì¸ íƒ€ì… (BroadcastHashJoin vs SortMergeJoin)",
        "ì…”í”Œ íŒŒí‹°ì…˜ ìˆ˜",
        "ì§‘ê³„ ì—°ì‚°ì˜ ë¶€ë¶„ ì§‘ê³„ ì—¬ë¶€",
        "ì»¬ëŸ¼ í”„ë£¨ë‹ ì ìš© ì—¬ë¶€"
    ]
    
    for point in check_points:
        print(f"   - {point}")

def common_performance_patterns():
    """ìì£¼ ì‚¬ìš©ë˜ëŠ” ì„±ëŠ¥ íŒ¨í„´"""
    
    print("=== ìì£¼ ì‚¬ìš©ë˜ëŠ” ì„±ëŠ¥ íŒ¨í„´ ===")
    
    patterns = {
        "í•„í„° ìš°ì„  ì ìš©": {
            "ì„¤ëª…": "ì¡°ì¸ì´ë‚˜ ì§‘ê³„ ì „ì— í•„í„° ì ìš©",
            "ì˜ˆì œ": "df.filter(condition).join(other_df)"
        },
        "ì»¬ëŸ¼ ì„ íƒ ìµœì í™”": {
            "ì„¤ëª…": "í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ í›„ ì—°ì‚°",
            "ì˜ˆì œ": "df.select('col1', 'col2').groupBy('col1').sum('col2')"
        },
        "ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸": {
            "ì„¤ëª…": "ì‘ì€ í…Œì´ë¸”ì„ ë¸Œë¡œë“œìºìŠ¤íŠ¸í•˜ì—¬ ì¡°ì¸",
            "ì˜ˆì œ": "large_df.join(broadcast(small_df), 'key')"
        },
        "íŒŒí‹°ì…˜ í‚¤ ì¡°ì¸": {
            "ì„¤ëª…": "ì¡°ì¸ í‚¤ë¡œ ì‚¬ì „ íŒŒí‹°ì…”ë‹",
            "ì˜ˆì œ": "df1.repartition('key').join(df2.repartition('key'), 'key')"
        },
        "ìºì‹œ í™œìš©": {
            "ì„¤ëª…": "ë°˜ë³µ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìºì‹œ",
            "ì˜ˆì œ": "df.filter(condition).cache()"
        }
    }
    
    print("ğŸš€ ì„±ëŠ¥ ìµœì í™” íŒ¨í„´:")
    for pattern, info in patterns.items():
        print(f"\n{pattern}:")
        print(f"   ì„¤ëª…: {info['ì„¤ëª…']}")
        print(f"   ì˜ˆì œ: {info['ì˜ˆì œ']}")

def troubleshooting_guide():
    """ì„±ëŠ¥ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ"""
    
    print("=== ì„±ëŠ¥ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ ===")
    
    common_issues = {
        "OutOfMemoryError": {
            "ì›ì¸": ["Executor ë©”ëª¨ë¦¬ ë¶€ì¡±", "íŒŒí‹°ì…˜ í¬ê¸° ë¶ˆê· í˜•", "ê³¼ë„í•œ ìºì‹œ ì‚¬ìš©"],
            "í•´ê²°ì±…": ["executor-memory ì¦ê°€", "íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€", "ìºì‹œ ì •ë¦¬"]
        },
        "ëŠë¦° ì¡°ì¸ ì„±ëŠ¥": {
            "ì›ì¸": ["í° í…Œì´ë¸”ê°„ ì¡°ì¸", "ë°ì´í„° ìŠ¤í", "ë¶€ì ì ˆí•œ ì¡°ì¸ íƒ€ì…"],
            "í•´ê²°ì±…": ["ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì¡°ì¸", "ì†”íŠ¸ í‚¤ ì¶”ê°€", "ì¡°ì¸ íŒíŠ¸ ì‚¬ìš©"]
        },
        "ê¸´ GC ì‹œê°„": {
            "ì›ì¸": ["í™ ë©”ëª¨ë¦¬ ë¶€ì¡±", "í° ê°ì²´ ìƒì„±", "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜"],
            "í•´ê²°ì±…": ["G1GC ì‚¬ìš©", "ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •", "ê°ì²´ ì¬ì‚¬ìš©"]
        },
        "ìŠ¤í…Œì´ì§€ ì‹¤íŒ¨": {
            "ì›ì¸": ["ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜", "ë…¸ë“œ ì¥ì• ", "íƒœìŠ¤í¬ íƒ€ì„ì•„ì›ƒ"],
            "í•´ê²°ì±…": ["ì¬ì‹œë„ ì„¤ì • ì¦ê°€", "í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸", "íƒ€ì„ì•„ì›ƒ ì¦ê°€"]
        }
    }
    
    print("ğŸ”§ ì¼ë°˜ì ì¸ ì„±ëŠ¥ ë¬¸ì œ ë° í•´ê²°ì±…:")
    for issue, info in common_issues.items():
        print(f"\n{issue}:")
        print(f"   ì›ì¸: {', '.join(info['ì›ì¸'])}")
        print(f"   í•´ê²°ì±…: {', '.join(info['í•´ê²°ì±…'])}")

if __name__ == "__main__":
    print("âœ… ì„±ëŠ¥ ìµœì í™” í•¨ìˆ˜ë“¤ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   - dataframe_optimization_techniques(): DataFrame ìµœì í™”")
    print("   - caching_strategies(): ìºì‹± ì „ëµ")
    print("   - schema_optimization(): ìŠ¤í‚¤ë§ˆ ìµœì í™”")
    print("   - performance_monitoring_and_tuning(): ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("   - spark_ui_analysis_guide(): Spark UI ë¶„ì„ ê°€ì´ë“œ")
    print("   - execution_plan_analysis(): ì‹¤í–‰ ê³„íš ë¶„ì„")
    print("   - common_performance_patterns(): ì„±ëŠ¥ íŒ¨í„´")
    print("   - troubleshooting_guide(): ë¬¸ì œ í•´ê²° ê°€ì´ë“œ")
