# Spark DataFrame ê¸°ë³¸ ì—°ì‚° í•¨ìˆ˜ë“¤
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

def create_optimized_spark_session(app_name="DataFrame-API-Guide"):
    """ìµœì í™”ëœ Spark ì„¸ì…˜ ìƒì„±"""
    
    spark = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    print(f"âœ… Spark Session ìƒì„± ì™„ë£Œ")
    print(f"   App Name: {app_name}")
    print(f"   Spark Version: {spark.version}")
    print(f"   Default Parallelism: {spark.sparkContext.defaultParallelism}")
    
    return spark

def define_common_schemas():
    """ìì£¼ ì‚¬ìš©ë˜ëŠ” ìŠ¤í‚¤ë§ˆ ì •ì˜"""
    
    # ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ
    user_schema = StructType([
        StructField("user_id", LongType(), False),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
        StructField("age", IntegerType(), True),
        StructField("signup_date", DateType(), True),
        StructField("preferences", MapType(StringType(), StringType()), True),
        StructField("tags", ArrayType(StringType()), True)
    ])
    
    # ì´ë²¤íŠ¸ ìŠ¤í‚¤ë§ˆ
    event_schema = StructType([
        StructField("event_id", LongType(), False),
        StructField("user_id", LongType(), False),
        StructField("event_type", StringType(), False),
        StructField("timestamp", TimestampType(), False),
        StructField("value", DoubleType(), True),
        StructField("properties", MapType(StringType(), StringType()), True)
    ])
    
    # ì£¼ë¬¸ ìŠ¤í‚¤ë§ˆ
    order_schema = StructType([
        StructField("order_id", LongType(), False),
        StructField("user_id", LongType(), False),
        StructField("product_id", LongType(), False),
        StructField("amount", DecimalType(10, 2), False),
        StructField("order_date", DateType(), False),
        StructField("status", StringType(), False)
    ])
    
    return {
        'user': user_schema,
        'event': event_schema,
        'order': order_schema
    }

def load_data_with_schema(spark, file_path, schema_name, file_format="parquet"):
    """ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë¡œë“œ"""
    
    schemas = define_common_schemas()
    
    if schema_name not in schemas:
        raise ValueError(f"Unknown schema: {schema_name}. Available: {list(schemas.keys())}")
    
    schema = schemas[schema_name]
    
    try:
        if file_format.lower() == "parquet":
            df = spark.read.schema(schema).parquet(file_path)
        elif file_format.lower() == "json":
            df = spark.read.schema(schema).option("multiline", "true").json(file_path)
        elif file_format.lower() == "csv":
            df = spark.read.schema(schema).option("header", "true").csv(file_path)
        else:
            raise ValueError(f"Unsupported format: {file_format}")
        
        print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {file_path}")
        print(f"   ë ˆì½”ë“œ ìˆ˜: {df.count():,}")
        print(f"   ìŠ¤í‚¤ë§ˆ: {schema_name}")
        
        return df
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def basic_dataframe_operations(df):
    """ê¸°ë³¸ DataFrame ì—°ì‚°"""
    
    print("=== ê¸°ë³¸ ì •ë³´ ===")
    print(f"ë ˆì½”ë“œ ìˆ˜: {df.count():,}")
    print(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"íŒŒí‹°ì…˜ ìˆ˜: {df.rdd.getNumPartitions()}")
    
    print("\n=== ì»¬ëŸ¼ ì •ë³´ ===")
    for field in df.schema.fields:
        nullable = "nullable" if field.nullable else "not null"
        print(f"  {field.name}: {field.dataType} ({nullable})")
    
    print("\n=== ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===")
    df.show(5, truncate=False)
    
    print("\n=== ê¸°ë³¸ í†µê³„ ===")
    df.describe().show()
    
    # ê¸°ë³¸ ì—°ì‚° ì˜ˆì œ
    operations = {}
    
    # ì»¬ëŸ¼ ì„ íƒ
    if "user_id" in df.columns and "name" in df.columns:
        operations['selected'] = df.select("user_id", "name")
    
    # í•„í„°ë§
    if "age" in df.columns:
        operations['filtered'] = df.filter(col("age") >= 18)
    
    # ì •ë ¬
    if "timestamp" in df.columns:
        operations['sorted'] = df.orderBy(desc("timestamp"))
    elif "signup_date" in df.columns:
        operations['sorted'] = df.orderBy(desc("signup_date"))
    
    # ì¤‘ë³µ ì œê±°
    if "email" in df.columns:
        operations['unique'] = df.dropDuplicates(["email"])
    
    return operations

def advanced_transformations(df):
    """ê³ ê¸‰ DataFrame ë³€í™˜"""
    
    enhanced_df = df
    
    # ë‚˜ì´ ê·¸ë£¹ ì¶”ê°€ (age ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
    if "age" in df.columns:
        enhanced_df = enhanced_df.withColumn("age_group", 
            when(col("age") < 18, "minor")
            .when(col("age") < 65, "adult")
            .otherwise("senior")
        )
    
    # ì´ë©”ì¼ ë„ë©”ì¸ ì¶”ì¶œ (email ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
    if "email" in df.columns:
        enhanced_df = enhanced_df.withColumn("email_domain", 
            regexp_extract(col("email"), r"@(.+)", 1)
        )
    
    # ë‚ ì§œ ê´€ë ¨ ë³€í™˜
    date_cols = [col for col in df.columns if "date" in col.lower() or col == "timestamp"]
    for date_col in date_cols:
        enhanced_df = enhanced_df \
            .withColumn(f"{date_col}_year", year(col(date_col))) \
            .withColumn(f"{date_col}_month", month(col(date_col))) \
            .withColumn(f"{date_col}_dayofweek", dayofweek(col(date_col)))
    
    # ë³µì¡í•œ ë°ì´í„° íƒ€ì… ì²˜ë¦¬
    for field in df.schema.fields:
        if isinstance(field.dataType, MapType):
            enhanced_df = enhanced_df \
                .withColumn(f"{field.name}_keys", map_keys(col(field.name))) \
                .withColumn(f"{field.name}_values", map_values(col(field.name)))
        
        elif isinstance(field.dataType, ArrayType):
            enhanced_df = enhanced_df \
                .withColumn(f"{field.name}_size", size(col(field.name)))
    
    # ìˆ˜ì¹˜í˜• ë³€í™˜
    numeric_cols = [field.name for field in df.schema.fields 
                   if isinstance(field.dataType, (IntegerType, LongType, DoubleType, FloatType))]
    
    for col_name in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œë§Œ
        if col_name != "user_id" and col_name != "event_id":  # ID ì»¬ëŸ¼ ì œì™¸
            enhanced_df = enhanced_df \
                .withColumn(f"{col_name}_log", log(col(col_name) + 1)) \
                .withColumn(f"{col_name}_sqrt", sqrt(abs(col(col_name))))
    
    return enhanced_df

def create_sample_data(spark):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
    
    # ì‚¬ìš©ì ë°ì´í„°
    users_data = [
        (1, "Alice Johnson", "alice@gmail.com", 28, "2023-01-15"),
        (2, "Bob Smith", "bob@yahoo.com", 35, "2023-02-20"),
        (3, "Charlie Brown", "charlie@gmail.com", 42, "2023-01-10"),
        (4, "Diana Wilson", "diana@hotmail.com", 31, "2023-03-05"),
        (5, "Eve Davis", "eve@gmail.com", 26, "2023-02-28")
    ]
    
    users_schema = ["user_id", "name", "email", "age", "signup_date"]
    users_df = spark.createDataFrame(users_data, users_schema)
    users_df = users_df.withColumn("signup_date", to_date(col("signup_date")))
    
    # ì´ë²¤íŠ¸ ë°ì´í„°
    events_data = [
        (101, 1, "login", "2024-01-01 10:00:00", 0.0),
        (102, 1, "view", "2024-01-01 10:05:00", 0.0),
        (103, 1, "purchase", "2024-01-01 10:30:00", 99.99),
        (104, 2, "login", "2024-01-01 11:00:00", 0.0),
        (105, 2, "view", "2024-01-01 11:10:00", 0.0),
        (106, 3, "login", "2024-01-01 12:00:00", 0.0),
        (107, 3, "purchase", "2024-01-01 12:30:00", 149.99),
        (108, 1, "view", "2024-01-02 09:00:00", 0.0)
    ]
    
    events_schema = ["event_id", "user_id", "event_type", "timestamp", "value"]
    events_df = spark.createDataFrame(events_data, events_schema)
    events_df = events_df.withColumn("timestamp", to_timestamp(col("timestamp")))
    
    return users_df, events_df

def dataframe_info_summary(df, df_name="DataFrame"):
    """DataFrame ì •ë³´ ìš”ì•½"""
    
    print(f"\n{'='*50}")
    print(f"{df_name} ì •ë³´ ìš”ì•½")
    print(f"{'='*50}")
    
    # ê¸°ë³¸ ì •ë³´
    record_count = df.count()
    column_count = len(df.columns)
    partition_count = df.rdd.getNumPartitions()
    
    print(f"ğŸ“Š ë ˆì½”ë“œ ìˆ˜: {record_count:,}")
    print(f"ğŸ“‹ ì»¬ëŸ¼ ìˆ˜: {column_count}")
    print(f"ğŸ—‚ï¸  íŒŒí‹°ì…˜ ìˆ˜: {partition_count}")
    
    # ìŠ¤í‚¤ë§ˆ ì •ë³´
    print(f"\nğŸ“ ìŠ¤í‚¤ë§ˆ:")
    for field in df.schema.fields:
        print(f"   {field.name}: {field.dataType}")
    
    # ìƒ˜í”Œ ë°ì´í„°
    print(f"\nğŸ” ìƒ˜í”Œ ë°ì´í„°:")
    df.show(3, truncate=True)
    
    # ê²°ì¸¡ê°’ ì²´í¬
    print(f"\nâ“ ê²°ì¸¡ê°’ ì²´í¬:")
    for col_name in df.columns:
        null_count = df.filter(col(col_name).isNull()).count()
        if null_count > 0:
            print(f"   {col_name}: {null_count:,} ({null_count/record_count*100:.1f}%)")
    
    return {
        'record_count': record_count,
        'column_count': column_count,
        'partition_count': partition_count
    }

if __name__ == "__main__":
    # ìƒ˜í”Œ ì‹¤í–‰
    spark = create_optimized_spark_session()
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    users_df, events_df = create_sample_data(spark)
    
    # ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
    print("=== ì‚¬ìš©ì ë°ì´í„° ë¶„ì„ ===")
    user_info = dataframe_info_summary(users_df, "Users")
    user_ops = basic_dataframe_operations(users_df)
    
    print("\n=== ì´ë²¤íŠ¸ ë°ì´í„° ë¶„ì„ ===")
    event_info = dataframe_info_summary(events_df, "Events")
    event_ops = basic_dataframe_operations(events_df)
    
    print("âœ… ê¸°ë³¸ ì—°ì‚° í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ")
