# 1.3 EMR-Spark 데이터 파이프라인

## 개요
이 섹션은 AWS EMR 기반 Spark 데이터 파이프라인의 실전 구현 및 최적화 전략을 다룹니다.  
S3에 저장된 대용량 데이터를 Spark로 집계/분석하고, 고급 최적화 기법과 자동화된 배치 실행까지 실습할 수 있습니다.

---

## 주요 코드/스크립트 설명

### 1. 기본 데이터 처리: `batch-processing.py`
- **기능:**  
  S3의 파티션 데이터를 읽어 사용자별/일별/월별 집계 및 통계 산출 후 S3에 저장
- **주요 처리 흐름:**  
  - S3 Parquet 데이터 읽기  
  - 사용자별 일별 활동 집계  
  - 월별 요약 통계  
  - 결과 S3 저장 (파티셔닝)
- **실행 예시:**  
  ```bash
  python 3.4-batch-processing.py --input s3://bucket/input/ --output s3://bucket/output/
  ```
- **핵심 코드 예시:**
  ```python
  daily_user_activity = df \
      .filter(col("event_type").isin(["click", "view", "purchase"])) \
      .groupBy("user_id", "year", "month", "day", "event_type") \
      .agg(
          count("*").alias("event_count"),
          sum("value").alias("total_value"),
          avg("value").alias("avg_value")
      )
  ```

---

### 2. Spark 최적화 및 성능 분석: `3.5-emr-spark-job.py`
- **기능:**  
  고급 Spark 최적화(파티션, 캐싱, 브로드캐스트, 윈도우, Cube/Rollup 등) 및 성능 비교, 쿼리 실행 계획 분석
- **주요 처리 흐름:**  
  - 파티션 최적화 및 캐싱 전략  
  - 사용자 행동/상품 성과/시계열 트렌드/다차원 집계  
  - 성능 비교(기본 vs 최적화), 쿼리 실행 계획 분석
- **실행 예시:**  
  ```bash
  python 3.5-emr-spark-job.py --input s3://bucket/input/ --output s3://bucket/output/ --performance-test
  ```
- **핵심 코드 예시:**
  ```python
  user_behavior = df_cached \
      .groupBy("user_id") \
      .agg(
          countDistinct("event_type").alias("distinct_events"),
          count("*").alias("total_events"),
          sum("value").alias("lifetime_value")
      )
  ```

---

### 3. Spark 작업 제출 자동화: `3.5-submit-job.sh`
- **기능:**  
  Spark Python 스크립트(위 py 파일들)를 EMR 또는 로컬 환경에 자동 제출  
  Terraform output에서 S3/EMR 정보 자동 추출, S3 업로드, 로그 확인 등 지원
- **사용법:**  
  ```bash
  ./3.5-submit-job.sh <python-script> [추가-인자들...]
  ```
  예시:
  ```bash
  ./3.5-submit-job.sh 3.4-batch-processing.py --input s3://bucket/input/ --output s3://bucket/output/
  ```
- **주요 옵션:**  
  - --input, --output: S3 경로 지정(없으면 자동 추출)
  - --help: 사용법 안내

---

## 실습/활용 가이드

1. S3 및 EMR 환경 준비 (Terraform 등으로 인프라 배포)
2. 데이터 적재 후, py 스크립트로 Spark 집계/분석 실행
3. submit-job.sh로 자동화된 배치 실행 및 결과 확인
4. 각 py 파일의 파라미터/옵션은 --help로 확인 가능

---

## 코드/문서 분리 원칙

- **코드(py, sh):** 실제 실행 로직만 포함, 주석 최소화
- **문서(md):** 전체 흐름, 목적, 실습 가이드, 주요 코드 인용, 실행법 등 설명 중심

---
