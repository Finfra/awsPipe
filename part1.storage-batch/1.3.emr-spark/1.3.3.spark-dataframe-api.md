# 1.3.3. Spark DataFrame API 활용

## 개요
Spark DataFrame API의 핵심 기능과 성능 최적화 기법을 실습 중심으로 학습

## DataFrame 기본 개념
* **구조화된 데이터**: 스키마가 정의된 분산 데이터셋
* **지연 실행**: 액션이 호출될 때까지 변환 연산을 지연
* **최적화**: Catalyst 옵티마이저를 통한 자동 최적화

## 실행 방법

### 1. 기본 연산 실습
```python
# 기본 연산 함수 로드
%run 1.3.3.src/basic-operations.py

# Spark 세션 생성
spark = create_optimized_spark_session("DataFrame-Practice")

# 샘플 데이터 생성
users_df, events_df = create_sample_data(spark)

# 기본 연산 실행
user_ops = basic_dataframe_operations(users_df)
enhanced_df = advanced_transformations(events_df)
```

### 2. 집계 및 그룹화
```python
# 집계 함수 로드
%run 1.3.3.src/aggregations.py

# 기본 집계 실행
basic_stats = basic_aggregations(events_df)
grouped_stats = group_by_aggregations(events_df)

# 시계열 집계
time_stats = time_series_aggregations(events_df)
advanced_stats = advanced_aggregations(events_df)
```

### 3. 조인 연산
```python
# 조인 함수 로드
%run 1.3.3.src/joins.py

# 샘플 데이터 생성
users_df, orders_df, products_df, order_details_df = create_join_sample_data(spark)

# 기본 조인 실행
join_results = basic_join_operations(users_df, orders_df)

# 복잡한 조인
complex_joins = complex_join_conditions(users_df, orders_df)
multi_joins = multi_table_joins(users_df, orders_df, products_df, order_details_df)
```

### 4. 윈도우 함수
```python
# 윈도우 함수 로드
%run 1.3.3.src/window-functions.py

# 윈도우 함수용 데이터 생성
sales_df, stock_df = create_window_sample_data(spark)

# 윈도우 함수 실행
ranking_results = ranking_functions(sales_df)
analytical_results = analytical_functions(sales_df)
aggregate_results = aggregate_window_functions(sales_df)
advanced_results = advanced_window_techniques(stock_df)
```

### 5. 성능 최적화
```python
# 성능 최적화 함수 로드
%run 1.3.3.src/performance-optimization.py

# 최적화 적용
optimized_df = dataframe_optimization_techniques(events_df)
cached_results = caching_strategies(events_df)
schema_optimized = schema_optimization(events_df)

# 성능 모니터링
performance_monitoring_and_tuning()
spark_ui_analysis_guide()
```

## 소스 파일 구성

### 핵심 기능별 파일
* **basic-operations.py**: DataFrame 생성, 기본 연산, 변환
* **aggregations.py**: 집계, 그룹화, 시계열 분석
* **joins.py**: 조인 연산, 복잡한 조인 조건
* **window-functions.py**: 윈도우 함수, 랭킹, 분석 함수
* **performance-optimization.py**: 성능 최적화, 캐싱, 스키마 최적화

### 실행 권한 설정
```bash
chmod +x 1.3.3.src/*.py
```

## 주요 기능

### DataFrame 기본 연산
* 스키마 정의 및 데이터 로드
* 선택, 필터링, 정렬, 중복 제거
* 컬럼 추가, 수정, 변환
* 복잡한 데이터 타입 처리 (Map, Array)

### 집계 및 분석
* 기본 통계 (count, sum, avg, min, max)
* 그룹별 집계 및 조건부 집계
* 백분위수, 표준편차 계산
* 시계열 데이터 집계 (일별, 월별, 요일별)

### 조인 최적화
* 다양한 조인 타입 (Inner, Left, Right, Full, Semi, Anti)
* 브로드캐스트 조인을 통한 성능 향상
* 복잡한 조인 조건 및 다중 테이블 조인
* 조인 성능 문제 해결 가이드

### 윈도우 함수 활용
* 랭킹 함수 (ROW_NUMBER, RANK, DENSE_RANK)
* 분석 함수 (LAG, LEAD, FIRST, LAST)
* 집계 윈도우 함수 (누적 합계, 이동 평균)
* 윈도우 프레임 최적화

### 성능 최적화
* 파티셔닝 및 캐싱 전략
* 스키마 최적화 및 데이터 타입 조정
* 실행 계획 분석 및 튜닝
* Spark UI를 통한 성능 모니터링

## SQL vs DataFrame API 비교

| 기능 | SQL | DataFrame API |
| 기본 선택 | `SELECT col FROM table` | `df.select("col")` |
| 필터링 | `WHERE condition` | `df.filter(condition)` |
| 집계 | `GROUP BY col` | `df.groupBy("col")` |
| 조인 | `JOIN table ON key` | `df.join(other, "key")` |
| 윈도우 함수 | `OVER (PARTITION BY)` | `Window.partitionBy()` |

## 모범 사례

### 성능 최적화
* 필터 조건을 가능한 한 일찍 적용
* 필요한 컬럼만 선택 후 연산 수행
* 작은 테이블에 브로드캐스트 조인 사용
* 반복 사용되는 DataFrame 캐시 활용

### 코드 가독성
* 복잡한 변환은 단계별로 분리
* 의미 있는 컬럼 이름 사용
* 스키마를 명시적으로 정의
* 실행 계획을 정기적으로 검토

### 에러 처리
* NULL 값 처리 전략 수립
* 데이터 타입 변환 시 예외 처리
* 조인 시 키 존재 여부 확인
* 메모리 부족 시 파티션 수 조정

## 실습 시나리오

### 1. 사용자 행동 분석
```python
# 사용자별 활동 요약
user_activity = events_df.groupBy("user_id").agg(
    count("*").alias("total_events"),
    countDistinct("event_type").alias("unique_events"),
    sum("value").alias("total_value")
)
```

### 2. 시계열 트렌드 분석
```python
# 일별 매출 트렌드
daily_sales = events_df.filter(col("event_type") == "purchase") \
    .groupBy(to_date("timestamp").alias("date")) \
    .agg(sum("value").alias("daily_sales"))
```

### 3. 상위 N개 분석
```python
# 상위 10 사용자 (매출 기준)
top_users = events_df.groupBy("user_id") \
    .agg(sum("value").alias("total_spent")) \
    .orderBy(desc("total_spent")) \
    .limit(10)
```

이제 Spark DataFrame API의 모든 핵심 기능을 체계적으로 학습하고 실습할 수 있음.
