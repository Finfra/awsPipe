# 3.3 Spark DataFrame API 활용

## DataFrame 기본 연산

### DataFrame 생성 및 스키마 정의
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Spark 세션 생성
spark = SparkSession.builder \
    .appName("DataFrame-API-Guide") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# 명시적 스키마 정의
user_schema = StructType([
    StructField("user_id", LongType(), False),
    StructField("name", StringType(), True),
    StructField("email", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("signup_date", DateType(), True),
    StructField("preferences", MapType(StringType(), StringType()), True),
    StructField("tags", ArrayType(StringType()), True)
])

# 스키마를 사용한 DataFrame 생성
df = spark.read \
    .schema(user_schema) \
    .option("multiline", "true") \
    .json("s3://bucket/users/")

print("스키마 정보:")
df.printSchema()
```

### 기본 DataFrame 연산
```python
def basic_dataframe_operations(df):
    """기본 DataFrame 연산 예제"""
    
    print("=== 기본 정보 ===")
    print(f"레코드 수: {df.count():,}")
    print(f"컬럼 수: {len(df.columns)}")
    print(f"파티션 수: {df.rdd.getNumPartitions()}")
    
    print("\n=== 컬럼 정보 ===")
    for field in df.schema.fields:
        print(f"{field.name}: {field.dataType} (nullable: {field.nullable})")
    
    print("\n=== 데이터 미리보기 ===")
    df.show(5, truncate=False)
    
    print("\n=== 기본 통계 ===")
    df.describe().show()
    
    # 특정 컬럼 선택
    selected_df = df.select("user_id", "name", "age")
    
    # 필터링
    adult_users = df.filter(col("age") >= 18)
    
    # 정렬
    sorted_df = df.orderBy(desc("signup_date"), "name")
    
    # 중복 제거
    unique_df = df.dropDuplicates(["email"])
    
    return {
        'selected': selected_df,
        'filtered': adult_users,
        'sorted': sorted_df,
        'unique': unique_df
    }

# 실행
results = basic_dataframe_operations(df)
```

### 고급 변환 연산
```python
def advanced_transformations(df):
    """고급 DataFrame 변환"""
    
    # 1. 컬럼 추가 및 수정
    enhanced_df = df \
        .withColumn("age_group", 
            when(col("age") < 18, "minor")
            .when(col("age") < 65, "adult")
            .otherwise("senior")
        ) \
        .withColumn("email_domain", 
            regexp_extract(col("email"), r"@(.+)", 1)
        ) \
        .withColumn("signup_year", 
            year(col("signup_date"))
        ) \
        .withColumn("days_since_signup", 
            datediff(current_date(), col("signup_date"))
        )
    
    # 2. 복잡한 데이터 타입 처리
    complex_df = enhanced_df \
        .withColumn("preferences_keys", 
            map_keys(col("preferences"))
        ) \
        .withColumn("preferences_values", 
            map_values(col("preferences"))
        ) \
        .withColumn("tag_count", 
            size(col("tags"))
        ) \
        .withColumn("has_premium_tag", 
            array_contains(col("tags"), "premium")
        )
    
    # 3. 문자열 함수 활용
    string_processing = complex_df \
        .withColumn("name_upper", 
            upper(col("name"))
        ) \
        .withColumn("name_initials", 
            regexp_extract(col("name"), r"(\w)\w*\s+(\w)", "1.2")
        ) \
        .withColumn("email_masked", 
            regexp_replace(col("email"), r"(.{2}).*(@.*)", "$1***$2")
        )
    
    # 4. 수치 함수 활용
    numeric_processing = string_processing \
        .withColumn("age_normalized", 
            (col("age") - lit(18)) / lit(65 - 18)
        ) \
        .withColumn("age_rounded", 
            round(col("age") / 10) * 10
        ) \
        .withColumn("random_score", 
            rand() * 100
        )
    
    return numeric_processing

# 실행
transformed_df = advanced_transformations(df)
transformed_df.show(5, truncate=False)
```

## 집계 및 그룹화 연산

### 기본 집계
```python
def basic_aggregations(df):
    """기본 집계 연산"""
    
    # 전체 통계
    overall_stats = df.agg(
        count("*").alias("total_users"),
        countDistinct("email_domain").alias("unique_domains"),
        avg("age").alias("avg_age"),
        min("signup_date").alias("earliest_signup"),
        max("signup_date").alias("latest_signup"),
        stddev("age").alias("age_stddev")
    )
    
    print("전체 통계:")
    overall_stats.show()
    
    # 그룹별 집계
    domain_stats = df \
        .groupBy("email_domain") \
        .agg(
            count("*").alias("user_count"),
            avg("age").alias("avg_age"),
            min("age").alias("min_age"),
            max("age").alias("max_age"),
            collect_list("name").alias("user_names")
        ) \
        .orderBy(desc("user_count"))
    
    print("도메인별 통계:")
    domain_stats.show(10, truncate=False)
    
    return overall_stats, domain_stats

# 고급 집계 함수
def advanced_aggregations(df):
    """고급 집계 함수 활용"""
    
    # 사용자 정의 집계 함수 (UDAF)
    from pyspark.sql.functions import pandas_udf
    from pyspark.sql.types import DoubleType
    
    @pandas_udf(returnType=DoubleType())
    def geometric_mean(values):
        """기하평균 계산"""
        import pandas as pd
        import numpy as np
        return np.exp(np.log(values).mean())
    
    # 백분위수 계산
    percentile_stats = df \
        .select(
            expr("percentile_approx(age, 0.25)").alias("age_q1"),
            expr("percentile_approx(age, 0.5)").alias("age_median"), 
            expr("percentile_approx(age, 0.75)").alias("age_q3"),
            expr("percentile_approx(age, array(0.1, 0.9))").alias("age_deciles")
        )
    
    print("백분위수 통계:")
    percentile_stats.show()
    
    # 고급 그룹 집계
    multi_level_agg = df \
        .groupBy("age_group", "email_domain") \
        .agg(
            count("*").alias("count"),
            avg("age").alias("avg_age"),
            # 조건부 집계
            sum(when(col("tags").isNotNull(), 1).otherwise(0)).alias("users_with_tags"),
            # 중복 제거 후 집계
            countDistinct("signup_date").alias("unique_signup_dates"),
            # 첫번째/마지막 값
            first("name").alias("first_user"),
            last("name").alias("last_user")
        )
    
    return percentile_stats, multi_level_agg

# 실행
basic_stats = basic_aggregations(df)
advanced_stats = advanced_aggregations(df)
```

### 윈도우 함수 활용
```python
from pyspark.sql.window import Window

def window_functions_examples(df):
    """윈도우 함수 활용 예제"""
    
    # 1. 기본 윈도우 정의
    # 전체 데이터에 대한 윈도우
    overall_window = Window.orderBy("age")
    
    # 그룹별 윈도우
    domain_window = Window.partitionBy("email_domain").orderBy("signup_date")
    
    # 범위 제한 윈도우
    recent_users_window = Window \
        .partitionBy("email_domain") \
        .orderBy("signup_date") \
        .rowsBetween(-30, 0)  # 현재 행부터 이전 30행까지
    
    # 2. 랭킹 함수
    ranking_df = df \
        .withColumn("age_rank", 
            row_number().over(overall_window)
        ) \
        .withColumn("age_dense_rank", 
            dense_rank().over(overall_window)
        ) \
        .withColumn("age_percentile", 
            percent_rank().over(overall_window)
        ) \
        .withColumn("domain_signup_order", 
            row_number().over(domain_window)
        )
    
    # 3. 분석 함수
    analytical_df = ranking_df \
        .withColumn("prev_user_age", 
            lag("age", 1).over(domain_window)
        ) \
        .withColumn("next_user_age", 
            lead("age", 1).over(domain_window)
        ) \
        .withColumn("age_diff_from_prev", 
            col("age") - col("prev_user_age")
        ) \
        .withColumn("running_avg_age", 
            avg("age").over(recent_users_window)
        ) \
        .withColumn("domain_user_count", 
            count("*").over(Window.partitionBy("email_domain"))
        )
    
    # 4. 집계 윈도우 함수
    window_agg_df = analytical_df \
        .withColumn("domain_total_users", 
            sum(lit(1)).over(Window.partitionBy("email_domain"))
        ) \
        .withColumn("domain_avg_age", 
            avg("age").over(Window.partitionBy("email_domain"))
        ) \
        .withColumn("cumulative_users", 
            sum(lit(1)).over(domain_window)
        )
    
    return window_agg_df

# 실행
windowed_df = window_functions_examples(df)
windowed_df.select("name", "age", "email_domain", "age_rank", "domain_signup_order", "running_avg_age").show(10)
```

## 조인 연산

### 다양한 조인 타입
```python
def join_operations_examples():
    """조인 연산 예제"""
    
    # 샘플 데이터 생성
    users_data = [
        (1, "Alice", "alice@gmail.com"),
        (2, "Bob", "bob@yahoo.com"),
        (3, "Charlie", "charlie@gmail.com")
    ]
    users_df = spark.createDataFrame(users_data, ["user_id", "name", "email"])
    
    orders_data = [
        (101, 1, 100.0, "2024-01-01"),
        (102, 1, 150.0, "2024-01-02"),
        (103, 2, 200.0, "2024-01-01"),
        (104, 4, 75.0, "2024-01-03")  # user_id 4는 users에 없음
    ]
    orders_df = spark.createDataFrame(orders_data, ["order_id", "user_id", "amount", "order_date"])
    
    # 1. Inner Join (기본)
    inner_join = users_df.join(orders_df, "user_id")
    print("Inner Join:")
    inner_join.show()
    
    # 2. Left Join
    left_join = users_df.join(orders_df, "user_id", "left")
    print("Left Join:")
    left_join.show()
    
    # 3. Right Join
    right_join = users_df.join(orders_df, "user_id", "right")
    print("Right Join:")
    right_join.show()
    
    # 4. Full Outer Join
    full_join = users_df.join(orders_df, "user_id", "full")
    print("Full Outer Join:")
    full_join.show()
    
    # 5. Semi Join (왼쪽 테이블의 매칭되는 행만)
    semi_join = users_df.join(orders_df, "user_id", "left_semi")
    print("Left Semi Join:")
    semi_join.show()
    
    # 6. Anti Join (왼쪽 테이블의 매칭되지 않는 행만)
    anti_join = users_df.join(orders_df, "user_id", "left_anti")
    print("Left Anti Join:")
    anti_join.show()
    
    return inner_join, left_join, right_join, full_join, semi_join, anti_join

# 복잡한 조인 조건
def complex_join_conditions():
    """복잡한 조인 조건 예제"""
    
    # 다중 컬럼 조인
    multi_col_join = df1.join(
        df2, 
        (df1.user_id == df2.user_id) & (df1.date == df2.date)
    )
    
    # 범위 조인
    range_join = df1.join(
        df2,
        (df1.user_id == df2.user_id) & 
        (df1.timestamp >= df2.start_time) & 
        (df1.timestamp <= df2.end_time)
    )
    
    # 브로드캐스트 조인 (작은 테이블 최적화)
    from pyspark.sql.functions import broadcast
    
    broadcast_join = large_df.join(
        broadcast(small_df), 
        "common_key"
    )
    
    return multi_col_join, range_join, broadcast_join

# 실행
join_examples = join_operations_examples()
```

## 성능 최적화

### DataFrame 최적화 기법
```python
def dataframe_optimization_techniques(df):
    """DataFrame 최적화 기법"""
    
    # 1. 파티셔닝 최적화
    def optimize_partitioning(df):
        """파티셔닝 최적화"""
        
        current_partitions = df.rdd.getNumPartitions()
        record_count = df.count()
        
        # 적절한 파티션 수 계산 (파티션당 약 100K-1M 레코드)
        optimal_partitions = max(record_count // 500000, 1)
        
        print(f"현재 파티션: {current_partitions}, 최적 파티션: {optimal_partitions}")
        
        if optimal_partitions < current_partitions:
            # 파티션 수 줄이기
            return df.coalesce(optimal_partitions)
        elif optimal_partitions > current_partitions:
            # 파티션 수 늘리기 (재분산)
            return df.repartition(optimal_partitions)
        else:
            return df
    
    # 2. 캐싱 전략
    def apply_caching_strategy(df):
        """캐싱 전략 적용"""
        
        # 여러 번 사용될 DataFrame은 캐시
        cached_df = df.cache()
        
        # 캐시 워밍업
        cached_df.count()
        
        # 스토리지 레벨 선택
        from pyspark import StorageLevel
        
        # 메모리 + 디스크 캐시
        memory_and_disk_df = df.persist(StorageLevel.MEMORY_AND_DISK)
        
        # 직렬화된 메모리 캐시 (메모리 절약)
        serialized_df = df.persist(StorageLevel.MEMORY_ONLY_SER)
        
        return cached_df, memory_and_disk_df, serialized_df
    
    # 3. 조건부 최적화
    def conditional_optimization(df):
        """조건부 최적화"""
        
        # 자주 필터링되는 조건을 미리 적용
        hot_data = df.filter(col("status") == "active").cache()
        cold_data = df.filter(col("status") != "active")
        
        # 컬럼 순서 최적화 (자주 사용되는 컬럼을 앞에)
        optimized_columns = ["user_id", "timestamp", "event_type", "value"] + \
                          [c for c in df.columns if c not in ["user_id", "timestamp", "event_type", "value"]]
        
        reordered_df = df.select(*optimized_columns)
        
        return hot_data, cold_data, reordered_df
    
    # 4. 스키마 최적화
    def optimize_schema(df):
        """스키마 최적화"""
        
        optimized_df = df
        
        # String을 Category로 (반복되는 값이 많은 경우)
        category_columns = ["status", "category", "region"]
        
        # 불필요한 정밀도 줄이기
        if "amount" in df.columns:
            optimized_df = optimized_df.withColumn(
                "amount", 
                round(col("amount"), 2).cast(DecimalType(10, 2))
            )
        
        # Boolean 타입 최적화
        if "is_active" in df.columns:
            optimized_df = optimized_df.withColumn(
                "is_active",
                when(col("is_active") == "true", True)
                .when(col("is_active") == "false", False)
                .otherwise(None).cast(BooleanType())
            )
        
        return optimized_df
    
    # 모든 최적화 적용
    optimized_df = optimize_partitioning(df)
    cached_dfs = apply_caching_strategy(optimized_df)
    conditional_dfs = conditional_optimization(optimized_df)
    schema_optimized = optimize_schema(optimized_df)
    
    return {
        'partitioned': optimized_df,
        'cached': cached_dfs,
        'conditional': conditional_dfs,
        'schema_optimized': schema_optimized
    }

# 실행
optimization_results = dataframe_optimization_techniques(df)
```

### SQL과 DataFrame API 비교
```python
def sql_vs_dataframe_api():
    """SQL vs DataFrame API 비교"""
    
    # DataFrame을 임시 뷰로 등록
    df.createOrReplaceTempView("users")
    
    # 1. 기본 선택 및 필터링
    # SQL 방식
    sql_result1 = spark.sql("""
        SELECT name, age, email_domain
        FROM users 
        WHERE age >= 18 AND age <= 65
        ORDER BY age DESC
    """)
    
    # DataFrame API 방식
    df_result1 = df \
        .select("name", "age", "email_domain") \
        .filter((col("age") >= 18) & (col("age") <= 65)) \
        .orderBy(desc("age"))
    
    # 2. 복잡한 집계
    # SQL 방식
    sql_result2 = spark.sql("""
        SELECT 
            email_domain,
            COUNT(*) as user_count,
            AVG(age) as avg_age,
            PERCENTILE_APPROX(age, 0.5) as median_age,
            MIN(signup_date) as first_signup,
            MAX(signup_date) as last_signup
        FROM users
        GROUP BY email_domain
        HAVING COUNT(*) >= 10
        ORDER BY user_count DESC
    """)
    
    # DataFrame API 방식
    df_result2 = df \
        .groupBy("email_domain") \
        .agg(
            count("*").alias("user_count"),
            avg("age").alias("avg_age"),
            expr("percentile_approx(age, 0.5)").alias("median_age"),
            min("signup_date").alias("first_signup"),
            max("signup_date").alias("last_signup")
        ) \
        .filter(col("user_count") >= 10) \
        .orderBy(desc("user_count"))
    
    # 3. 윈도우 함수
    # SQL 방식
    sql_result3 = spark.sql("""
        SELECT 
            name, age, email_domain,
            ROW_NUMBER() OVER (PARTITION BY email_domain ORDER BY age) as age_rank,
            AVG(age) OVER (PARTITION BY email_domain) as domain_avg_age,
            LAG(age, 1) OVER (PARTITION BY email_domain ORDER BY signup_date) as prev_user_age
        FROM users
    """)
    
    # DataFrame API 방식
    window_spec = Window.partitionBy("email_domain").orderBy("age")
    domain_window = Window.partitionBy("email_domain").orderBy("signup_date")
    
    df_result3 = df \
        .withColumn("age_rank", row_number().over(window_spec)) \
        .withColumn("domain_avg_age", avg("age").over(Window.partitionBy("email_domain"))) \
        .withColumn("prev_user_age", lag("age", 1).over(domain_window)) \
        .select("name", "age", "email_domain", "age_rank", "domain_avg_age", "prev_user_age")
    
    return {
        'sql_results': [sql_result1, sql_result2, sql_result3],
        'df_results': [df_result1, df_result2, df_result3]
    }

# 실행 및 성능 비교
def compare_performance():
    """SQL vs DataFrame API 성능 비교"""
    import time
    
    comparison_results = sql_vs_dataframe_api()
    
    for i, (sql_df, api_df) in enumerate(zip(comparison_results['sql_results'], comparison_results['df_results'])):
        print(f"\n=== 쿼리 {i+1} 성능 비교 ===")
        
        # SQL 실행 시간
        start_time = time.time()
        sql_count = sql_df.count()
        sql_time = time.time() - start_time
        
        # DataFrame API 실행 시간
        start_time = time.time()
        api_count = api_df.count()
        api_time = time.time() - start_time
        
        print(f"SQL 방식: {sql_time:.3f}초 ({sql_count:,} 레코드)")
        print(f"DataFrame API: {api_time:.3f}초 ({api_count:,} 레코드)")
        
        # 실행 계획 비교
        print("\nSQL 실행 계획:")
        sql_df.explain()
        print("\nDataFrame API 실행 계획:")
        api_df.explain()

# 실행
compare_performance()
```

## 참고 링크
- [Apache Spark 공식 문서](https://spark.apache.org/docs/latest/)
- [Spark DataFrame API 가이드](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [EMR에서 Spark DataFrame 활용](https://docs.aws.amazon.com/ko_kr/emr/latest/ReleaseGuide/emr-spark.html)
- [Spark 성능 최적화](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

이제 Spark DataFrame API의 핵심 기능들을 실전에서 바로 활용할 수 있는 예제들로 정리했음. 기본 연산부터 고급 최적화까지 체계적으로 다뤘으니 EMR에서 효과적으로 데이터 처리할 수 있을 것임.
