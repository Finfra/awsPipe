# 3.2 Jupyter Notebook 연동

## EMR Notebooks 설정

### EMR Studio 구성
```python
# Terraform으로 EMR Studio 설정
resource "aws_emr_studio" "bigdata_studio" {
  name                        = "${var.project_name}-studio"
  auth_mode                  = "IAM"
  default_s3_location        = "s3://${var.s3_bucket_name}/studio/"
  engine_security_group_id   = aws_security_group.emr_studio_engine.id
  service_role               = aws_iam_role.emr_studio_service_role.arn
  subnet_ids                 = data.aws_subnets.default.ids
  vpc_id                     = data.aws_vpc.default.id
  workspace_security_group_id = aws_security_group.emr_studio_workspace.id

  tags = var.tags
}

# EMR Studio 서비스 역할
resource "aws_iam_role" "emr_studio_service_role" {
  name = "${var.project_name}-emr-studio-service-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "elasticmapreduce.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "emr_studio_service_role_policy" {
  role       = aws_iam_role.emr_studio_service_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/EMRStudioServiceRolePolicy"
}
```

### Notebook 환경 설정
```python
# PySpark 환경 설정
import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import RandomForestClassifier

# Spark 세션 생성 (EMR Notebooks에서 자동 설정)
spark = SparkSession.builder \
    .appName("EMR-Notebook-Analysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .getOrCreate()

print(f"Spark Version: {spark.version}")
print(f"Spark UI: {spark.sparkContext.uiWebUrl}")
```

### S3 데이터 직접 읽기
```python
# S3에서 데이터 읽기
def load_datalake_data(layer, table_name):
    """데이터 레이크에서 데이터 로드"""
    
    s3_path = f"s3://{BUCKET_NAME}/{layer}/{table_name}/"
    
    try:
        df = spark.read.parquet(s3_path)
        print(f"✅ 로드 완료: {s3_path}")
        print(f"   레코드 수: {df.count():,}")
        print(f"   컬럼 수: {len(df.columns)}")
        return df
    except Exception as e:
        print(f"❌ 로드 실패: {e}")
        return None

# 데이터 탐색
events_df = load_datalake_data("silver", "events")
events_df.printSchema()
events_df.show(5)
```

## 대화형 데이터 분석

### 탐색적 데이터 분석 (EDA)
```python
# 기본 통계 정보
def explore_dataframe(df, sample_size=10000):
    """DataFrame 탐색적 분석"""
    
    print("=== 데이터셋 기본 정보 ===")
    print(f"전체 레코드 수: {df.count():,}")
    print(f"컬럼 수: {len(df.columns)}")
    
    # 샘플링으로 성능 최적화
    sample_df = df.sample(0.1).limit(sample_size)
    
    print("\n=== 수치형 컬럼 통계 ===")
    numeric_cols = [field.name for field in df.schema.fields 
                   if field.dataType.typeName() in ['integer', 'long', 'double', 'float']]
    
    if numeric_cols:
        sample_df.select(numeric_cols).describe().show()
    
    print("\n=== 범주형 컬럼 분포 ===")
    categorical_cols = [field.name for field in df.schema.fields 
                       if field.dataType.typeName() == 'string']
    
    for col in categorical_cols[:5]:  # 상위 5개만
        print(f"\n{col} 분포:")
        sample_df.groupBy(col).count().orderBy(desc("count")).show(10)
    
    print("\n=== 결측값 확인 ===")
    null_counts = []
    for col in df.columns:
        null_count = df.filter(F.col(col).isNull()).count()
        if null_count > 0:
            null_counts.append((col, null_count))
    
    if null_counts:
        for col, count in null_counts:
            print(f"{col}: {count:,} ({count/df.count()*100:.1f}%)")
    else:
        print("결측값 없음")

# 실행
explore_dataframe(events_df)
```

### 시각화 통합
```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Spark DataFrame을 Pandas로 변환하여 시각화
def visualize_trends(spark_df, date_col, value_col, category_col=None):
    """시계열 트렌드 시각화"""
    
    # 일별 집계
    if category_col:
        daily_stats = spark_df \
            .groupBy(date_col, category_col) \
            .agg(
                sum(value_col).alias("total_value"),
                count("*").alias("count")
            ) \
            .orderBy(date_col)
    else:
        daily_stats = spark_df \
            .groupBy(date_col) \
            .agg(
                sum(value_col).alias("total_value"),
                count("*").alias("count")
            ) \
            .orderBy(date_col)
    
    # Pandas로 변환 (작은 데이터셋으로 제한)
    pandas_df = daily_stats.limit(1000).toPandas()
    
    # 시각화
    plt.figure(figsize=(12, 8))
    
    if category_col:
        # 카테고리별 트렌드
        for category in pandas_df[category_col].unique():
            category_data = pandas_df[pandas_df[category_col] == category]
            plt.plot(category_data[date_col], category_data["total_value"], 
                    label=category, marker='o')
        plt.legend()
    else:
        plt.plot(pandas_df[date_col], pandas_df["total_value"], 
                marker='o', linewidth=2)
    
    plt.title(f'{value_col} 트렌드')
    plt.xlabel('날짜')
    plt.ylabel('총합')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# 사용 예시
visualize_trends(events_df, "date", "value", "category")
```

### 실시간 모니터링 대시보드
```python
import ipywidgets as widgets
from IPython.display import display, clear_output
import time

class RealTimeMonitor:
    def __init__(self, spark_session, data_path):
        self.spark = spark_session
        self.data_path = data_path
        self.running = False
    
    def create_dashboard(self):
        """실시간 모니터링 대시보드 생성"""
        
        # 위젯 생성
        self.start_button = widgets.Button(description="모니터링 시작")
        self.stop_button = widgets.Button(description="중지")
        self.output = widgets.Output()
        
        # 이벤트 핸들러
        self.start_button.on_click(self.start_monitoring)
        self.stop_button.on_click(self.stop_monitoring)
        
        # 레이아웃
        controls = widgets.HBox([self.start_button, self.stop_button])
        dashboard = widgets.VBox([controls, self.output])
        
        display(dashboard)
    
    def start_monitoring(self, button):
        """모니터링 시작"""
        self.running = True
        self.monitor_loop()
    
    def stop_monitoring(self, button):
        """모니터링 중지"""
        self.running = False
    
    def monitor_loop(self):
        """모니터링 루프"""
        while self.running:
            with self.output:
                clear_output(wait=True)
                self.update_metrics()
            time.sleep(10)  # 10초마다 업데이트
    
    def update_metrics(self):
        """메트릭 업데이트"""
        try:
            df = self.spark.read.parquet(self.data_path)
            
            # 실시간 통계
            current_hour = datetime.now().hour
            recent_data = df.filter(hour("timestamp") == current_hour)
            
            total_events = recent_data.count()
            unique_users = recent_data.select("user_id").distinct().count()
            total_value = recent_data.agg(sum("value")).collect()[0][0] or 0
            
            print(f"=== 실시간 메트릭 ({datetime.now().strftime('%H:%M:%S')}) ===")
            print(f"현재 시간 이벤트: {total_events:,}")
            print(f"활성 사용자: {unique_users:,}")
            print(f"총 거래액: ${total_value:,.2f}")
            
            # 상위 카테고리
            print("\n상위 카테고리:")
            recent_data.groupBy("category") \
                .count() \
                .orderBy(desc("count")) \
                .show(5, truncate=False)
            
        except Exception as e:
            print(f"오류: {e}")

# 사용
monitor = RealTimeMonitor(spark, "s3://bucket/silver/events/")
monitor.create_dashboard()
```

## 3.3 Spark DataFrame API 활용

### 고급 DataFrame 연산
```python
from pyspark.sql.window import Window
from pyspark.sql.types import *

# 윈도우 함수 활용
def calculate_user_metrics(events_df):
    """사용자별 고급 메트릭 계산"""
    
    # 사용자별 세션 윈도우
    user_window = Window.partitionBy("user_id").orderBy("timestamp")
    
    # 이전 이벤트와의 시간 차이
    events_with_lag = events_df \
        .withColumn("prev_timestamp", 
                   lag("timestamp", 1).over(user_window)) \
        .withColumn("time_diff_minutes", 
                   (col("timestamp").cast("long") - col("prev_timestamp").cast("long")) / 60)
    
    # 세션 구분 (30분 이상 간격시 새 세션)
    session_events = events_with_lag \
        .withColumn("new_session", 
                   when(col("time_diff_minutes") > 30, 1).otherwise(0)) \
        .withColumn("session_id", 
                   sum("new_session").over(user_window))
    
    # 세션별 메트릭
    session_metrics = session_events \
        .groupBy("user_id", "session_id") \
        .agg(
            count("*").alias("events_per_session"),
            (max("timestamp").cast("long") - min("timestamp").cast("long")).alias("session_duration_seconds"),
            sum("value").alias("session_value"),
            collect_list("event_type").alias("event_sequence")
        )
    
    return session_metrics

# 사용자 행동 패턴 분석
def analyze_user_behavior(events_df):
    """사용자 행동 패턴 분석"""
    
    # RFM 분석 (Recency, Frequency, Monetary)
    current_date = datetime.now()
    
    user_rfm = events_df \
        .groupBy("user_id") \
        .agg(
            # Recency: 마지막 활동으로부터 며칠 지났는지
            datediff(lit(current_date), max("timestamp")).alias("recency_days"),
            # Frequency: 총 이벤트 수
            count("*").alias("frequency"),
            # Monetary: 총 거래액
            sum("value").alias("monetary_value")
        )
    
    # RFM 점수 계산 (1-5 스케일)
    from pyspark.sql.functions import percent_rank
    
    rfm_window = Window.orderBy("recency_days")
    freq_window = Window.orderBy(desc("frequency"))
    mon_window = Window.orderBy(desc("monetary_value"))
    
    user_scores = user_rfm \
        .withColumn("R_score", 
                   (5 - floor(percent_rank().over(rfm_window) * 5)).cast("int")) \
        .withColumn("F_score", 
                   (floor(percent_rank().over(freq_window) * 5) + 1).cast("int")) \
        .withColumn("M_score", 
                   (floor(percent_rank().over(mon_window) * 5) + 1).cast("int"))
    
    # 고객 세그먼트 분류
    user_segments = user_scores \
        .withColumn("customer_segment",
            when((col("R_score") >= 4) & (col("F_score") >= 4) & (col("M_score") >= 4), "Champions")
            .when((col("R_score") >= 3) & (col("F_score") >= 3), "Loyal Customers")
            .when((col("R_score") >= 4) & (col("F_score") < 2), "New Customers")
            .when((col("R_score") < 2) & (col("F_score") >= 3), "At Risk")
            .otherwise("Others")
        )
    
    return user_segments

# 실행
session_metrics = calculate_user_metrics(events_df)
user_segments = analyze_user_behavior(events_df)

print("세션 메트릭 예시:")
session_metrics.show(5)

print("\n고객 세그먼트 분포:")
user_segments.groupBy("customer_segment").count().show()
```

### 복잡한 조인 및 집계
```python
def complex_business_analysis(events_df, users_df, products_df):
    """복합 비즈니스 분석"""
    
    # 1. 브로드캐스트 조인으로 성능 최적화
    from pyspark.sql.functions import broadcast
    
    # 작은 테이블은 브로드캐스트
    enriched_events = events_df \
        .join(broadcast(users_df), "user_id") \
        .join(broadcast(products_df), "product_id")
    
    # 2. 복잡한 집계 쿼리
    regional_analysis = enriched_events \
        .groupBy("region", "product_category", "user_segment") \
        .agg(
            count("*").alias("total_events"),
            countDistinct("user_id").alias("unique_users"),
            sum("value").alias("total_revenue"),
            avg("value").alias("avg_order_value"),
            # 중앙값 계산
            expr("percentile_approx(value, 0.5)").alias("median_order_value"),
            # 분위수 계산
            expr("percentile_approx(value, array(0.25, 0.75))").alias("quartiles")
        )
    
    # 3. 피벗 테이블
    pivot_analysis = enriched_events \
        .groupBy("region") \
        .pivot("product_category") \
        .agg(sum("value").alias("revenue"))
    
    # 4. 윈도우 함수로 랭킹
    window_spec = Window.partitionBy("region").orderBy(desc("total_revenue"))
    
    ranked_analysis = regional_analysis \
        .withColumn("revenue_rank", 
                   row_number().over(window_spec)) \
        .withColumn("revenue_percentile", 
                   percent_rank().over(window_spec))
    
    return {
        'enriched_events': enriched_events,
        'regional_analysis': regional_analysis,
        'pivot_analysis': pivot_analysis,
        'ranked_analysis': ranked_analysis
    }

# SQL 스타일 분석도 가능
events_df.createOrReplaceTempView("events")

# 복잡한 SQL 쿼리
cohort_analysis = spark.sql("""
    WITH user_first_purchase AS (
        SELECT 
            user_id,
            MIN(DATE(timestamp)) as first_purchase_date,
            DATE_FORMAT(MIN(timestamp), 'yyyy-MM') as cohort_month
        FROM events 
        WHERE event_type = 'purchase'
        GROUP BY user_id
    ),
    user_purchases AS (
        SELECT 
            e.user_id,
            DATE_FORMAT(e.timestamp, 'yyyy-MM') as purchase_month,
            f.cohort_month,
            MONTHS_BETWEEN(e.timestamp, f.first_purchase_date) as month_diff
        FROM events e
        JOIN user_first_purchase f ON e.user_id = f.user_id
        WHERE e.event_type = 'purchase'
    )
    SELECT 
        cohort_month,
        month_diff,
        COUNT(DISTINCT user_id) as users,
        COUNT(*) as purchases,
        SUM(value) as revenue
    FROM user_purchases
    GROUP BY cohort_month, month_diff
    ORDER BY cohort_month, month_diff
""")

print("코호트 분석 결과:")
cohort_analysis.show(20)
```

### 성능 최적화 기법
```python
def optimize_dataframe_operations(df):
    """DataFrame 연산 최적화"""
    
    # 1. 캐싱 전략
    # 여러 번 사용될 DataFrame은 캐시
    if df.rdd.getNumPartitions() > 1:
        df_cached = df.cache()
        
        # 캐시 워밍업
        df_cached.count()
        print(f"캐시된 파티션 수: {df_cached.rdd.getNumPartitions()}")
    
    # 2. 파티션 최적화
    # 적절한 파티션 수 계산
    total_records = df.count()
    optimal_partitions = max(total_records // 100000, 1)  # 10만 레코드당 1 파티션
    
    if df.rdd.getNumPartitions() != optimal_partitions:
        df_optimized = df.coalesce(optimal_partitions)
    else:
        df_optimized = df
    
    # 3. 조건부 최적화
    def conditional_optimization(df, condition_column, frequently_used_values):
        """조건부 데이터 최적화"""
        
        # 자주 사용되는 값들을 별도 파티션으로
        frequent_df = df.filter(col(condition_column).isin(frequently_used_values))
        others_df = df.filter(~col(condition_column).isin(frequently_used_values))
        
        return frequent_df.cache(), others_df
    
    # 4. 사전 집계 최적화
    def create_preaggregate_tables(df):
        """사전 집계 테이블 생성"""
        
        # 일별 집계
        daily_agg = df \
            .groupBy("date", "category") \
            .agg(
                count("*").alias("event_count"),
                sum("value").alias("total_value"),
                countDistinct("user_id").alias("unique_users")
            )
        
        # 월별 집계
        monthly_agg = df \
            .withColumn("year_month", date_format("timestamp", "yyyy-MM")) \
            .groupBy("year_month", "category") \
            .agg(
                count("*").alias("event_count"),
                sum("value").alias("total_value"),
                countDistinct("user_id").alias("unique_users")
            )
        
        return daily_agg, monthly_agg
    
    # 5. 스키마 최적화
    def optimize_schema(df):
        """스키마 최적화"""
        
        # 데이터 타입 최적화
        optimized_df = df
        
        for field in df.schema.fields:
            if field.dataType == LongType():
                # Long이 실제로는 Int 범위면 변환
                max_val = df.agg(max(field.name)).collect()[0][0]
                if max_val < 2147483647:  # Integer 최대값
                    optimized_df = optimized_df.withColumn(
                        field.name, 
                        col(field.name).cast(IntegerType())
                    )
        
        return optimized_df
    
    return df_optimized

# 사용
optimized_df = optimize_dataframe_operations(events_df)
```

### 메모리 관리 및 스필링
```python
def manage_memory_efficiently():
    """메모리 효율적 관리"""
    
    # 1. 스트리밍 처리로 대용량 데이터 핸들링
    def process_large_dataset_streaming(input_path, output_path):
        """대용량 데이터셋 스트리밍 처리"""
        
        # 배치 크기 제어
        spark.conf.set("spark.sql.streaming.maxBatchDuration", "30s")
        
        streaming_df = spark \
            .readStream \
            .option("maxFilesPerTrigger", 1) \
            .parquet(input_path)
        
        processed_df = streaming_df \
            .filter(col("value") > 0) \
            .groupBy("category") \
            .agg(sum("value").alias("total"))
        
        query = processed_df \
            .writeStream \
            .outputMode("complete") \
            .format("parquet") \
            .option("path", output_path) \
            .option("checkpointLocation", f"{output_path}/_checkpoint") \
            .start()
        
        return query
    
    # 2. 메모리 사용량 모니터링
    def monitor_memory_usage():
        """메모리 사용량 모니터링"""
        
        # Spark Context 정보
        sc = spark.sparkContext
        
        print("=== Spark 메모리 정보 ===")
        print(f"총 Executor 수: {len(sc._jsc.sc().statusTracker().getExecutorInfos())}")
        
        # JVM 메모리 정보
        for executor in sc._jsc.sc().statusTracker().getExecutorInfos():
            print(f"Executor {executor.executorId()}:")
            print(f"  최대 메모리: {executor.maxMemory() / 1024 / 1024:.0f} MB")
            print(f"  사용 메모리: {executor.memoryUsed() / 1024 / 1024:.0f} MB")
    
    # 3. 가비지 컬렉션 튜닝
    spark.conf.set("spark.executor.extraJavaOptions", 
                   "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")
    
    # 4. 스필링 최적화
    spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
    spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
    
    return monitor_memory_usage

# 실행
memory_monitor = manage_memory_efficiently()
memory_monitor()
```

이제 EMR의 Jupyter 환경에서 대화형으로 데이터 분석하고 Spark DataFrame API를 효과적으로 활용할 수 있는 실전 예제들을 제공했음. 다음 파일들도 목차에 맞게 계속 정리하겠음.
