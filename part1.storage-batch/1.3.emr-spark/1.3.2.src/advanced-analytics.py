# í˜„ì¬ JSONL ë°ì´í„°ì— ë§ëŠ” ê³ ê¸‰ ë¶„ì„ í•¨ìˆ˜ë“¤
from pyspark.sql.functions import *
from pyspark.sql.window import Window
from pyspark.sql.types import *

def business_summary_analysis(events_df):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½ ë¶„ì„ - í˜„ì¬ ë°ì´í„° êµ¬ì¡°ìš©"""
    
    # 1. ê¸°ë³¸ ì§‘ê³„
    summary = events_df \
        .groupBy("category", "event_type") \
        .agg(
            count("*").alias("total_events"),
            countDistinct("user_id").alias("unique_users"),
            sum("amount").alias("total_amount"),
            avg("amount").alias("avg_amount"),
            expr("percentile_approx(amount, 0.5)").alias("median_amount")
        ) \
        .orderBy(desc("total_amount"))
    
    # 2. ì‹œê°„ëŒ€ë³„ ë¶„ì„
    hourly_analysis = events_df \
        .groupBy("hour", "category") \
        .agg(
            count("*").alias("events_count"),
            sum("amount").alias("hourly_revenue")
        ) \
        .orderBy("hour")
    
    return {
        'summary': summary,
        'hourly_analysis': hourly_analysis
    }

def user_behavior_analysis(events_df):
    """ì‚¬ìš©ì í–‰ë™ ë¶„ì„"""
    
    # ì‚¬ìš©ìë³„ ìœˆë„ìš°
    user_window = Window.partitionBy("user_id").orderBy("timestamp")
    
    # ì‚¬ìš©ì í–‰ë™ íŒ¨í„´
    user_behavior = events_df \
        .withColumn("event_sequence", row_number().over(user_window)) \
        .withColumn("prev_event", lag("event_type", 1).over(user_window)) \
        .withColumn("prev_amount", lag("amount", 1).over(user_window)) \
        .withColumn("time_diff_minutes", 
                   (unix_timestamp("timestamp") - 
                    unix_timestamp(lag("timestamp", 1).over(user_window))) / 60)
    
    # ì‚¬ìš©ìë³„ ì§‘ê³„
    user_summary = events_df \
        .groupBy("user_id") \
        .agg(
            count("*").alias("total_events"),
            sum("amount").alias("total_spent"),
            avg("amount").alias("avg_transaction"),
            countDistinct("category").alias("categories_used"),
            countDistinct("event_type").alias("event_types"),
            collect_set("event_type").alias("event_pattern")
        )
    
    return {
        'user_behavior': user_behavior,
        'user_summary': user_summary
    }

def time_series_analysis(events_df):
    """ì‹œê³„ì—´ ë¶„ì„"""
    
    # ì‹œê°„ë³„ íŠ¸ë Œë“œ
    hourly_trend = events_df \
        .groupBy("year", "month", "day", "hour") \
        .agg(
            count("*").alias("events"),
            sum("amount").alias("revenue"),
            countDistinct("user_id").alias("active_users")
        ) \
        .orderBy("year", "month", "day", "hour")
    
    # ì´ë™í‰ê·  (3ì‹œê°„ ìœˆë„ìš°)
    time_window = Window.orderBy("year", "month", "day", "hour").rowsBetween(-2, 0)
    
    trend_with_ma = hourly_trend \
        .withColumn("events_ma3", avg("events").over(time_window)) \
        .withColumn("revenue_ma3", avg("revenue").over(time_window))
    
    return trend_with_ma

def event_sequence_analysis(events_df):
    """ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤ ë¶„ì„"""
    
    # ì‚¬ìš©ìë³„ ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤
    user_window = Window.partitionBy("user_id").orderBy("timestamp")
    
    sequence_df = events_df \
        .withColumn("next_event", lead("event_type", 1).over(user_window)) \
        .withColumn("event_pair", 
                   concat(col("event_type"), lit("->"), col("next_event"))) \
        .filter(col("next_event").isNotNull())
    
    # ì´ë²¤íŠ¸ ì „í™˜ íŒ¨í„´
    transition_patterns = sequence_df \
        .groupBy("event_pair") \
        .count() \
        .orderBy(desc("count"))
    
    return transition_patterns

def category_performance_analysis(events_df):
    """ì¹´í…Œê³ ë¦¬ ì„±ê³¼ ë¶„ì„"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼ ë©”íŠ¸ë¦­
    category_performance = events_df \
        .groupBy("category") \
        .agg(
            count("*").alias("total_transactions"),
            sum("amount").alias("total_revenue"),
            avg("amount").alias("avg_transaction_value"),
            countDistinct("user_id").alias("unique_customers"),
            # êµ¬ë§¤ ì „í™˜ìœ¨ (purchase ì´ë²¤íŠ¸ ë¹„ìœ¨)
            (sum(when(col("event_type") == "purchase", 1).otherwise(0)) / count("*") * 100)
            .alias("purchase_conversion_rate")
        ) \
        .withColumn("revenue_per_customer", 
                   col("total_revenue") / col("unique_customers"))
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë­í‚¹
    category_window = Window.orderBy(desc("total_revenue"))
    
    ranked_categories = category_performance \
        .withColumn("revenue_rank", row_number().over(category_window)) \
        .withColumn("revenue_percentile", percent_rank().over(category_window))
    
    return ranked_categories

def anomaly_detection(events_df):
    """ì´ìƒì¹˜ íƒì§€ - ê¸ˆì•¡ ê¸°ì¤€"""
    
    # í†µê³„ê°’ ê³„ì‚°
    stats = events_df.agg(
        avg("amount").alias("mean_amount"),
        stddev("amount").alias("std_amount")
    ).collect()[0]
    
    mean_val = stats["mean_amount"]
    std_val = stats["std_amount"]
    
    # Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
    anomaly_df = events_df \
        .withColumn("z_score", 
                   abs((col("amount") - lit(mean_val)) / lit(std_val))) \
        .withColumn("is_anomaly", col("z_score") > 2.5) \
        .withColumn("anomaly_type",
                   when(col("amount") > (lit(mean_val) + lit(std_val) * 2.5), "high")
                   .when(col("amount") < (lit(mean_val) - lit(std_val) * 2.5), "low")
                   .otherwise("normal"))
    
    return anomaly_df

def create_pivot_analysis(events_df):
    """í”¼ë²— ë¶„ì„"""
    
    # ì¹´í…Œê³ ë¦¬ë³„ x ì´ë²¤íŠ¸íƒ€ì…ë³„ í”¼ë²—
    pivot_df = events_df \
        .groupBy("category") \
        .pivot("event_type") \
        .agg(
            count("*").alias("count"),
            sum("amount").alias("total_amount")
        )
    
    return pivot_df

def calculate_conversion_funnel(events_df):
    """ì „í™˜ í¼ë„ ë¶„ì„"""
    
    # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì‚¬ìš©ì ìˆ˜
    funnel_data = events_df \
        .groupBy("event_type") \
        .agg(countDistinct("user_id").alias("unique_users")) \
        .orderBy(desc("unique_users"))
    
    # ì „í™˜ìœ¨ ê³„ì‚° (ê°€ì •: login -> view -> purchase ìˆœì„œ)
    event_counts = {row["event_type"]: row["unique_users"] for row in funnel_data.collect()}
    
    print("ğŸ”„ ì „í™˜ í¼ë„ ë¶„ì„:")
    for event_type, count in event_counts.items():
        if "login" in event_counts:
            conversion_rate = (count / event_counts["login"] * 100) if event_counts["login"] > 0 else 0
            print(f"   {event_type}: {count:,} ëª… ({conversion_rate:.1f}%)")
    
    return funnel_data

def daily_cohort_simple(events_df):
    """ê°„ë‹¨í•œ ì¼ë³„ ì½”í˜¸íŠ¸ ë¶„ì„"""
    
    # ì‚¬ìš©ìë³„ ì²« êµ¬ë§¤ì¼
    first_purchase = events_df \
        .filter(col("event_type") == "purchase") \
        .groupBy("user_id") \
        .agg(min("timestamp").alias("first_purchase_date"))
    
    # ì½”í˜¸íŠ¸ ì¡°ì¸
    cohort_df = events_df \
        .join(first_purchase, "user_id") \
        .withColumn("days_since_first", 
                   datediff(to_date(col("timestamp")), to_date(col("first_purchase_date"))))
    
    # ì½”í˜¸íŠ¸ ì§‘ê³„
    cohort_analysis = cohort_df \
        .groupBy("first_purchase_date", "days_since_first") \
        .agg(
            countDistinct("user_id").alias("active_users"),
            sum("amount").alias("cohort_revenue")
        ) \
        .orderBy("first_purchase_date", "days_since_first")
    
    return cohort_analysis

def quick_insights(events_df):
    """ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
    
    print("ğŸ” ë¹ ë¥¸ ë°ì´í„° ì¸ì‚¬ì´íŠ¸")
    print("="*40)
    
    # 1. ê¸°ë³¸ í†µê³„
    total_records = events_df.count()
    total_users = events_df.select("user_id").distinct().count()
    total_revenue = events_df.agg(sum("amount")).collect()[0][0]
    
    print(f"ğŸ“Š ì´ ë ˆì½”ë“œ: {total_records:,}")
    print(f"ğŸ‘¥ ì´ ì‚¬ìš©ì: {total_users:,}")
    print(f"ğŸ’° ì´ ê±°ë˜ì•¡: ${total_revenue:,.2f}")
    print(f"ğŸ“ˆ ì‚¬ìš©ìë‹¹ í‰ê·  ê±°ë˜ì•¡: ${total_revenue/total_users:.2f}")
    
    # 2. ìƒìœ„ ì¹´í…Œê³ ë¦¬
    print(f"\nğŸ† ìƒìœ„ ì¹´í…Œê³ ë¦¬ (ê±°ë˜ì•¡ ê¸°ì¤€):")
    events_df.groupBy("category") \
        .agg(sum("amount").alias("revenue")) \
        .orderBy(desc("revenue")) \
        .show(5, truncate=False)
    
    # 3. í”¼í¬ ì‹œê°„ëŒ€
    print(f"\nâ° ì‹œê°„ëŒ€ë³„ í™œë™:")
    events_df.groupBy("hour") \
        .count() \
        .orderBy(desc("count")) \
        .show(5, truncate=False)

# ëª¨ë“  ë¶„ì„ì„ í•œë²ˆì— ì‹¤í–‰
def run_complete_analysis(events_df):
    """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
    
    print("ğŸš€ ì „ì²´ ë¶„ì„ ì‹œì‘...")
    
    # 1. ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸
    quick_insights(events_df)
    
    # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„
    business_results = business_summary_analysis(events_df)
    print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼:")
    business_results['summary'].show(10, truncate=False)
    
    # 3. ì´ìƒì¹˜ íƒì§€
    anomalies = anomaly_detection(events_df)
    anomaly_count = anomalies.filter(col("is_anomaly")).count()
    print(f"\nâš ï¸  íƒì§€ëœ ì´ìƒì¹˜: {anomaly_count}ê°œ")
    
    # 4. ì „í™˜ í¼ë„
    calculate_conversion_funnel(events_df)
    
    # 5. ì´ë²¤íŠ¸ ì‹œí€€ìŠ¤
    print(f"\nğŸ”— ì£¼ìš” ì´ë²¤íŠ¸ ì „í™˜ íŒ¨í„´:")
    event_sequence_analysis(events_df).show(10, truncate=False)
    
    return {
        'business_analysis': business_results,
        'anomalies': anomalies,
        'user_behavior': user_behavior_analysis(events_df)
    }

if __name__ == "__main__":
    print("âœ… í˜„ì¬ ë°ì´í„° êµ¬ì¡°ì— ë§ëŠ” ê³ ê¸‰ ë¶„ì„ í•¨ìˆ˜ë“¤ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("   - business_summary_analysis(): ë¹„ì¦ˆë‹ˆìŠ¤ ìš”ì•½")
    print("   - user_behavior_analysis(): ì‚¬ìš©ì í–‰ë™ ë¶„ì„")
    print("   - time_series_analysis(): ì‹œê³„ì—´ ë¶„ì„")
    print("   - anomaly_detection(): ì´ìƒì¹˜ íƒì§€")
    print("   - calculate_conversion_funnel(): ì „í™˜ í¼ë„")
    print("   - quick_insights(): ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸")
    print("   - run_complete_analysis(): ì „ì²´ ë¶„ì„ ì‹¤í–‰")